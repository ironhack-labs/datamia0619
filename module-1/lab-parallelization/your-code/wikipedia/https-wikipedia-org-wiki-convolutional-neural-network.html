doctype-html-html-class-client-nojs-lang-en-dir-ltr-head-meta-charset-utf-8-title-convolutional-neural-network-wikipedia-title-script-document-documentelement-classname-document-documentelement-classname-replace-s-client-nojs-s-1client-js-2-rlconf-wgcanonicalnamespace-wgcanonicalspecialpagename-1-wgnamespacenumber-0-wgpagename-convolutional-neural-network-wgtitle-convolutional-neural-network-wgcurrevisionid-903560845-wgrevisionid-903560845-wgarticleid-40409788-wgisarticle-0-wgisredirect-1-wgaction-view-wgusername-null-wgusergroups-wgcategories-cs1-maint-multiple-names-authors-list-articles-needing-additional-references-from-june-2019-all-articles-needing-additional-references-all-articles-with-unsourced-statements-articles-with-unsourced-statements-from-june-2019-articles-with-unsourced-statements-from-october-2017-articles-containing-explicitly-cited-english-language-text-wikipedia-articles-needing-clarification-from-december-2018-all-articles-needing-examples-articles-needing-examples-from-october-2017-articles-with-unsourced-statements-from-march-2019-articles-needing-additional-references-from-june-2017-articles-with-unsourced-statements-from-december-2018-all-articles-with-specifically-marked-weasel-worded-phrases-articles-with-specifically-marked-weasel-worded-phrases-from-december-2018-artificial-neural-networks-computer-vision-computational-neuroscience-machine-learning-wgbreakframes-1-wgpagecontentlanguage-en-wgpagecontentmodel-wikitext-wgseparatortransformtable-wgdigittransformtable-wgdefaultdateformat-dmy-wgmonthnames-january-february-march-april-may-june-july-august-september-october-november-december-wgmonthnamesshort-jan-feb-mar-apr-may-jun-jul-aug-sep-oct-nov-dec-wgrelevantpagename-convolutional-neural-network-wgrelevantarticleid-40409788-wgrequestid-xrnc-gpaadkaaj-qvsuaaabq-wgcspnonce-1-wgisprobablyeditable-0-wgrelevantpageisprobablyeditable-0-wgrestrictionedit-wgrestrictionmove-wgmediavieweronclick-0-wgmediaviewerenabledbydefault-0-wgpopupsreferencepreviews-1-wgpopupsconflictswithnavpopupgadget-1-wgvisualeditor-pagelanguagecode-en-pagelanguagedir-ltr-pagevariantfallbacks-en-wgmfdisplaywikibasedescriptions-search-0-nearby-0-watchlist-0-tagline-1-wgwmeschemaeditattemptstepoversample-1-wgpoweredbyhhvm-0-wgulscurrentautonym-english-wgnoticeproject-wikipedia-wgcentralnoticecategoriesusinglegacy-fundraising-fundraising-wgwikibaseitemid-q17084460-wgcentralauthmobiledomain-1-wgeditsubmitbuttonlabelpublish-0-rlstate-ext-gadget-charinsert-styles-ready-ext-globalcssjs-user-styles-ready-ext-globalcssjs-site-styles-ready-site-styles-ready-noscript-ready-user-styles-ready-ext-globalcssjs-user-ready-ext-globalcssjs-site-ready-user-ready-user-options-ready-user-tokens-loading-ext-cite-styles-ready-ext-math-styles-ready-mediawiki-legacy-shared-ready-mediawiki-legacy-commonprint-ready-mediawiki-toc-styles-ready-wikibase-client-init-ready-ext-visualeditor-desktoparticletarget-noscript-ready-ext-uls-interlanguage-ready-ext-wikimediabadges-ready-ext-3d-styles-ready-mediawiki-skinning-interface-ready-skins-vector-styles-ready-rlpagemodules-ext-cite-ux-enhancements-ext-math-scripts-ext-scribunto-logs-site-mediawiki-page-startup-mediawiki-page-ready-mediawiki-toc-mediawiki-searchsuggest-ext-gadget-teahouse-ext-gadget-referencetooltips-ext-gadget-watchlist-notice-ext-gadget-drn-wizard-ext-gadget-charinsert-ext-gadget-reftoolbar-ext-gadget-extra-toolbar-buttons-ext-gadget-switcher-ext-centralauth-centralautologin-mmv-head-mmv-bootstrap-autostart-ext-popups-ext-visualeditor-desktoparticletarget-init-ext-visualeditor-targetloader-ext-eventlogging-ext-wikimediaevents-ext-navigationtiming-ext-uls-compactlinks-ext-uls-interface-ext-quicksurveys-init-ext-centralnotice-geoip-ext-centralnotice-startup-skins-vector-js-script-script-rlq-window-rlq-push-function-mw-loader-implement-user-tokens-0tffind-function-jquery-require-module-nomin-mw-user-tokens-set-edittoken-patroltoken-watchtoken-csrftoken-script-link-rel-stylesheet-href-w-load-php-lang-en-modules-ext-3d-styles-7cext-cite-styles-7cext-math-styles-7cext-uls-interlanguage-7cext-visualeditor-desktoparticletarget-noscript-7cext-wikimediabadges-7cmediawiki-legacy-commonprint-2cshared-7cmediawiki-skinning-interface-7cmediawiki-toc-styles-7cskins-vector-styles-7cwikibase-client-init-only-styles-skin-vector-script-async-src-w-load-php-lang-en-modules-startup-only-scripts-skin-vector-script-meta-name-resourceloaderdynamicstyles-content-link-rel-stylesheet-href-w-load-php-lang-en-modules-ext-gadget-charinsert-styles-only-styles-skin-vector-link-rel-stylesheet-href-w-load-php-lang-en-modules-site-styles-only-styles-skin-vector-meta-name-generator-content-mediawiki-1-34-0-wmf-10-meta-name-referrer-content-origin-meta-name-referrer-content-origin-when-crossorigin-meta-name-referrer-content-origin-when-cross-origin-meta-property-og-image-content-https-upload-wikimedia-org-wikipedia-commons-thumb-f-fe-kernel-machine-svg-1200px-kernel-machine-svg-png-link-rel-alternate-href-android-app-org-wikipedia-http-en-m-wikipedia-org-wiki-convolutional-neural-network-link-rel-alternate-type-application-x-wiki-title-edit-this-page-href-w-index-php-title-convolutional-neural-network-action-edit-link-rel-edit-title-edit-this-page-href-w-index-php-title-convolutional-neural-network-action-edit-link-rel-apple-touch-icon-href-static-apple-touch-wikipedia-png-link-rel-shortcut-icon-href-static-favicon-wikipedia-ico-link-rel-search-type-application-opensearchdescription-xml-href-w-opensearch-desc-php-title-wikipedia-en-link-rel-edituri-type-application-rsd-xml-href-en-wikipedia-org-w-api-php-action-rsd-link-rel-license-href-creativecommons-org-licenses-by-sa-3-0-link-rel-canonical-href-https-en-wikipedia-org-wiki-convolutional-neural-network-link-rel-dns-prefetch-href-login-wikimedia-org-link-rel-dns-prefetch-href-meta-wikimedia-org-if-lt-ie-9-script-src-w-load-php-modules-html5shiv-only-scripts-sync-1-script-endif-head-body-class-mediawiki-ltr-sitedir-ltr-mw-hide-empty-elt-ns-0-ns-subject-mw-editable-page-convolutional-neural-network-rootpage-convolutional-neural-network-skin-vector-action-view-div-id-mw-page-base-class-noprint-div-div-id-mw-head-base-class-noprint-div-div-id-content-class-mw-body-role-main-a-id-top-a-div-id-sitenotice-class-mw-body-content-centralnotice-div-div-class-mw-indicators-mw-body-content-div-h1-id-firstheading-class-firstheading-lang-en-convolutional-neural-network-h1-div-id-bodycontent-class-mw-body-content-div-id-sitesub-class-noprint-from-wikipedia-the-free-encyclopedia-div-div-id-contentsub-div-div-id-jump-to-nav-div-a-class-mw-jump-link-href-mw-head-jump-to-navigation-a-a-class-mw-jump-link-href-p-search-jump-to-search-a-div-id-mw-content-text-lang-en-dir-ltr-class-mw-content-ltr-div-class-mw-parser-output-div-role-note-class-hatnote-navigation-not-searchable-for-other-uses-see-a-href-wiki-cnn-disambiguation-class-mw-disambig-title-cnn-disambiguation-cnn-disambiguation-a-div-table-class-box-more-citations-needed-plainlinks-metadata-ambox-ambox-content-ambox-refimprove-role-presentation-tbody-tr-td-class-mbox-image-div-style-width-52px-a-href-wiki-file-question-book-new-svg-class-image-img-alt-src-upload-wikimedia-org-wikipedia-en-thumb-9-99-question-book-new-svg-50px-question-book-new-svg-png-decoding-async-width-50-height-39-srcset-upload-wikimedia-org-wikipedia-en-thumb-9-99-question-book-new-svg-75px-question-book-new-svg-png-1-5x-upload-wikimedia-org-wikipedia-en-thumb-9-99-question-book-new-svg-100px-question-book-new-svg-png-2x-data-file-width-512-data-file-height-399-a-div-td-td-class-mbox-text-div-class-mbox-text-span-this-article-b-needs-additional-citations-for-a-href-wiki-wikipedia-verifiability-title-wikipedia-verifiability-verification-a-b-span-class-hide-when-compact-please-help-a-class-external-text-href-en-wikipedia-org-w-index-php-title-convolutional-neural-network-action-edit-improve-this-article-a-by-a-href-wiki-help-introduction-to-referencing-with-wiki-markup-1-title-help-introduction-to-referencing-with-wiki-markup-1-adding-citations-to-reliable-sources-a-unsourced-material-may-be-challenged-and-removed-br-small-span-class-plainlinks-i-find-sources-i-a-rel-nofollow-class-external-text-href-www-google-com-search-as-eq-wikipedia-q-22convolutional-neural-network-22-convolutional-neural-network-a-a-rel-nofollow-class-external-text-href-www-google-com-search-tbm-nws-q-22convolutional-neural-network-22-wikipedia-news-a-b-b-a-rel-nofollow-class-external-text-href-www-google-com-search-q-22convolutional-neural-network-22-site-news-google-com-newspapers-source-newspapers-newspapers-a-b-b-a-rel-nofollow-class-external-text-href-www-google-com-search-tbs-bks-1-q-22convolutional-neural-network-22-wikipedia-books-a-b-b-a-rel-nofollow-class-external-text-href-scholar-google-com-scholar-q-22convolutional-neural-network-22-scholar-a-b-b-a-rel-nofollow-class-external-text-href-https-www-jstor-org-action-dobasicsearch-query-22convolutional-neural-network-22-acc-on-wc-on-jstor-a-span-small-span-small-class-date-container-i-span-class-date-june-2019-span-i-small-small-class-hide-when-compact-i-a-href-wiki-help-maintenance-template-removal-title-help-maintenance-template-removal-learn-how-and-when-to-remove-this-template-message-a-i-small-div-td-tr-tbody-table-table-class-vertical-navbox-nowraplinks-style-float-right-clear-right-width-22-0em-margin-0-0-1-0em-1-0em-background-f9f9f9-border-1px-solid-aaa-padding-0-2em-border-spacing-0-4em-0-text-align-center-line-height-1-4em-font-size-88-tbody-tr-th-style-padding-0-2em-0-4em-0-2em-font-size-145-line-height-1-2em-a-href-wiki-machine-learning-title-machine-learning-machine-learning-a-and-br-a-href-wiki-data-mining-title-data-mining-data-mining-a-th-tr-tr-td-style-padding-0-2em-0-0-4em-padding-0-25em-0-25em-0-75em-a-href-wiki-file-kernel-machine-svg-class-image-img-alt-kernel-machine-svg-src-upload-wikimedia-org-wikipedia-commons-thumb-f-fe-kernel-machine-svg-220px-kernel-machine-svg-png-decoding-async-width-220-height-100-srcset-upload-wikimedia-org-wikipedia-commons-thumb-f-fe-kernel-machine-svg-330px-kernel-machine-svg-png-1-5x-upload-wikimedia-org-wikipedia-commons-thumb-f-fe-kernel-machine-svg-440px-kernel-machine-svg-png-2x-data-file-width-512-data-file-height-233-a-td-tr-tr-td-style-padding-0-0-1em-0-4em-div-class-navframe-collapsed-style-border-none-padding-0-div-class-navhead-style-font-size-105-background-transparent-text-align-left-problems-div-div-class-navcontent-style-font-size-105-padding-0-2em-0-0-4em-text-align-center-div-class-hlist-ul-li-a-href-wiki-statistical-classification-title-statistical-classification-classification-a-li-li-a-href-wiki-cluster-analysis-title-cluster-analysis-clustering-a-li-li-a-href-wiki-regression-analysis-title-regression-analysis-regression-a-li-li-a-href-wiki-anomaly-detection-title-anomaly-detection-anomaly-detection-a-li-li-a-href-wiki-automated-machine-learning-title-automated-machine-learning-automl-a-li-li-a-href-wiki-association-rule-learning-title-association-rule-learning-association-rules-a-li-li-a-href-wiki-reinforcement-learning-title-reinforcement-learning-reinforcement-learning-a-li-li-a-href-wiki-structured-prediction-title-structured-prediction-structured-prediction-a-li-li-a-href-wiki-feature-engineering-title-feature-engineering-feature-engineering-a-li-li-a-href-wiki-feature-learning-title-feature-learning-feature-learning-a-li-li-a-href-wiki-online-machine-learning-title-online-machine-learning-online-learning-a-li-li-a-href-wiki-semi-supervised-learning-title-semi-supervised-learning-semi-supervised-learning-a-li-li-a-href-wiki-unsupervised-learning-title-unsupervised-learning-unsupervised-learning-a-li-li-a-href-wiki-learning-to-rank-title-learning-to-rank-learning-to-rank-a-li-li-a-href-wiki-grammar-induction-title-grammar-induction-grammar-induction-a-li-ul-div-div-div-td-tr-tr-td-style-padding-0-0-1em-0-4em-div-class-navframe-collapsed-style-border-none-padding-0-div-class-navhead-style-font-size-105-background-transparent-text-align-left-div-style-padding-0-1em-0-line-height-1-2em-a-href-wiki-supervised-learning-title-supervised-learning-supervised-learning-a-br-style-data-mw-deduplicate-templatestyles-r886047488-mw-parser-output-nobold-font-weight-normal-style-span-class-nobold-span-style-font-size-85-b-a-href-wiki-statistical-classification-title-statistical-classification-classification-a-b-b-a-href-wiki-regression-analysis-title-regression-analysis-regression-a-b-span-span-div-div-div-class-navcontent-style-font-size-105-padding-0-2em-0-0-4em-text-align-center-div-class-hlist-ul-li-a-href-wiki-decision-tree-learning-title-decision-tree-learning-decision-trees-a-li-li-a-href-wiki-ensemble-learning-title-ensemble-learning-ensembles-a-ul-li-a-href-wiki-bootstrap-aggregating-title-bootstrap-aggregating-bagging-a-li-li-a-href-wiki-boosting-machine-learning-title-boosting-machine-learning-boosting-a-li-li-a-href-wiki-random-forest-title-random-forest-random-forest-a-li-ul-li-li-a-href-wiki-k-nearest-neighbors-algorithm-title-k-nearest-neighbors-algorithm-i-k-i-nn-a-li-li-a-href-wiki-linear-regression-title-linear-regression-linear-regression-a-li-li-a-href-wiki-naive-bayes-classifier-title-naive-bayes-classifier-naive-bayes-a-li-li-a-href-wiki-artificial-neural-network-title-artificial-neural-network-artificial-neural-networks-a-li-li-a-href-wiki-logistic-regression-title-logistic-regression-logistic-regression-a-li-li-a-href-wiki-perceptron-title-perceptron-perceptron-a-li-li-a-href-wiki-relevance-vector-machine-title-relevance-vector-machine-relevance-vector-machine-rvm-a-li-li-a-href-wiki-support-vector-machine-title-support-vector-machine-support-vector-machine-svm-a-li-ul-div-div-div-td-tr-tr-td-style-padding-0-0-1em-0-4em-div-class-navframe-collapsed-style-border-none-padding-0-div-class-navhead-style-font-size-105-background-transparent-text-align-left-a-href-wiki-cluster-analysis-title-cluster-analysis-clustering-a-div-div-class-navcontent-style-font-size-105-padding-0-2em-0-0-4em-text-align-center-div-class-hlist-ul-li-a-href-wiki-birch-title-birch-birch-a-li-li-a-href-wiki-cure-data-clustering-algorithm-class-mw-redirect-title-cure-data-clustering-algorithm-cure-a-li-li-a-href-wiki-hierarchical-clustering-title-hierarchical-clustering-hierarchical-a-li-li-a-href-wiki-k-means-clustering-title-k-means-clustering-i-k-i-means-a-li-li-a-href-wiki-expectation-e2-80-93maximization-algorithm-title-expectation-maximization-algorithm-expectation-maximization-em-a-li-li-br-a-href-wiki-dbscan-title-dbscan-dbscan-a-li-li-a-href-wiki-optics-algorithm-title-optics-algorithm-optics-a-li-li-a-href-wiki-mean-shift-class-mw-redirect-title-mean-shift-mean-shift-a-li-ul-div-div-div-td-tr-tr-td-style-padding-0-0-1em-0-4em-div-class-navframe-collapsed-style-border-none-padding-0-div-class-navhead-style-font-size-105-background-transparent-text-align-left-a-href-wiki-dimensionality-reduction-title-dimensionality-reduction-dimensionality-reduction-a-div-div-class-navcontent-style-font-size-105-padding-0-2em-0-0-4em-text-align-center-div-class-hlist-ul-li-a-href-wiki-factor-analysis-title-factor-analysis-factor-analysis-a-li-li-a-href-wiki-canonical-correlation-analysis-class-mw-redirect-title-canonical-correlation-analysis-cca-a-li-li-a-href-wiki-independent-component-analysis-title-independent-component-analysis-ica-a-li-li-a-href-wiki-linear-discriminant-analysis-title-linear-discriminant-analysis-lda-a-li-li-a-href-wiki-non-negative-matrix-factorization-title-non-negative-matrix-factorization-nmf-a-li-li-a-href-wiki-principal-component-analysis-title-principal-component-analysis-pca-a-li-li-a-href-wiki-t-distributed-stochastic-neighbor-embedding-title-t-distributed-stochastic-neighbor-embedding-t-sne-a-li-ul-div-div-div-td-tr-tr-td-style-padding-0-0-1em-0-4em-div-class-navframe-collapsed-style-border-none-padding-0-div-class-navhead-style-font-size-105-background-transparent-text-align-left-a-href-wiki-structured-prediction-title-structured-prediction-structured-prediction-a-div-div-class-navcontent-style-font-size-105-padding-0-2em-0-0-4em-text-align-center-div-class-hlist-ul-li-a-href-wiki-graphical-model-title-graphical-model-graphical-models-a-ul-li-a-href-wiki-bayesian-network-title-bayesian-network-bayes-net-a-li-li-a-href-wiki-conditional-random-field-title-conditional-random-field-conditional-random-field-a-li-li-a-href-wiki-hidden-markov-model-title-hidden-markov-model-hidden-markov-a-li-ul-li-ul-div-div-div-td-tr-tr-td-style-padding-0-0-1em-0-4em-div-class-navframe-collapsed-style-border-none-padding-0-div-class-navhead-style-font-size-105-background-transparent-text-align-left-a-href-wiki-anomaly-detection-title-anomaly-detection-anomaly-detection-a-div-div-class-navcontent-style-font-size-105-padding-0-2em-0-0-4em-text-align-center-div-class-hlist-ul-li-a-href-wiki-k-nearest-neighbors-classification-class-mw-redirect-title-k-nearest-neighbors-classification-i-k-i-nn-a-li-li-a-href-wiki-local-outlier-factor-title-local-outlier-factor-local-outlier-factor-a-li-ul-div-div-div-td-tr-tr-td-style-padding-0-0-1em-0-4em-div-class-navframe-collapsed-style-border-none-padding-0-div-class-navhead-style-font-size-105-background-transparent-text-align-left-a-href-wiki-artificial-neural-networks-class-mw-redirect-title-artificial-neural-networks-artificial-neural-networks-a-div-div-class-navcontent-style-font-size-105-padding-0-2em-0-0-4em-text-align-center-div-class-hlist-ul-li-a-href-wiki-autoencoder-title-autoencoder-autoencoder-a-li-li-a-href-wiki-deep-learning-title-deep-learning-deep-learning-a-li-li-a-href-wiki-deepdream-title-deepdream-deepdream-a-li-li-a-href-wiki-multilayer-perceptron-title-multilayer-perceptron-multilayer-perceptron-a-li-li-a-href-wiki-recurrent-neural-network-title-recurrent-neural-network-rnn-a-ul-li-a-href-wiki-long-short-term-memory-title-long-short-term-memory-lstm-a-li-li-a-href-wiki-gated-recurrent-unit-title-gated-recurrent-unit-gru-a-li-ul-li-li-a-href-wiki-restricted-boltzmann-machine-title-restricted-boltzmann-machine-restricted-boltzmann-machine-a-li-li-a-href-wiki-generative-adversarial-network-title-generative-adversarial-network-gan-a-li-li-a-href-wiki-self-organizing-map-title-self-organizing-map-som-a-li-li-a-class-mw-selflink-selflink-convolutional-neural-network-a-ul-li-a-href-wiki-u-net-title-u-net-u-net-a-li-ul-li-ul-div-div-div-td-tr-tr-td-style-padding-0-0-1em-0-4em-div-class-navframe-collapsed-style-border-none-padding-0-div-class-navhead-style-font-size-105-background-transparent-text-align-left-a-href-wiki-reinforcement-learning-title-reinforcement-learning-reinforcement-learning-a-div-div-class-navcontent-style-font-size-105-padding-0-2em-0-0-4em-text-align-center-div-class-hlist-ul-li-a-href-wiki-q-learning-title-q-learning-q-learning-a-li-li-a-href-wiki-state-e2-80-93action-e2-80-93reward-e2-80-93state-e2-80-93action-title-state-action-reward-state-action-sarsa-a-li-li-a-href-wiki-temporal-difference-learning-title-temporal-difference-learning-temporal-difference-td-a-li-ul-div-div-div-td-tr-tr-td-style-padding-0-0-1em-0-4em-div-class-navframe-collapsed-style-border-none-padding-0-div-class-navhead-style-font-size-105-background-transparent-text-align-left-theory-div-div-class-navcontent-style-font-size-105-padding-0-2em-0-0-4em-text-align-center-div-class-hlist-ul-li-a-href-wiki-bias-e2-80-93variance-dilemma-class-mw-redirect-title-bias-variance-dilemma-bias-variance-dilemma-a-li-li-a-href-wiki-computational-learning-theory-title-computational-learning-theory-computational-learning-theory-a-li-li-a-href-wiki-empirical-risk-minimization-title-empirical-risk-minimization-empirical-risk-minimization-a-li-li-a-href-wiki-occam-learning-title-occam-learning-occam-learning-a-li-li-a-href-wiki-probably-approximately-correct-learning-title-probably-approximately-correct-learning-pac-learning-a-li-li-a-href-wiki-statistical-learning-theory-title-statistical-learning-theory-statistical-learning-a-li-li-a-href-wiki-vapnik-e2-80-93chervonenkis-theory-title-vapnik-chervonenkis-theory-vc-theory-a-li-ul-div-div-div-td-tr-tr-td-style-padding-0-0-1em-0-4em-div-class-navframe-collapsed-style-border-none-padding-0-div-class-navhead-style-font-size-105-background-transparent-text-align-left-machine-learning-venues-div-div-class-navcontent-style-font-size-105-padding-0-2em-0-0-4em-text-align-center-div-class-hlist-ul-li-a-href-wiki-conference-on-neural-information-processing-systems-title-conference-on-neural-information-processing-systems-nips-a-li-li-a-href-wiki-international-conference-on-machine-learning-title-international-conference-on-machine-learning-icml-a-li-li-a-href-wiki-machine-learning-journal-title-machine-learning-journal-ml-a-li-li-a-href-wiki-journal-of-machine-learning-research-title-journal-of-machine-learning-research-jmlr-a-li-li-a-rel-nofollow-class-external-text-href-https-arxiv-org-list-cs-lg-recent-arxiv-cs-lg-a-li-ul-div-div-div-td-tr-tr-td-style-padding-0-0-1em-0-4em-div-class-navframe-collapsed-style-border-none-padding-0-div-class-navhead-style-font-size-105-background-transparent-text-align-left-a-href-wiki-glossary-of-artificial-intelligence-title-glossary-of-artificial-intelligence-glossary-of-artificial-intelligence-a-div-div-class-navcontent-style-font-size-105-padding-0-2em-0-0-4em-text-align-center-div-class-hlist-ul-li-a-href-wiki-glossary-of-artificial-intelligence-title-glossary-of-artificial-intelligence-glossary-of-artificial-intelligence-a-li-ul-div-div-div-td-tr-tr-td-style-padding-0-0-1em-0-4em-div-class-navframe-collapsed-style-border-none-padding-0-div-class-navhead-style-font-size-105-background-transparent-text-align-left-related-articles-div-div-class-navcontent-style-font-size-105-padding-0-2em-0-0-4em-text-align-center-div-class-hlist-ul-li-a-href-wiki-list-of-datasets-for-machine-learning-research-title-list-of-datasets-for-machine-learning-research-list-of-datasets-for-machine-learning-research-a-li-li-a-href-wiki-outline-of-machine-learning-title-outline-of-machine-learning-outline-of-machine-learning-a-li-ul-div-div-div-td-tr-tr-td-class-plainlist-style-padding-0-3em-0-4em-0-3em-font-weight-bold-border-top-1px-solid-aaa-border-bottom-1px-solid-aaa-border-top-1px-solid-aaa-border-bottom-1px-solid-aaa-ul-li-a-href-wiki-file-portal-puzzle-svg-class-image-img-alt-portal-puzzle-svg-src-upload-wikimedia-org-wikipedia-en-thumb-f-fd-portal-puzzle-svg-16px-portal-puzzle-svg-png-decoding-async-width-16-height-14-class-noviewer-srcset-upload-wikimedia-org-wikipedia-en-thumb-f-fd-portal-puzzle-svg-24px-portal-puzzle-svg-png-1-5x-upload-wikimedia-org-wikipedia-en-thumb-f-fd-portal-puzzle-svg-32px-portal-puzzle-svg-png-2x-data-file-width-32-data-file-height-28-a-a-href-wiki-portal-machine-learning-title-portal-machine-learning-machine-learning-portal-a-li-ul-td-tr-tr-td-style-text-align-right-font-size-115-padding-top-0-6em-div-class-plainlinks-hlist-navbar-mini-ul-li-class-nv-view-a-href-wiki-template-machine-learning-bar-title-template-machine-learning-bar-abbr-title-view-this-template-v-abbr-a-li-li-class-nv-talk-a-href-wiki-template-talk-machine-learning-bar-title-template-talk-machine-learning-bar-abbr-title-discuss-this-template-t-abbr-a-li-li-class-nv-edit-a-class-external-text-href-en-wikipedia-org-w-index-php-title-template-machine-learning-bar-action-edit-abbr-title-edit-this-template-e-abbr-a-li-ul-div-td-tr-tbody-table-p-in-a-href-wiki-deep-learning-title-deep-learning-deep-learning-a-a-b-convolutional-neural-network-b-b-cnn-b-or-b-convnet-b-is-a-class-of-a-href-wiki-deep-neural-network-class-mw-redirect-title-deep-neural-network-deep-neural-networks-a-most-commonly-applied-to-analyzing-visual-imagery-p-p-cnns-are-a-href-wiki-regularization-mathematics-title-regularization-mathematics-regularized-a-versions-of-a-href-wiki-multilayer-perceptron-title-multilayer-perceptron-multilayer-perceptrons-a-multilayer-perceptrons-usually-refer-to-fully-connected-networks-that-is-each-neuron-in-one-layer-is-connected-to-all-neurons-in-the-next-layer-the-fully-connectedness-of-these-networks-make-them-prone-to-overfitting-data-typical-ways-of-regularization-includes-adding-some-form-of-magnitude-measurement-of-weights-to-the-loss-function-however-cnns-take-a-different-approach-towards-regularization-they-take-advantage-of-the-hierarchical-pattern-in-data-and-assemble-more-complex-patterns-using-smaller-and-simpler-patterns-therefore-on-the-scale-of-connectedness-and-complexity-cnns-are-on-the-lower-extreme-p-p-they-are-also-known-as-b-shift-invariant-b-or-b-space-invariant-artificial-neural-networks-b-b-siann-b-based-on-their-shared-weights-architecture-and-a-href-wiki-translation-invariance-class-mw-redirect-title-translation-invariance-translation-invariance-a-characteristics-sup-id-cite-ref-0-1-0-class-reference-a-href-cite-note-0-1-1-a-sup-sup-id-cite-ref-1-2-0-class-reference-a-href-cite-note-1-2-2-a-sup-p-p-convolutional-networks-were-a-href-wiki-mathematical-biology-class-mw-redirect-title-mathematical-biology-inspired-a-by-a-href-wiki-biological-class-mw-redirect-title-biological-biological-a-processes-sup-id-cite-ref-fukuneoscholar-3-0-class-reference-a-href-cite-note-fukuneoscholar-3-3-a-sup-sup-id-cite-ref-hubelwiesel1968-4-0-class-reference-a-href-cite-note-hubelwiesel1968-4-4-a-sup-sup-id-cite-ref-intro-5-0-class-reference-a-href-cite-note-intro-5-5-a-sup-sup-id-cite-ref-robust-face-detection-6-0-class-reference-a-href-cite-note-robust-face-detection-6-6-a-sup-in-that-the-connectivity-pattern-between-a-href-wiki-artificial-neuron-title-artificial-neuron-neurons-a-resembles-the-organization-of-the-animal-a-href-wiki-visual-cortex-title-visual-cortex-visual-cortex-a-individual-a-href-wiki-cortical-neuron-class-mw-redirect-title-cortical-neuron-cortical-neurons-a-respond-to-stimuli-only-in-a-restricted-region-of-the-a-href-wiki-visual-field-title-visual-field-visual-field-a-known-as-the-a-href-wiki-receptive-field-title-receptive-field-receptive-field-a-the-receptive-fields-of-different-neurons-partially-overlap-such-that-they-cover-the-entire-visual-field-p-p-cnns-use-relatively-little-pre-processing-compared-to-other-a-href-wiki-image-classification-class-mw-redirect-title-image-classification-image-classification-algorithms-a-this-means-that-the-network-learns-the-a-href-wiki-filter-signal-processing-title-filter-signal-processing-filters-a-that-in-traditional-algorithms-were-a-href-wiki-feature-engineering-title-feature-engineering-hand-engineered-a-this-independence-from-prior-knowledge-and-human-effort-in-feature-design-is-a-major-advantage-p-p-they-have-applications-in-a-href-wiki-computer-vision-title-computer-vision-image-and-video-recognition-a-a-href-wiki-recommender-system-title-recommender-system-recommender-systems-a-sup-id-cite-ref-7-class-reference-a-href-cite-note-7-7-a-sup-a-href-wiki-image-classification-class-mw-redirect-title-image-classification-image-classification-a-a-href-wiki-medical-image-computing-title-medical-image-computing-medical-image-analysis-a-and-a-href-wiki-natural-language-processing-title-natural-language-processing-natural-language-processing-a-sup-id-cite-ref-8-class-reference-a-href-cite-note-8-8-a-sup-style-data-mw-deduplicate-templatestyles-r886046785-mw-parser-output-toclimit-2-toclevel-1-ul-mw-parser-output-toclimit-3-toclevel-2-ul-mw-parser-output-toclimit-4-toclevel-3-ul-mw-parser-output-toclimit-5-toclevel-4-ul-mw-parser-output-toclimit-6-toclevel-5-ul-mw-parser-output-toclimit-7-toclevel-6-ul-display-none-style-p-div-class-toclimit-3-div-id-toc-class-toc-input-type-checkbox-role-button-id-toctogglecheckbox-class-toctogglecheckbox-style-display-none-div-class-toctitle-lang-en-dir-ltr-h2-contents-h2-span-class-toctogglespan-label-class-toctogglelabel-for-toctogglecheckbox-label-span-div-ul-li-class-toclevel-1-tocsection-1-a-href-design-span-class-tocnumber-1-span-span-class-toctext-design-span-a-ul-li-class-toclevel-2-tocsection-2-a-href-convolutional-span-class-tocnumber-1-1-span-span-class-toctext-convolutional-span-a-li-li-class-toclevel-2-tocsection-3-a-href-pooling-span-class-tocnumber-1-2-span-span-class-toctext-pooling-span-a-li-li-class-toclevel-2-tocsection-4-a-href-fully-connected-span-class-tocnumber-1-3-span-span-class-toctext-fully-connected-span-a-li-li-class-toclevel-2-tocsection-5-a-href-receptive-field-span-class-tocnumber-1-4-span-span-class-toctext-receptive-field-span-a-li-li-class-toclevel-2-tocsection-6-a-href-weights-span-class-tocnumber-1-5-span-span-class-toctext-weights-span-a-li-ul-li-li-class-toclevel-1-tocsection-7-a-href-history-span-class-tocnumber-2-span-span-class-toctext-history-span-a-ul-li-class-toclevel-2-tocsection-8-a-href-receptive-fields-in-the-visual-cortex-span-class-tocnumber-2-1-span-span-class-toctext-receptive-fields-in-the-visual-cortex-span-a-li-li-class-toclevel-2-tocsection-9-a-href-neocognitron-origin-of-the-cnn-architecture-span-class-tocnumber-2-2-span-span-class-toctext-neocognitron-origin-of-the-cnn-architecture-span-a-li-li-class-toclevel-2-tocsection-10-a-href-time-delay-neural-networks-span-class-tocnumber-2-3-span-span-class-toctext-time-delay-neural-networks-span-a-li-li-class-toclevel-2-tocsection-11-a-href-image-recognition-with-cnns-trained-by-gradient-descent-span-class-tocnumber-2-4-span-span-class-toctext-image-recognition-with-cnns-trained-by-gradient-descent-span-a-ul-li-class-toclevel-3-tocsection-12-a-href-lenet-5-span-class-tocnumber-2-4-1-span-span-class-toctext-lenet-5-span-a-li-ul-li-li-class-toclevel-2-tocsection-13-a-href-shift-invariant-neural-network-span-class-tocnumber-2-5-span-span-class-toctext-shift-invariant-neural-network-span-a-li-li-class-toclevel-2-tocsection-14-a-href-neural-abstraction-pyramid-span-class-tocnumber-2-6-span-span-class-toctext-neural-abstraction-pyramid-span-a-li-li-class-toclevel-2-tocsection-15-a-href-gpu-implementations-span-class-tocnumber-2-7-span-span-class-toctext-gpu-implementations-span-a-li-li-class-toclevel-2-tocsection-16-a-href-intel-xeon-phi-implementations-span-class-tocnumber-2-8-span-span-class-toctext-intel-xeon-phi-implementations-span-a-li-ul-li-li-class-toclevel-1-tocsection-17-a-href-distinguishing-features-span-class-tocnumber-3-span-span-class-toctext-distinguishing-features-span-a-li-li-class-toclevel-1-tocsection-18-a-href-building-blocks-span-class-tocnumber-4-span-span-class-toctext-building-blocks-span-a-ul-li-class-toclevel-2-tocsection-19-a-href-convolutional-layer-span-class-tocnumber-4-1-span-span-class-toctext-convolutional-layer-span-a-ul-li-class-toclevel-3-tocsection-20-a-href-local-connectivity-span-class-tocnumber-4-1-1-span-span-class-toctext-local-connectivity-span-a-li-li-class-toclevel-3-tocsection-21-a-href-spatial-arrangement-span-class-tocnumber-4-1-2-span-span-class-toctext-spatial-arrangement-span-a-li-li-class-toclevel-3-tocsection-22-a-href-parameter-sharing-span-class-tocnumber-4-1-3-span-span-class-toctext-parameter-sharing-span-a-li-ul-li-li-class-toclevel-2-tocsection-23-a-href-pooling-layer-span-class-tocnumber-4-2-span-span-class-toctext-pooling-layer-span-a-li-li-class-toclevel-2-tocsection-24-a-href-relu-layer-span-class-tocnumber-4-3-span-span-class-toctext-relu-layer-span-a-li-li-class-toclevel-2-tocsection-25-a-href-fully-connected-layer-span-class-tocnumber-4-4-span-span-class-toctext-fully-connected-layer-span-a-li-li-class-toclevel-2-tocsection-26-a-href-loss-layer-span-class-tocnumber-4-5-span-span-class-toctext-loss-layer-span-a-li-ul-li-li-class-toclevel-1-tocsection-27-a-href-choosing-hyperparameters-span-class-tocnumber-5-span-span-class-toctext-choosing-hyperparameters-span-a-ul-li-class-toclevel-2-tocsection-28-a-href-number-of-filters-span-class-tocnumber-5-1-span-span-class-toctext-number-of-filters-span-a-li-li-class-toclevel-2-tocsection-29-a-href-filter-shape-span-class-tocnumber-5-2-span-span-class-toctext-filter-shape-span-a-li-li-class-toclevel-2-tocsection-30-a-href-max-pooling-shape-span-class-tocnumber-5-3-span-span-class-toctext-max-pooling-shape-span-a-li-ul-li-li-class-toclevel-1-tocsection-31-a-href-regularization-methods-span-class-tocnumber-6-span-span-class-toctext-regularization-methods-span-a-ul-li-class-toclevel-2-tocsection-32-a-href-empirical-span-class-tocnumber-6-1-span-span-class-toctext-empirical-span-a-ul-li-class-toclevel-3-tocsection-33-a-href-dropout-span-class-tocnumber-6-1-1-span-span-class-toctext-dropout-span-a-li-li-class-toclevel-3-tocsection-34-a-href-dropconnect-span-class-tocnumber-6-1-2-span-span-class-toctext-dropconnect-span-a-li-li-class-toclevel-3-tocsection-35-a-href-stochastic-pooling-span-class-tocnumber-6-1-3-span-span-class-toctext-stochastic-pooling-span-a-li-li-class-toclevel-3-tocsection-36-a-href-artificial-data-span-class-tocnumber-6-1-4-span-span-class-toctext-artificial-data-span-a-li-ul-li-li-class-toclevel-2-tocsection-37-a-href-explicit-span-class-tocnumber-6-2-span-span-class-toctext-explicit-span-a-ul-li-class-toclevel-3-tocsection-38-a-href-early-stopping-span-class-tocnumber-6-2-1-span-span-class-toctext-early-stopping-span-a-li-li-class-toclevel-3-tocsection-39-a-href-number-of-parameters-span-class-tocnumber-6-2-2-span-span-class-toctext-number-of-parameters-span-a-li-li-class-toclevel-3-tocsection-40-a-href-weight-decay-span-class-tocnumber-6-2-3-span-span-class-toctext-weight-decay-span-a-li-li-class-toclevel-3-tocsection-41-a-href-max-norm-constraints-span-class-tocnumber-6-2-4-span-span-class-toctext-max-norm-constraints-span-a-li-ul-li-ul-li-li-class-toclevel-1-tocsection-42-a-href-hierarchical-coordinate-frames-span-class-tocnumber-7-span-span-class-toctext-hierarchical-coordinate-frames-span-a-li-li-class-toclevel-1-tocsection-43-a-href-applications-span-class-tocnumber-8-span-span-class-toctext-applications-span-a-ul-li-class-toclevel-2-tocsection-44-a-href-image-recognition-span-class-tocnumber-8-1-span-span-class-toctext-image-recognition-span-a-li-li-class-toclevel-2-tocsection-45-a-href-video-analysis-span-class-tocnumber-8-2-span-span-class-toctext-video-analysis-span-a-li-li-class-toclevel-2-tocsection-46-a-href-natural-language-processing-span-class-tocnumber-8-3-span-span-class-toctext-natural-language-processing-span-a-li-li-class-toclevel-2-tocsection-47-a-href-drug-discovery-span-class-tocnumber-8-4-span-span-class-toctext-drug-discovery-span-a-li-li-class-toclevel-2-tocsection-48-a-href-health-risk-assessment-and-biomarkers-of-aging-discovery-span-class-tocnumber-8-5-span-span-class-toctext-health-risk-assessment-and-biomarkers-of-aging-discovery-span-a-li-li-class-toclevel-2-tocsection-49-a-href-checkers-game-span-class-tocnumber-8-6-span-span-class-toctext-checkers-game-span-a-li-li-class-toclevel-2-tocsection-50-a-href-go-span-class-tocnumber-8-7-span-span-class-toctext-go-span-a-li-ul-li-li-class-toclevel-1-tocsection-51-a-href-fine-tuning-span-class-tocnumber-9-span-span-class-toctext-fine-tuning-span-a-li-li-class-toclevel-1-tocsection-52-a-href-human-interpretable-explanations-span-class-tocnumber-10-span-span-class-toctext-human-interpretable-explanations-span-a-li-li-class-toclevel-1-tocsection-53-a-href-related-architectures-span-class-tocnumber-11-span-span-class-toctext-related-architectures-span-a-ul-li-class-toclevel-2-tocsection-54-a-href-deep-q-networks-span-class-tocnumber-11-1-span-span-class-toctext-deep-q-networks-span-a-li-li-class-toclevel-2-tocsection-55-a-href-deep-belief-networks-span-class-tocnumber-11-2-span-span-class-toctext-deep-belief-networks-span-a-li-ul-li-li-class-toclevel-1-tocsection-56-a-href-notable-libraries-span-class-tocnumber-12-span-span-class-toctext-notable-libraries-span-a-li-li-class-toclevel-1-tocsection-57-a-href-notable-apis-span-class-tocnumber-13-span-span-class-toctext-notable-apis-span-a-li-li-class-toclevel-1-tocsection-58-a-href-see-also-span-class-tocnumber-14-span-span-class-toctext-see-also-span-a-li-li-class-toclevel-1-tocsection-59-a-href-notes-span-class-tocnumber-15-span-span-class-toctext-notes-span-a-li-li-class-toclevel-1-tocsection-60-a-href-references-span-class-tocnumber-16-span-span-class-toctext-references-span-a-li-li-class-toclevel-1-tocsection-61-a-href-external-links-span-class-tocnumber-17-span-span-class-toctext-external-links-span-a-li-ul-div-div-h2-span-class-mw-headline-id-design-design-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-1-title-edit-section-design-edit-a-span-class-mw-editsection-bracket-span-span-h2-p-a-convolutional-neural-network-consists-of-an-input-and-an-output-layer-as-well-as-multiple-a-href-wiki-multilayer-perceptron-layers-title-multilayer-perceptron-hidden-layers-a-the-hidden-layers-of-a-cnn-typically-consist-of-a-series-of-convolutional-layers-that-i-convolve-i-with-a-multiplication-or-other-a-href-wiki-dot-product-title-dot-product-dot-product-a-the-activation-function-is-commonly-a-a-href-wiki-rectifier-neural-networks-title-rectifier-neural-networks-relu-layer-a-and-is-subsequently-followed-by-additional-convolutions-such-as-pooling-layers-fully-connected-layers-and-normalization-layers-referred-to-as-hidden-layers-because-their-inputs-and-outputs-are-masked-by-the-activation-function-and-final-a-href-wiki-convolution-title-convolution-convolution-a-the-final-convolution-in-turn-often-involves-a-href-wiki-backpropagation-title-backpropagation-backpropagation-a-in-order-to-more-accurately-weight-the-end-product-sup-id-cite-ref-9-class-reference-a-href-cite-note-9-9-a-sup-p-p-though-the-layers-are-colloquially-referred-to-as-convolutions-this-is-only-by-convention-mathematically-it-is-technically-a-i-sliding-dot-product-i-or-a-href-wiki-cross-correlation-title-cross-correlation-cross-correlation-a-this-has-significance-for-the-indices-in-the-matrix-in-that-it-affects-how-weight-is-determined-at-a-specific-index-point-sup-class-noprint-inline-template-template-fact-style-white-space-nowrap-i-a-href-wiki-wikipedia-citation-needed-title-wikipedia-citation-needed-span-title-this-claim-needs-references-to-reliable-sources-june-2019-citation-needed-span-a-i-sup-p-h3-span-class-mw-headline-id-convolutional-convolutional-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-2-title-edit-section-convolutional-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-when-programming-a-cnn-each-convolutional-layer-within-a-neural-network-should-have-the-following-attributes-p-ul-li-input-is-a-tensor-with-shape-number-of-images-x-image-width-x-image-height-x-image-depth-li-li-convolutional-kernels-whose-width-and-height-are-hyper-parameters-and-whose-depth-must-be-equal-to-that-of-the-image-convolutional-layers-convolve-the-input-and-pass-its-result-to-the-next-layer-this-is-similar-to-the-response-of-a-neuron-in-the-visual-cortex-to-a-specific-stimulus-sup-id-cite-ref-deeplearning-10-0-class-reference-a-href-cite-note-deeplearning-10-10-a-sup-li-ul-p-each-convolutional-neuron-processes-data-only-for-its-a-href-wiki-receptive-field-title-receptive-field-receptive-field-a-although-a-href-wiki-multilayer-perceptron-title-multilayer-perceptron-fully-connected-feedforward-neural-networks-a-can-be-used-to-learn-features-as-well-as-classify-data-it-is-not-practical-to-apply-this-architecture-to-images-a-very-high-number-of-neurons-would-be-necessary-even-in-a-shallow-opposite-of-deep-architecture-due-to-the-very-large-input-sizes-associated-with-images-where-each-pixel-is-a-relevant-variable-for-instance-a-fully-connected-layer-for-a-small-image-of-size-100-x-100-has-10000-weights-for-i-each-i-neuron-in-the-second-layer-the-convolution-operation-brings-a-solution-to-this-problem-as-it-reduces-the-number-of-free-parameters-allowing-the-network-to-be-deeper-with-fewer-parameters-sup-id-cite-ref-11-class-reference-a-href-cite-note-11-11-a-sup-for-instance-regardless-of-image-size-tiling-regions-of-size-5-x-5-each-with-the-same-shared-weights-requires-only-25-learnable-parameters-in-this-way-it-resolves-the-vanishing-or-exploding-gradients-problem-in-training-traditional-multi-layer-neural-networks-with-many-layers-by-using-a-href-wiki-backpropagation-title-backpropagation-backpropagation-a-sup-class-noprint-inline-template-template-fact-style-white-space-nowrap-i-a-href-wiki-wikipedia-citation-needed-title-wikipedia-citation-needed-span-title-this-claim-needs-references-to-reliable-sources-october-2017-citation-needed-span-a-i-sup-p-h3-span-class-mw-headline-id-pooling-pooling-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-3-title-edit-section-pooling-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-convolutional-networks-may-include-local-or-global-pooling-layers-to-streamline-the-underlying-computation-pooling-layers-reduce-the-dimensions-of-the-data-by-combining-the-outputs-of-neuron-clusters-at-one-layer-into-a-single-neuron-in-the-next-layer-local-pooling-combines-small-clusters-typically-2-x-2-global-pooling-acts-on-all-the-neurons-of-the-convolutional-layer-sup-id-cite-ref-flexible-12-0-class-reference-a-href-cite-note-flexible-12-12-a-sup-sup-id-cite-ref-13-class-reference-a-href-cite-note-13-13-a-sup-in-addition-pooling-may-compute-a-max-or-an-average-m-i-ax-pooling-i-uses-the-maximum-value-from-each-of-a-cluster-of-neurons-at-the-prior-layer-sup-id-cite-ref-mcdns-14-0-class-reference-a-href-cite-note-mcdns-14-14-a-sup-a-i-verage-pooling-i-uses-the-average-value-from-each-of-a-cluster-of-neurons-at-the-prior-layer-sup-id-cite-ref-cnnbackground-15-0-class-reference-a-href-cite-note-cnnbackground-15-15-a-sup-p-h3-span-class-mw-headline-id-fully-connected-fully-connected-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-4-title-edit-section-fully-connected-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-fully-connected-layers-connect-every-neuron-in-one-layer-to-every-neuron-in-another-layer-it-is-in-principle-the-same-as-the-traditional-a-href-wiki-multi-layer-perceptron-class-mw-redirect-title-multi-layer-perceptron-multi-layer-perceptron-a-neural-network-mlp-the-flattened-matrix-goes-through-a-fully-connected-layer-to-classify-the-images-p-h3-span-class-mw-headline-id-receptive-field-receptive-field-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-5-title-edit-section-receptive-field-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-in-neural-networks-each-neuron-receives-input-from-some-number-of-locations-in-the-previous-layer-in-a-fully-connected-layer-each-neuron-receives-input-from-i-every-i-element-of-the-previous-layer-in-a-convolutional-layer-neurons-receive-input-from-only-a-restricted-subarea-of-the-previous-layer-typically-the-subarea-is-of-a-square-shape-e-g-size-5-by-5-the-input-area-of-a-neuron-is-called-its-i-receptive-field-i-so-in-a-fully-connected-layer-the-receptive-field-is-the-entire-previous-layer-in-a-convolutional-layer-the-receptive-area-is-smaller-than-the-entire-previous-layer-p-h3-span-class-mw-headline-id-weights-weights-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-6-title-edit-section-weights-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-each-neuron-in-a-neural-network-computes-an-output-value-by-applying-a-specific-function-to-the-input-values-coming-from-the-receptive-field-in-the-previous-layer-the-function-that-is-applied-to-the-input-values-is-determined-by-a-vector-of-weights-and-a-bias-typically-real-numbers-learning-in-a-neural-network-progresses-by-making-iterative-adjustments-to-these-biases-and-weights-p-p-the-vector-of-weights-and-the-bias-are-called-i-filters-i-and-represent-particular-a-href-wiki-feature-machine-learning-title-feature-machine-learning-features-a-of-the-input-e-g-a-particular-shape-a-distinguishing-feature-of-cnns-is-that-many-neurons-can-share-the-same-filter-this-reduces-a-href-wiki-memory-footprint-title-memory-footprint-memory-footprint-a-because-a-single-bias-and-a-single-vector-of-weights-is-used-across-all-receptive-fields-sharing-that-filter-as-opposed-to-each-receptive-field-having-its-own-bias-and-vector-weighting-sup-id-cite-ref-lecun-16-0-class-reference-a-href-cite-note-lecun-16-16-a-sup-p-h2-span-class-mw-headline-id-history-history-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-7-title-edit-section-history-edit-a-span-class-mw-editsection-bracket-span-span-h2-p-cnn-design-follows-vision-processing-in-a-href-wiki-living-organisms-class-mw-redirect-title-living-organisms-living-organisms-a-sup-class-noprint-inline-template-template-fact-style-white-space-nowrap-i-a-href-wiki-wikipedia-citation-needed-title-wikipedia-citation-needed-span-title-this-claim-needs-references-to-reliable-sources-october-2017-citation-needed-span-a-i-sup-p-h3-span-class-mw-headline-id-receptive-fields-in-the-visual-cortex-receptive-fields-in-the-visual-cortex-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-8-title-edit-section-receptive-fields-in-the-visual-cortex-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-work-by-a-href-wiki-david-h-hubel-title-david-h-hubel-hubel-a-and-a-href-wiki-torsten-wiesel-title-torsten-wiesel-wiesel-a-in-the-1950s-and-1960s-showed-that-cat-and-monkey-visual-a-href-wiki-cortex-anatomy-title-cortex-anatomy-cortexes-a-contain-neurons-that-individually-respond-to-small-regions-of-the-a-href-wiki-visual-field-title-visual-field-visual-field-a-provided-the-eyes-are-not-moving-the-region-of-visual-space-within-which-visual-stimuli-affect-the-firing-of-a-single-neuron-is-known-as-its-b-a-href-wiki-receptive-field-title-receptive-field-receptive-field-a-b-sup-class-noprint-inline-template-template-fact-style-white-space-nowrap-i-a-href-wiki-wikipedia-citation-needed-title-wikipedia-citation-needed-span-title-this-claim-needs-references-to-reliable-sources-october-2017-citation-needed-span-a-i-sup-neighboring-cells-have-similar-and-overlapping-receptive-fields-sup-class-noprint-inline-template-template-fact-style-white-space-nowrap-i-a-href-wiki-wikipedia-citation-needed-title-wikipedia-citation-needed-span-title-this-claim-needs-references-to-reliable-sources-october-2017-citation-needed-span-a-i-sup-receptive-field-size-and-location-varies-systematically-across-the-cortex-to-form-a-complete-map-of-visual-space-sup-class-noprint-inline-template-template-fact-style-white-space-nowrap-i-a-href-wiki-wikipedia-citation-needed-title-wikipedia-citation-needed-span-title-this-claim-needs-references-to-reliable-sources-october-2017-citation-needed-span-a-i-sup-the-cortex-in-each-hemisphere-represents-the-contralateral-a-href-wiki-visual-field-title-visual-field-visual-field-a-sup-class-noprint-inline-template-template-fact-style-white-space-nowrap-i-a-href-wiki-wikipedia-citation-needed-title-wikipedia-citation-needed-span-title-this-claim-needs-references-to-reliable-sources-october-2017-citation-needed-span-a-i-sup-p-p-their-1968-paper-identified-two-basic-visual-cell-types-in-the-brain-sup-id-cite-ref-hubelwiesel1968-4-1-class-reference-a-href-cite-note-hubelwiesel1968-4-4-a-sup-p-ul-li-a-href-wiki-simple-cells-visual-cortex-class-mw-redirect-title-simple-cells-visual-cortex-simple-cells-a-whose-output-is-maximized-by-straight-edges-having-particular-orientations-within-their-receptive-field-li-li-a-href-wiki-complex-cells-visual-cortex-class-mw-redirect-title-complex-cells-visual-cortex-complex-cells-a-which-have-larger-a-href-wiki-receptive-field-title-receptive-field-receptive-fields-a-whose-output-is-insensitive-to-the-exact-position-of-the-edges-in-the-field-li-ul-p-hubel-and-wiesel-also-proposed-a-cascading-model-of-these-two-types-of-cells-for-use-in-pattern-recognition-tasks-sup-id-cite-ref-17-class-reference-a-href-cite-note-17-17-a-sup-sup-id-cite-ref-18-class-reference-a-href-cite-note-18-18-a-sup-p-h3-span-id-neocognitron-2c-origin-of-the-cnn-architecture-span-span-class-mw-headline-id-neocognitron-origin-of-the-cnn-architecture-neocognitron-origin-of-the-cnn-architecture-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-9-title-edit-section-neocognitron-origin-of-the-cnn-architecture-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-the-a-href-wiki-neocognitron-title-neocognitron-neocognitron-a-sup-id-cite-ref-fukuneoscholar-3-1-class-reference-a-href-cite-note-fukuneoscholar-3-3-a-sup-was-introduced-by-a-href-wiki-kunihiko-fukushima-title-kunihiko-fukushima-kunihiko-fukushima-a-in-1980-sup-id-cite-ref-intro-5-1-class-reference-a-href-cite-note-intro-5-5-a-sup-sup-id-cite-ref-mcdns-14-1-class-reference-a-href-cite-note-mcdns-14-14-a-sup-sup-id-cite-ref-19-class-reference-a-href-cite-note-19-19-a-sup-it-was-inspired-by-the-above-mentioned-work-of-hubel-and-wiesel-the-neocognitron-introduced-the-two-basic-types-of-layers-in-cnns-convolutional-layers-and-downsampling-layers-a-convolutional-layer-contains-units-whose-receptive-fields-cover-a-patch-of-the-previous-layer-the-weight-vector-the-set-of-adaptive-parameters-of-such-a-unit-is-often-called-a-filter-units-can-share-filters-downsampling-layers-contain-units-whose-receptive-fields-cover-patches-of-previous-convolutional-layers-such-a-unit-typically-computes-the-average-of-the-activations-of-the-units-in-its-patch-this-downsampling-helps-to-correctly-classify-objects-in-visual-scenes-even-when-the-objects-are-shifted-p-p-in-a-variant-of-the-neocognitron-called-the-cresceptron-instead-of-using-fukushima-s-spatial-averaging-j-weng-et-al-introduced-a-method-called-max-pooling-where-a-downsampling-unit-computes-the-maximum-of-the-activations-of-the-units-in-its-patch-sup-id-cite-ref-weng1993-20-0-class-reference-a-href-cite-note-weng1993-20-20-a-sup-max-pooling-is-often-used-in-modern-cnns-sup-id-cite-ref-schdeepscholar-21-0-class-reference-a-href-cite-note-schdeepscholar-21-21-a-sup-p-p-several-supervised-and-unsupervised-learning-algorithms-have-been-proposed-over-the-decades-to-train-the-weights-of-a-neocognitron-sup-id-cite-ref-fukuneoscholar-3-2-class-reference-a-href-cite-note-fukuneoscholar-3-3-a-sup-today-however-the-cnn-architecture-is-usually-trained-through-a-href-wiki-backpropagation-title-backpropagation-backpropagation-a-p-p-the-a-href-wiki-neocognitron-title-neocognitron-neocognitron-a-is-the-first-cnn-which-requires-units-located-at-multiple-network-positions-to-have-shared-weights-neocognitrons-were-adapted-in-1988-to-analyze-time-varying-signals-sup-id-cite-ref-22-class-reference-a-href-cite-note-22-22-a-sup-p-h3-span-class-mw-headline-id-time-delay-neural-networks-time-delay-neural-networks-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-10-title-edit-section-time-delay-neural-networks-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-the-a-href-wiki-time-delay-neural-network-title-time-delay-neural-network-time-delay-neural-network-a-tdnn-was-introduced-in-1987-by-a-href-wiki-alex-waibel-title-alex-waibel-alex-waibel-a-et-al-and-was-the-first-convolutional-network-as-it-achieved-shift-invariance-sup-id-cite-ref-waibel1987-23-0-class-reference-a-href-cite-note-waibel1987-23-23-a-sup-it-did-so-by-utilizing-weight-sharing-in-combination-with-a-href-wiki-backpropagation-title-backpropagation-backpropagation-a-training-sup-id-cite-ref-speechsignal-24-0-class-reference-a-href-cite-note-speechsignal-24-24-a-sup-thus-while-also-using-a-pyramidal-structure-as-in-the-neocognitron-it-performed-a-global-optimization-of-the-weights-instead-of-a-local-one-sup-id-cite-ref-waibel1987-23-1-class-reference-a-href-cite-note-waibel1987-23-23-a-sup-p-p-tdnns-are-convolutional-networks-that-share-weights-along-the-temporal-dimension-sup-id-cite-ref-25-class-reference-a-href-cite-note-25-25-a-sup-they-allow-speech-signals-to-be-processed-time-invariantly-this-inspired-translation-invariance-in-image-processing-with-cnns-sup-id-cite-ref-speechsignal-24-1-class-reference-a-href-cite-note-speechsignal-24-24-a-sup-the-tiling-of-neuron-outputs-can-cover-timed-stages-sup-id-cite-ref-video-quality-26-0-class-reference-a-href-cite-note-video-quality-26-26-a-sup-p-p-tdnns-now-achieve-the-best-performance-in-far-distance-speech-recognition-sup-id-cite-ref-ko2017-27-0-class-reference-a-href-cite-note-ko2017-27-27-a-sup-p-h3-span-class-mw-headline-id-image-recognition-with-cnns-trained-by-gradient-descent-image-recognition-with-cnns-trained-by-gradient-descent-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-11-title-edit-section-image-recognition-with-cnns-trained-by-gradient-descent-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-a-system-to-recognize-hand-written-a-href-wiki-zip-code-title-zip-code-zip-code-a-numbers-sup-id-cite-ref-28-class-reference-a-href-cite-note-28-28-a-sup-involved-convolutions-in-which-the-kernel-coefficients-had-been-laboriously-hand-designed-sup-id-cite-ref-2-29-0-class-reference-a-href-cite-note-2-29-29-a-sup-p-p-a-href-wiki-yann-lecun-title-yann-lecun-yann-lecun-a-et-al-1989-sup-id-cite-ref-2-29-1-class-reference-a-href-cite-note-2-29-29-a-sup-used-back-propagation-to-learn-the-convolution-kernel-coefficients-directly-from-images-of-hand-written-numbers-learning-was-thus-fully-automatic-performed-better-than-manual-coefficient-design-and-was-suited-to-a-broader-range-of-image-recognition-problems-and-image-types-p-p-this-approach-became-a-foundation-of-modern-a-href-wiki-computer-vision-title-computer-vision-computer-vision-a-p-h4-span-class-mw-headline-id-lenet-5-lenet-5-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-12-title-edit-section-lenet-5-edit-a-span-class-mw-editsection-bracket-span-span-h4-p-lenet-5-a-pioneering-7-level-convolutional-network-by-a-href-wiki-yann-lecun-title-yann-lecun-lecun-a-et-al-in-1998-sup-id-cite-ref-lecun98-30-0-class-reference-a-href-cite-note-lecun98-30-30-a-sup-that-classifies-digits-was-applied-by-several-banks-to-recognize-hand-written-numbers-on-checks-a-href-wiki-british-english-language-class-mw-redirect-title-british-english-language-british-english-a-span-lang-en-gb-cheques-span-digitized-in-32x32-pixel-images-the-ability-to-process-higher-resolution-images-requires-larger-and-more-layers-of-convolutional-neural-networks-so-this-technique-is-constrained-by-the-availability-of-computing-resources-p-h3-span-class-mw-headline-id-shift-invariant-neural-network-shift-invariant-neural-network-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-13-title-edit-section-shift-invariant-neural-network-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-similarly-a-shift-invariant-neural-network-was-proposed-by-w-zhang-et-al-for-image-character-recognition-in-1988-sup-id-cite-ref-0-1-1-class-reference-a-href-cite-note-0-1-1-a-sup-sup-id-cite-ref-1-2-1-class-reference-a-href-cite-note-1-2-2-a-sup-the-architecture-and-training-algorithm-were-modified-in-1991-sup-id-cite-ref-31-class-reference-a-href-cite-note-31-31-a-sup-and-applied-for-medical-image-processing-sup-id-cite-ref-32-class-reference-a-href-cite-note-32-32-a-sup-and-automatic-detection-of-breast-cancer-in-a-href-wiki-mammography-title-mammography-mammograms-a-sup-id-cite-ref-33-class-reference-a-href-cite-note-33-33-a-sup-p-p-a-different-convolution-based-design-was-proposed-in-1988-sup-id-cite-ref-34-class-reference-a-href-cite-note-34-34-a-sup-for-application-to-decomposition-of-one-dimensional-a-href-wiki-electromyography-title-electromyography-electromyography-a-convolved-signals-via-de-convolution-this-design-was-modified-in-1989-to-other-de-convolution-based-designs-sup-id-cite-ref-35-class-reference-a-href-cite-note-35-35-a-sup-sup-id-cite-ref-36-class-reference-a-href-cite-note-36-36-a-sup-p-h3-span-class-mw-headline-id-neural-abstraction-pyramid-neural-abstraction-pyramid-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-14-title-edit-section-neural-abstraction-pyramid-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-the-feed-forward-architecture-of-convolutional-neural-networks-was-extended-in-the-neural-abstraction-pyramid-sup-id-cite-ref-37-class-reference-a-href-cite-note-37-37-a-sup-by-lateral-and-feedback-connections-sup-class-noprint-inline-template-style-white-space-nowrap-i-a-href-wiki-wikipedia-please-clarify-title-wikipedia-please-clarify-span-title-an-example-or-at-least-elaboration-is-needed-december-2018-further-explanation-needed-span-a-i-sup-the-resulting-recurrent-convolutional-network-allows-for-the-flexible-incorporation-of-contextual-information-to-iteratively-resolve-local-ambiguities-in-contrast-to-previous-models-image-like-outputs-at-the-highest-resolution-were-generated-p-h3-span-class-mw-headline-id-gpu-implementations-gpu-implementations-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-15-title-edit-section-gpu-implementations-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-although-cnns-were-invented-in-the-1980s-their-breakthrough-in-the-2000s-required-fast-implementations-on-a-href-wiki-graphics-processing-unit-class-mw-redirect-title-graphics-processing-unit-graphics-processing-units-a-or-a-href-wiki-gpu-class-mw-redirect-title-gpu-gpus-a-p-p-in-2004-it-was-shown-by-k-s-oh-and-k-jung-that-standard-neural-networks-can-be-greatly-accelerated-on-gpus-their-implementation-was-20-times-faster-than-an-equivalent-implementation-on-a-href-wiki-cpu-class-mw-redirect-title-cpu-cpu-a-sup-id-cite-ref-38-class-reference-a-href-cite-note-38-38-a-sup-sup-id-cite-ref-schdeepscholar-21-1-class-reference-a-href-cite-note-schdeepscholar-21-21-a-sup-in-2005-another-paper-also-emphasised-the-value-of-a-href-wiki-gpgpu-class-mw-redirect-title-gpgpu-gpgpu-a-for-a-href-wiki-machine-learning-title-machine-learning-machine-learning-a-sup-id-cite-ref-39-class-reference-a-href-cite-note-39-39-a-sup-p-p-the-first-gpu-implementation-of-a-cnn-was-described-in-2006-by-k-chellapilla-et-al-their-implementation-was-4-times-faster-than-an-equivalent-implementation-on-cpu-sup-id-cite-ref-40-class-reference-a-href-cite-note-40-40-a-sup-subsequent-work-also-used-gpus-initially-for-other-types-of-neural-networks-different-from-cnns-especially-unsupervised-neural-networks-sup-id-cite-ref-41-class-reference-a-href-cite-note-41-41-a-sup-sup-id-cite-ref-42-class-reference-a-href-cite-note-42-42-a-sup-sup-id-cite-ref-43-class-reference-a-href-cite-note-43-43-a-sup-sup-id-cite-ref-44-class-reference-a-href-cite-note-44-44-a-sup-p-p-in-2010-dan-ciresan-et-al-at-a-href-wiki-idsia-class-mw-redirect-title-idsia-idsia-a-showed-that-even-deep-standard-neural-networks-with-many-layers-can-be-quickly-trained-on-gpu-by-supervised-learning-through-the-old-method-known-as-a-href-wiki-backpropagation-title-backpropagation-backpropagation-a-their-network-outperformed-previous-machine-learning-methods-on-the-a-href-wiki-mnist-class-mw-redirect-title-mnist-mnist-a-handwritten-digits-benchmark-sup-id-cite-ref-45-class-reference-a-href-cite-note-45-45-a-sup-in-2011-they-extended-this-gpu-approach-to-cnns-achieving-an-acceleration-factor-of-60-with-impressive-results-sup-id-cite-ref-flexible-12-1-class-reference-a-href-cite-note-flexible-12-12-a-sup-in-2011-they-used-such-cnns-on-gpu-to-win-an-image-recognition-contest-where-they-achieved-superhuman-performance-for-the-first-time-sup-id-cite-ref-46-class-reference-a-href-cite-note-46-46-a-sup-between-may-15-2011-and-september-30-2012-their-cnns-won-no-less-than-four-image-competitions-sup-id-cite-ref-47-class-reference-a-href-cite-note-47-47-a-sup-sup-id-cite-ref-schdeepscholar-21-2-class-reference-a-href-cite-note-schdeepscholar-21-21-a-sup-in-2012-they-also-significantly-improved-on-the-best-performance-in-the-literature-for-multiple-image-a-href-wiki-database-title-database-databases-a-including-the-a-href-wiki-mnist-database-title-mnist-database-mnist-database-a-the-norb-database-the-hwdb1-0-dataset-chinese-characters-and-the-a-href-wiki-cifar-10-title-cifar-10-cifar10-dataset-a-dataset-of-60000-32x32-labeled-a-href-wiki-rgb-images-class-mw-redirect-title-rgb-images-rgb-images-a-sup-id-cite-ref-mcdns-14-2-class-reference-a-href-cite-note-mcdns-14-14-a-sup-p-p-subsequently-a-similar-gpu-based-cnn-by-alex-krizhevsky-et-al-won-the-a-href-wiki-imagenet-large-scale-visual-recognition-challenge-class-mw-redirect-title-imagenet-large-scale-visual-recognition-challenge-imagenet-large-scale-visual-recognition-challenge-a-2012-sup-id-cite-ref-02-48-0-class-reference-a-href-cite-note-02-48-48-a-sup-a-very-deep-cnn-with-over-100-layers-by-microsoft-won-the-imagenet-2015-contest-sup-id-cite-ref-49-class-reference-a-href-cite-note-49-49-a-sup-p-h3-span-class-mw-headline-id-intel-xeon-phi-implementations-intel-xeon-phi-implementations-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-16-title-edit-section-intel-xeon-phi-implementations-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-compared-to-the-training-of-cnns-using-gpus-not-much-attention-was-given-to-the-intel-xeon-phi-coprocessor-sup-id-cite-ref-50-class-reference-a-href-cite-note-50-50-a-sup-a-notable-development-is-a-parallelization-method-for-training-convolutional-neural-networks-on-the-intel-xeon-phi-named-controlled-hogwild-with-arbitrary-order-of-synchronization-chaos-sup-id-cite-ref-51-class-reference-a-href-cite-note-51-51-a-sup-chaos-exploits-both-the-thread-and-simd-level-parallelism-that-is-available-on-the-intel-xeon-phi-p-h2-span-class-mw-headline-id-distinguishing-features-distinguishing-features-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-17-title-edit-section-distinguishing-features-edit-a-span-class-mw-editsection-bracket-span-span-h2-p-in-the-past-traditional-a-href-wiki-multilayer-perceptron-title-multilayer-perceptron-multilayer-perceptron-a-mlp-models-have-been-used-for-image-recognition-sup-class-noprint-inline-template-style-white-space-nowrap-i-a-href-wiki-wikipedia-audience-class-mw-redirect-title-wikipedia-audience-span-title-an-editor-has-requested-that-an-example-be-provided-october-2017-example-needed-span-a-i-sup-however-due-to-the-full-connectivity-between-nodes-they-suffered-from-the-a-href-wiki-curse-of-dimensionality-title-curse-of-dimensionality-curse-of-dimensionality-a-and-did-not-scale-well-with-higher-resolution-images-a-1000x1000-pixel-image-with-a-href-wiki-rgb-color-model-title-rgb-color-model-rgb-color-a-channels-has-3-million-weights-which-is-too-high-to-feasibly-process-efficiently-at-scale-with-full-connectivity-p-div-class-thumb-tleft-div-class-thumbinner-style-width-239px-a-href-wiki-file-conv-layers-png-class-image-img-alt-src-upload-wikimedia-org-wikipedia-commons-thumb-8-8a-conv-layers-png-237px-conv-layers-png-decoding-async-width-237-height-130-class-thumbimage-srcset-upload-wikimedia-org-wikipedia-commons-thumb-8-8a-conv-layers-png-356px-conv-layers-png-1-5x-upload-wikimedia-org-wikipedia-commons-thumb-8-8a-conv-layers-png-474px-conv-layers-png-2x-data-file-width-567-data-file-height-310-a-div-class-thumbcaption-div-class-magnify-a-href-wiki-file-conv-layers-png-class-internal-title-enlarge-a-div-cnn-layers-arranged-in-3-dimensions-div-div-div-p-for-example-in-a-href-wiki-cifar-10-title-cifar-10-cifar-10-a-images-are-only-of-size-32x32x3-32-wide-32-high-3-color-channels-so-a-single-fully-connected-neuron-in-a-first-hidden-layer-of-a-regular-neural-network-would-have-32-32-3-3072-weights-a-200x200-image-however-would-lead-to-neurons-that-have-200-200-3-120000-weights-p-p-also-such-network-architecture-does-not-take-into-account-the-spatial-structure-of-data-treating-input-pixels-which-are-far-apart-in-the-same-way-as-pixels-that-are-close-together-this-ignores-a-href-wiki-locality-of-reference-title-locality-of-reference-locality-of-reference-a-in-image-data-both-computationally-and-semantically-thus-full-connectivity-of-neurons-is-wasteful-for-purposes-such-as-image-recognition-that-are-dominated-by-a-href-wiki-spatial-locality-class-mw-redirect-title-spatial-locality-spatially-local-a-input-patterns-p-p-convolutional-neural-networks-are-biologically-inspired-variants-of-multilayer-perceptrons-that-are-designed-to-emulate-the-behavior-of-a-a-href-wiki-visual-cortex-title-visual-cortex-visual-cortex-a-sup-class-noprint-inline-template-template-fact-style-white-space-nowrap-i-a-href-wiki-wikipedia-citation-needed-title-wikipedia-citation-needed-span-title-this-claim-needs-references-to-reliable-sources-october-2017-citation-needed-span-a-i-sup-these-models-mitigate-the-challenges-posed-by-the-mlp-architecture-by-exploiting-the-strong-spatially-local-correlation-present-in-natural-images-as-opposed-to-mlps-cnns-have-the-following-distinguishing-features-p-ul-li-b-3d-volumes-of-neurons-b-the-layers-of-a-cnn-have-neurons-arranged-in-a-href-wiki-three-dimensional-space-title-three-dimensional-space-3-dimensions-a-width-height-and-depth-sup-class-noprint-inline-template-template-fact-style-white-space-nowrap-i-a-href-wiki-wikipedia-citation-needed-title-wikipedia-citation-needed-span-title-this-claim-needs-references-to-reliable-sources-march-2019-citation-needed-span-a-i-sup-the-neurons-inside-a-layer-are-connected-to-only-a-small-region-of-the-layer-before-it-called-a-receptive-field-distinct-types-of-layers-both-locally-and-completely-connected-are-stacked-to-form-a-cnn-architecture-li-li-b-local-connectivity-b-following-the-concept-of-receptive-fields-cnns-exploit-spatial-locality-by-enforcing-a-local-connectivity-pattern-between-neurons-of-adjacent-layers-the-architecture-thus-ensures-that-the-learned-a-href-wiki-filter-signal-processing-title-filter-signal-processing-filters-a-produce-the-strongest-response-to-a-spatially-local-input-pattern-stacking-many-such-layers-leads-to-a-href-wiki-nonlinear-filter-title-nonlinear-filter-non-linear-filters-a-that-become-increasingly-global-i-e-responsive-to-a-larger-region-of-pixel-space-so-that-the-network-first-creates-representations-of-small-parts-of-the-input-then-from-them-assembles-representations-of-larger-areas-li-li-b-shared-weights-b-in-cnns-each-filter-is-replicated-across-the-entire-visual-field-these-replicated-units-share-the-same-parameterization-weight-vector-and-bias-and-form-a-feature-map-this-means-that-all-the-neurons-in-a-given-convolutional-layer-respond-to-the-same-feature-within-their-specific-response-field-replicating-units-in-this-way-allows-for-features-to-be-detected-regardless-of-their-position-in-the-visual-field-thus-constituting-a-property-of-a-href-wiki-translational-symmetry-title-translational-symmetry-translation-invariance-a-li-ul-p-together-these-properties-allow-cnns-to-achieve-better-generalization-on-a-href-wiki-computer-vision-title-computer-vision-vision-problems-a-weight-sharing-dramatically-reduces-the-number-of-a-href-wiki-free-parameter-title-free-parameter-free-parameters-a-learned-thus-lowering-the-memory-requirements-for-running-the-network-and-allowing-the-training-of-larger-more-powerful-networks-p-h2-span-class-mw-headline-id-building-blocks-building-blocks-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-18-title-edit-section-building-blocks-edit-a-span-class-mw-editsection-bracket-span-span-h2-table-class-box-more-citations-needed-section-plainlinks-metadata-ambox-ambox-content-ambox-refimprove-role-presentation-tbody-tr-td-class-mbox-image-div-style-width-52px-a-href-wiki-file-question-book-new-svg-class-image-img-alt-src-upload-wikimedia-org-wikipedia-en-thumb-9-99-question-book-new-svg-50px-question-book-new-svg-png-decoding-async-width-50-height-39-srcset-upload-wikimedia-org-wikipedia-en-thumb-9-99-question-book-new-svg-75px-question-book-new-svg-png-1-5x-upload-wikimedia-org-wikipedia-en-thumb-9-99-question-book-new-svg-100px-question-book-new-svg-png-2x-data-file-width-512-data-file-height-399-a-div-td-td-class-mbox-text-div-class-mbox-text-span-this-section-b-needs-additional-citations-for-a-href-wiki-wikipedia-verifiability-title-wikipedia-verifiability-verification-a-b-span-class-hide-when-compact-please-help-a-class-external-text-href-en-wikipedia-org-w-index-php-title-convolutional-neural-network-action-edit-improve-this-article-a-by-a-href-wiki-help-introduction-to-referencing-with-wiki-markup-1-title-help-introduction-to-referencing-with-wiki-markup-1-adding-citations-to-reliable-sources-a-unsourced-material-may-be-challenged-and-removed-br-small-span-class-plainlinks-i-find-sources-i-a-rel-nofollow-class-external-text-href-www-google-com-search-as-eq-wikipedia-q-22convolutional-neural-network-22-convolutional-neural-network-a-a-rel-nofollow-class-external-text-href-www-google-com-search-tbm-nws-q-22convolutional-neural-network-22-wikipedia-news-a-b-b-a-rel-nofollow-class-external-text-href-www-google-com-search-q-22convolutional-neural-network-22-site-news-google-com-newspapers-source-newspapers-newspapers-a-b-b-a-rel-nofollow-class-external-text-href-www-google-com-search-tbs-bks-1-q-22convolutional-neural-network-22-wikipedia-books-a-b-b-a-rel-nofollow-class-external-text-href-scholar-google-com-scholar-q-22convolutional-neural-network-22-scholar-a-b-b-a-rel-nofollow-class-external-text-href-https-www-jstor-org-action-dobasicsearch-query-22convolutional-neural-network-22-acc-on-wc-on-jstor-a-span-small-span-small-class-date-container-i-span-class-date-june-2017-span-i-small-small-class-hide-when-compact-i-a-href-wiki-help-maintenance-template-removal-title-help-maintenance-template-removal-learn-how-and-when-to-remove-this-template-message-a-i-small-div-td-tr-tbody-table-p-a-cnn-architecture-is-formed-by-a-stack-of-distinct-layers-that-transform-the-input-volume-into-an-output-volume-e-g-holding-the-class-scores-through-a-differentiable-function-a-few-distinct-types-of-layers-are-commonly-used-these-are-further-discussed-below-p-div-class-thumb-tleft-div-class-thumbinner-style-width-231px-a-href-wiki-file-conv-layer-png-class-image-img-alt-src-upload-wikimedia-org-wikipedia-commons-thumb-6-68-conv-layer-png-229px-conv-layer-png-decoding-async-width-229-height-154-class-thumbimage-srcset-upload-wikimedia-org-wikipedia-commons-thumb-6-68-conv-layer-png-344px-conv-layer-png-1-5x-upload-wikimedia-org-wikipedia-commons-thumb-6-68-conv-layer-png-458px-conv-layer-png-2x-data-file-width-634-data-file-height-426-a-div-class-thumbcaption-div-class-magnify-a-href-wiki-file-conv-layer-png-class-internal-title-enlarge-a-div-neurons-of-a-convolutional-layer-blue-connected-to-their-receptive-field-red-div-div-div-h3-span-class-mw-headline-id-convolutional-layer-convolutional-layer-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-19-title-edit-section-convolutional-layer-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-the-convolutional-layer-is-the-core-building-block-of-a-cnn-the-layer-s-parameters-consist-of-a-set-of-learnable-a-href-wiki-filter-signal-processing-title-filter-signal-processing-filters-a-or-a-href-wiki-kernel-image-processing-title-kernel-image-processing-kernels-a-which-have-a-small-receptive-field-but-extend-through-the-full-depth-of-the-input-volume-during-the-forward-pass-each-filter-is-a-href-wiki-convolution-title-convolution-convolved-a-across-the-width-and-height-of-the-input-volume-computing-the-a-href-wiki-dot-product-title-dot-product-dot-product-a-between-the-entries-of-the-filter-and-the-input-and-producing-a-2-dimensional-a-href-wiki-activation-function-title-activation-function-activation-map-a-of-that-filter-as-a-result-the-network-learns-filters-that-activate-when-it-detects-some-specific-type-of-a-href-wiki-feature-machine-learning-title-feature-machine-learning-feature-a-at-some-spatial-position-in-the-input-sup-id-cite-ref-52-class-reference-a-href-cite-note-52-nb-1-a-sup-p-p-stacking-the-activation-maps-for-all-filters-along-the-depth-dimension-forms-the-full-output-volume-of-the-convolution-layer-every-entry-in-the-output-volume-can-thus-also-be-interpreted-as-an-output-of-a-neuron-that-looks-at-a-small-region-in-the-input-and-shares-parameters-with-neurons-in-the-same-activation-map-p-h4-span-class-mw-headline-id-local-connectivity-local-connectivity-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-20-title-edit-section-local-connectivity-edit-a-span-class-mw-editsection-bracket-span-span-h4-div-class-thumb-tright-div-class-thumbinner-style-width-397px-a-href-wiki-file-typical-cnn-png-class-image-img-alt-src-upload-wikimedia-org-wikipedia-commons-thumb-6-63-typical-cnn-png-395px-typical-cnn-png-decoding-async-width-395-height-122-class-thumbimage-srcset-upload-wikimedia-org-wikipedia-commons-thumb-6-63-typical-cnn-png-593px-typical-cnn-png-1-5x-upload-wikimedia-org-wikipedia-commons-thumb-6-63-typical-cnn-png-790px-typical-cnn-png-2x-data-file-width-1040-data-file-height-320-a-div-class-thumbcaption-div-class-magnify-a-href-wiki-file-typical-cnn-png-class-internal-title-enlarge-a-div-typical-cnn-architecture-div-div-div-p-when-dealing-with-high-dimensional-inputs-such-as-images-it-is-impractical-to-connect-neurons-to-all-neurons-in-the-previous-volume-because-such-a-network-architecture-does-not-take-the-spatial-structure-of-the-data-into-account-convolutional-networks-exploit-spatially-local-correlation-by-enforcing-a-a-href-wiki-sparse-network-title-sparse-network-sparse-local-connectivity-a-pattern-between-neurons-of-adjacent-layers-each-neuron-is-connected-to-only-a-small-region-of-the-input-volume-p-p-the-extent-of-this-connectivity-is-a-a-href-wiki-hyperparameter-optimization-title-hyperparameter-optimization-hyperparameter-a-called-the-a-href-wiki-receptive-field-title-receptive-field-receptive-field-a-of-the-neuron-the-connections-are-a-href-wiki-spatial-locality-class-mw-redirect-title-spatial-locality-local-in-space-a-along-width-and-height-but-always-extend-along-the-entire-depth-of-the-input-volume-such-an-architecture-ensures-that-the-learnt-filters-produce-the-strongest-response-to-a-spatially-local-input-pattern-p-h4-span-class-mw-headline-id-spatial-arrangement-spatial-arrangement-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-21-title-edit-section-spatial-arrangement-edit-a-span-class-mw-editsection-bracket-span-span-h4-p-three-a-href-wiki-hyperparameter-machine-learning-title-hyperparameter-machine-learning-hyperparameters-a-control-the-size-of-the-output-volume-of-the-convolutional-layer-the-depth-a-href-wiki-stride-of-an-array-title-stride-of-an-array-stride-a-and-zero-padding-p-ul-li-the-i-u-depth-u-i-of-the-output-volume-controls-the-number-of-neurons-in-a-layer-that-connect-to-the-same-region-of-the-input-volume-these-neurons-learn-to-activate-for-different-features-in-the-input-for-example-if-the-first-convolutional-layer-takes-the-raw-image-as-input-then-different-neurons-along-the-depth-dimension-may-activate-in-the-presence-of-various-oriented-edges-or-blobs-of-color-li-li-u-i-stride-i-u-controls-how-depth-columns-around-the-spatial-dimensions-width-and-height-are-allocated-when-the-stride-is-1-then-we-move-the-filters-one-pixel-at-a-time-this-leads-to-heavily-a-href-wiki-intersection-set-theory-title-intersection-set-theory-overlapping-a-receptive-fields-between-the-columns-and-also-to-large-output-volumes-when-the-stride-is-2-then-the-filters-jump-2-pixels-at-a-time-as-they-slide-around-similarly-for-any-integer-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-textstyle-s-0-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-false-scriptlevel-0-mi-s-mi-mo-mo-mn-0-mn-mo-mo-mstyle-mrow-annotation-encoding-application-x-tex-textstyle-s-0-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-d1fb1c59de56b22cfdf94e6c2a03f4b0dfc7c4d2-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-671ex-width-6-407ex-height-2-509ex-alt-textstyle-s-0-span-a-stride-of-i-s-i-causes-the-filter-to-be-translated-by-i-s-i-units-at-a-time-per-output-in-practice-stride-lengths-of-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-textstyle-s-geq-3-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-false-scriptlevel-0-mi-s-mi-mo-mo-mn-3-mn-mstyle-mrow-annotation-encoding-application-x-tex-textstyle-s-geq-3-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-1b1cc974b8af65e3a94aa730c0f360dd7059604d-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-505ex-width-5-76ex-height-2-343ex-alt-textstyle-s-geq-3-span-are-rare-the-receptive-fields-overlap-less-and-the-resulting-output-volume-has-smaller-spatial-dimensions-when-stride-length-is-increased-sup-id-cite-ref-53-class-reference-a-href-cite-note-53-52-a-sup-li-li-sometimes-it-is-convenient-to-pad-the-input-with-zeros-on-the-border-of-the-input-volume-the-size-of-this-padding-is-a-third-hyperparameter-padding-provides-control-of-the-output-volume-spatial-size-in-particular-sometimes-it-is-desirable-to-exactly-preserve-the-spatial-size-of-the-input-volume-li-ul-p-the-spatial-size-of-the-output-volume-can-be-computed-as-a-function-of-the-input-volume-size-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-w-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-mi-w-mi-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-w-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-54a9c4c547f4d6111f81946cad242b18298d70b7-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-338ex-width-2-435ex-height-2-176ex-alt-w-span-the-kernel-field-size-of-the-convolutional-layer-neurons-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-k-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-mi-k-mi-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-k-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-2b76fce82a62ed5461908f0dc8f037de4e3686b0-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-338ex-width-2-066ex-height-2-176ex-alt-k-span-the-stride-with-which-they-are-applied-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-s-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-mi-s-mi-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-s-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-4611d85173cd3b508e67077d4a1252c9c05abca2-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-338ex-width-1-499ex-height-2-176ex-alt-s-span-and-the-amount-of-zero-padding-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-p-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-mi-p-mi-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-p-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-b4dc73bf40314945ff376bd363916a738548d40a-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-338ex-width-1-745ex-height-2-176ex-alt-p-span-used-on-the-border-the-formula-for-calculating-how-many-neurons-fit-in-a-given-volume-is-given-by-p-p-div-class-mwe-math-element-div-class-mwe-math-mathml-display-mwe-math-mathml-a11y-style-display-none-math-display-block-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-frac-w-k-2p-s-1-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-mrow-class-mjx-texatom-ord-mfrac-mrow-mi-w-mi-mo-mo-mi-k-mi-mo-mo-mn-2-mn-mi-p-mi-mrow-mi-s-mi-mfrac-mrow-mo-mo-mn-1-mn-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-frac-w-k-2p-s-1-annotation-semantics-math-div-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-a5325dc0b1b8695f19ec8fe3485d0da19040c622-class-mwe-math-fallback-image-display-aria-hidden-true-style-vertical-align-2-005ex-width-18-576ex-height-5-343ex-alt-displaystyle-frac-w-k-2p-s-1-div-p-p-if-this-number-is-not-an-a-href-wiki-integer-title-integer-integer-a-then-the-strides-are-incorrect-and-the-neurons-cannot-be-tiled-to-fit-across-the-input-volume-in-a-a-href-wiki-symmetry-title-symmetry-symmetric-a-way-in-general-setting-zero-padding-to-be-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-textstyle-p-k-1-2-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-false-scriptlevel-0-mi-p-mi-mo-mo-mo-stretchy-false-mo-mi-k-mi-mo-mo-mn-1-mn-mo-stretchy-false-mo-mrow-class-mjx-texatom-ord-mo-mo-mrow-mn-2-mn-mstyle-mrow-annotation-encoding-application-x-tex-textstyle-p-k-1-2-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-4b1c80a44e8d2e568cb4adfc6f6c8bf554f210e6-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-838ex-width-15-047ex-height-2-843ex-alt-textstyle-p-k-1-2-span-when-the-stride-is-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-s-1-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-mi-s-mi-mo-mo-mn-1-mn-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-s-1-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-d5e2b58c1aaaf2718fb801e97bf21d1f72726372-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-338ex-width-5-76ex-height-2-176ex-alt-s-1-span-ensures-that-the-input-volume-and-output-volume-will-have-the-same-size-spatially-however-it-s-not-always-completely-necessary-to-use-all-of-the-neurons-of-the-previous-layer-for-example-a-neural-network-designer-may-decide-to-use-just-a-portion-of-padding-p-h4-span-class-mw-headline-id-parameter-sharing-parameter-sharing-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-22-title-edit-section-parameter-sharing-edit-a-span-class-mw-editsection-bracket-span-span-h4-p-a-parameter-sharing-scheme-is-used-in-convolutional-layers-to-control-the-number-of-free-parameters-it-relies-on-one-reasonable-assumption-if-a-patch-feature-is-useful-to-compute-at-some-spatial-position-then-it-should-also-be-useful-to-compute-at-other-positions-in-other-words-denoting-a-single-2-dimensional-slice-of-depth-as-a-b-depth-slice-b-we-constrain-the-neurons-in-each-depth-slice-to-use-the-same-weights-and-bias-p-p-since-all-neurons-in-a-single-depth-slice-share-the-same-parameters-the-forward-pass-in-each-depth-slice-of-the-convolutional-layer-can-be-computed-as-a-a-href-wiki-convolution-title-convolution-convolution-a-of-the-neuron-s-weights-with-the-input-volume-sup-id-cite-ref-54-class-reference-a-href-cite-note-54-nb-2-a-sup-therefore-it-is-common-to-refer-to-the-sets-of-weights-as-a-filter-or-a-a-href-wiki-kernel-image-processing-title-kernel-image-processing-kernel-a-which-is-convolved-with-the-input-the-result-of-this-convolution-is-an-a-href-wiki-activation-function-title-activation-function-activation-map-a-and-the-set-of-activation-maps-for-each-different-filter-are-stacked-together-along-the-depth-dimension-to-produce-the-output-volume-parameter-sharing-contributes-to-the-a-href-wiki-translational-symmetry-title-translational-symmetry-translation-invariance-a-of-the-cnn-architecture-p-p-sometimes-the-parameter-sharing-assumption-may-not-make-sense-this-is-especially-the-case-when-the-input-images-to-a-cnn-have-some-specific-centered-structure-for-which-we-expect-completely-different-features-to-be-learned-on-different-spatial-locations-one-practical-example-is-when-the-inputs-are-faces-that-have-been-centered-in-the-image-we-might-expect-different-eye-specific-or-hair-specific-features-to-be-learned-in-different-parts-of-the-image-in-that-case-it-is-common-to-relax-the-parameter-sharing-scheme-and-instead-simply-call-the-layer-a-locally-connected-layer-p-h3-span-class-mw-headline-id-pooling-layer-pooling-layer-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-23-title-edit-section-pooling-layer-edit-a-span-class-mw-editsection-bracket-span-span-h3-div-class-thumb-tright-div-class-thumbinner-style-width-316px-a-href-wiki-file-max-pooling-png-class-image-img-alt-src-upload-wikimedia-org-wikipedia-commons-thumb-e-e9-max-pooling-png-314px-max-pooling-png-decoding-async-width-314-height-182-class-thumbimage-srcset-upload-wikimedia-org-wikipedia-commons-thumb-e-e9-max-pooling-png-471px-max-pooling-png-1-5x-upload-wikimedia-org-wikipedia-commons-e-e9-max-pooling-png-2x-data-file-width-570-data-file-height-330-a-div-class-thumbcaption-div-class-magnify-a-href-wiki-file-max-pooling-png-class-internal-title-enlarge-a-div-max-pooling-with-a-2x2-filter-and-stride-2-div-div-div-p-another-important-concept-of-cnns-is-pooling-which-is-a-form-of-non-linear-a-href-wiki-downsampling-signal-processing-title-downsampling-signal-processing-down-sampling-a-there-are-several-non-linear-functions-to-implement-pooling-among-which-i-max-pooling-i-is-the-most-common-it-a-href-wiki-partition-of-a-set-title-partition-of-a-set-partitions-a-the-input-image-into-a-set-of-non-overlapping-rectangles-and-for-each-such-sub-region-outputs-the-maximum-p-p-intuitively-the-exact-location-of-a-feature-is-less-important-than-its-rough-location-relative-to-other-features-this-is-the-idea-behind-the-use-of-pooling-in-convolutional-neural-networks-the-pooling-layer-serves-to-progressively-reduce-the-spatial-size-of-the-representation-to-reduce-the-number-of-parameters-a-href-wiki-memory-footprint-title-memory-footprint-memory-footprint-a-and-amount-of-computation-in-the-network-and-hence-to-also-control-a-href-wiki-overfitting-title-overfitting-overfitting-a-it-is-common-to-periodically-insert-a-pooling-layer-between-successive-convolutional-layers-in-a-cnn-architecture-sup-class-noprint-inline-template-template-fact-style-white-space-nowrap-i-a-href-wiki-wikipedia-citation-needed-title-wikipedia-citation-needed-span-title-this-is-generally-accepted-but-to-someone-unfamiliar-may-be-questioned-december-2018-citation-needed-span-a-i-sup-the-pooling-operation-provides-another-form-of-translation-invariance-p-p-the-pooling-layer-operates-independently-on-every-depth-slice-of-the-input-and-resizes-it-spatially-the-most-common-form-is-a-pooling-layer-with-filters-of-size-2x2-applied-with-a-stride-of-2-downsamples-at-every-depth-slice-in-the-input-by-2-along-both-width-and-height-discarding-75-of-the-activations-in-this-case-every-a-href-wiki-maximum-class-mw-redirect-title-maximum-max-operation-a-is-over-4-numbers-the-depth-dimension-remains-unchanged-p-p-in-addition-to-max-pooling-pooling-units-can-use-other-functions-such-as-a-href-wiki-average-title-average-average-a-pooling-or-a-href-wiki-euclidean-norm-class-mw-redirect-title-euclidean-norm-sub-2-sub-norm-a-pooling-average-pooling-was-often-used-historically-but-has-recently-fallen-out-of-favor-compared-to-max-pooling-which-performs-better-in-practice-sup-id-cite-ref-scherer-icann-2010-55-0-class-reference-a-href-cite-note-scherer-icann-2010-55-53-a-sup-p-p-due-to-the-aggressive-reduction-in-the-size-of-the-representation-sup-class-noprint-inline-template-style-white-space-nowrap-i-a-href-wiki-wikipedia-avoid-weasel-words-class-mw-redirect-title-wikipedia-avoid-weasel-words-span-title-the-material-near-this-tag-possibly-uses-too-vague-attribution-or-weasel-words-december-2018-which-span-a-i-sup-there-is-a-recent-trend-towards-using-smaller-filters-sup-id-cite-ref-56-class-reference-a-href-cite-note-56-54-a-sup-or-discarding-pooling-layers-altogether-sup-id-cite-ref-57-class-reference-a-href-cite-note-57-55-a-sup-p-div-class-thumb-tright-div-class-thumbinner-style-width-402px-a-href-wiki-file-roi-pooling-animated-gif-class-image-img-alt-src-upload-wikimedia-org-wikipedia-commons-thumb-d-dc-roi-pooling-animated-gif-400px-roi-pooling-animated-gif-decoding-async-width-400-height-300-class-thumbimage-srcset-upload-wikimedia-org-wikipedia-commons-thumb-d-dc-roi-pooling-animated-gif-600px-roi-pooling-animated-gif-1-5x-upload-wikimedia-org-wikipedia-commons-d-dc-roi-pooling-animated-gif-2x-data-file-width-800-data-file-height-600-a-div-class-thumbcaption-div-class-magnify-a-href-wiki-file-roi-pooling-animated-gif-class-internal-title-enlarge-a-div-roi-pooling-to-size-2x2-in-this-example-region-proposal-an-input-parameter-has-size-7x5-div-div-div-p-a-href-wiki-region-of-interest-title-region-of-interest-region-of-interest-a-pooling-also-known-as-roi-pooling-is-a-variant-of-max-pooling-in-which-output-size-is-fixed-and-input-rectangle-is-a-parameter-sup-id-cite-ref-58-class-reference-a-href-cite-note-58-56-a-sup-p-p-pooling-is-an-important-component-of-convolutional-neural-networks-for-a-href-wiki-object-detection-title-object-detection-object-detection-a-based-on-fast-r-cnn-sup-id-cite-ref-rcnn-59-0-class-reference-a-href-cite-note-rcnn-59-57-a-sup-architecture-p-h3-span-class-mw-headline-id-relu-layer-relu-layer-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-24-title-edit-section-relu-layer-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-relu-is-the-abbreviation-of-a-href-wiki-rectifier-neural-networks-title-rectifier-neural-networks-rectified-linear-unit-a-which-applies-the-non-saturating-a-href-wiki-activation-function-title-activation-function-activation-function-a-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-textstyle-f-x-max-0-x-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-false-scriptlevel-0-mi-f-mi-mo-stretchy-false-mo-mi-x-mi-mo-stretchy-false-mo-mo-mo-mo-movablelimits-true-form-prefix-max-mo-mo-stretchy-false-mo-mn-0-mn-mo-mo-mi-x-mi-mo-stretchy-false-mo-mstyle-mrow-annotation-encoding-application-x-tex-textstyle-f-x-max-0-x-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-e2559111e4c3500df55d16f958f853e19e90804a-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-838ex-width-17-177ex-height-2-843ex-alt-textstyle-f-x-max-0-x-span-sup-id-cite-ref-02-48-1-class-reference-a-href-cite-note-02-48-48-a-sup-it-effectively-removes-negative-values-from-an-activation-map-by-setting-them-to-zero-sup-id-cite-ref-romanuke4-60-0-class-reference-a-href-cite-note-romanuke4-60-58-a-sup-it-increases-the-a-href-wiki-nonlinearity-class-mw-redirect-title-nonlinearity-nonlinear-properties-a-of-the-a-href-wiki-decision-boundary-title-decision-boundary-decision-function-a-and-of-the-overall-network-without-affecting-the-receptive-fields-of-the-convolution-layer-p-p-other-functions-are-also-used-to-increase-nonlinearity-for-example-the-saturating-a-href-wiki-hyperbolic-tangent-class-mw-redirect-title-hyperbolic-tangent-hyperbolic-tangent-a-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-f-x-tanh-x-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-mi-f-mi-mo-stretchy-false-mo-mi-x-mi-mo-stretchy-false-mo-mo-mo-mi-tanh-mi-mo-mo-mo-stretchy-false-mo-mi-x-mi-mo-stretchy-false-mo-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-f-x-tanh-x-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-1a319ec32dbb0c625fa4802baf9252d1f00854e2-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-838ex-width-15-307ex-height-2-843ex-alt-displaystyle-f-x-tanh-x-span-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-f-x-tanh-x-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-mi-f-mi-mo-stretchy-false-mo-mi-x-mi-mo-stretchy-false-mo-mo-mo-mrow-class-mjx-texatom-ord-mo-stretchy-false-mo-mrow-mi-tanh-mi-mo-mo-mo-stretchy-false-mo-mi-x-mi-mo-stretchy-false-mo-mrow-class-mjx-texatom-ord-mo-stretchy-false-mo-mrow-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-f-x-tanh-x-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-d1eb71e39ce9687851b7ec55bb8f54f42df2a828-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-838ex-width-16-988ex-height-2-843ex-alt-displaystyle-f-x-tanh-x-span-and-the-a-href-wiki-sigmoid-function-title-sigmoid-function-sigmoid-function-a-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-textstyle-sigma-x-1-e-x-1-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-false-scriptlevel-0-mi-s-mi-mo-stretchy-false-mo-mi-x-mi-mo-stretchy-false-mo-mo-mo-mo-stretchy-false-mo-mn-1-mn-mo-mo-msup-mi-e-mi-mrow-class-mjx-texatom-ord-mo-mo-mi-x-mi-mrow-msup-msup-mo-stretchy-false-mo-mrow-class-mjx-texatom-ord-mo-mo-mn-1-mn-mrow-msup-mstyle-mrow-annotation-encoding-application-x-tex-textstyle-sigma-x-1-e-x-1-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-3805da830a1a77523aefcdae0d2164d954849066-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-838ex-width-19-247ex-height-3-009ex-alt-textstyle-sigma-x-1-e-x-1-span-relu-is-often-preferred-to-other-functions-because-it-trains-the-neural-network-several-times-faster-without-a-significant-penalty-to-a-href-wiki-generalization-learning-title-generalization-learning-generalization-a-accuracy-sup-id-cite-ref-61-class-reference-a-href-cite-note-61-59-a-sup-p-h3-span-class-mw-headline-id-fully-connected-layer-fully-connected-layer-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-25-title-edit-section-fully-connected-layer-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-finally-after-several-convolutional-and-max-pooling-layers-the-high-level-reasoning-in-the-neural-network-is-done-via-fully-connected-layers-neurons-in-a-fully-connected-layer-have-connections-to-all-activations-in-the-previous-layer-as-seen-in-regular-non-convolutional-a-href-wiki-artificial-neural-network-title-artificial-neural-network-artificial-neural-networks-a-their-activations-can-thus-be-computed-as-an-a-href-wiki-affine-transformation-title-affine-transformation-affine-transformation-a-with-a-href-wiki-matrix-multiplication-title-matrix-multiplication-matrix-multiplication-a-followed-by-a-bias-offset-a-href-wiki-vector-addition-class-mw-redirect-title-vector-addition-vector-addition-a-of-a-learned-or-fixed-bias-term-p-h3-span-class-mw-headline-id-loss-layer-loss-layer-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-26-title-edit-section-loss-layer-edit-a-span-class-mw-editsection-bracket-span-span-h3-div-role-note-class-hatnote-navigation-not-searchable-main-articles-a-href-wiki-loss-function-title-loss-function-loss-function-a-and-a-href-wiki-loss-functions-for-classification-title-loss-functions-for-classification-loss-functions-for-classification-a-div-p-the-b-loss-layer-b-specifies-how-a-href-wiki-training-title-training-training-a-penalizes-the-deviation-between-the-predicted-output-and-a-href-wiki-ground-truth-title-ground-truth-true-a-labels-and-is-normally-the-final-layer-of-a-neural-network-various-a-href-wiki-loss-function-title-loss-function-loss-functions-a-appropriate-for-different-tasks-may-be-used-p-p-a-href-wiki-softmax-function-title-softmax-function-softmax-a-loss-is-used-for-predicting-a-single-class-of-i-k-i-mutually-exclusive-classes-sup-id-cite-ref-62-class-reference-a-href-cite-note-62-nb-3-a-sup-a-href-wiki-sigmoid-function-title-sigmoid-function-sigmoid-a-a-href-wiki-cross-entropy-title-cross-entropy-cross-entropy-a-loss-is-used-for-predicting-i-k-i-independent-probability-values-in-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-01-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-mo-stretchy-false-mo-mn-0-mn-mo-mo-mn-1-mn-mo-stretchy-false-mo-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-01-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-738f7d23bb2d9642bab520020873cccbef49768d-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-838ex-width-4-653ex-height-2-843ex-alt-01-span-a-href-wiki-euclidean-distance-title-euclidean-distance-euclidean-a-loss-is-used-for-a-href-wiki-regression-machine-learning-class-mw-redirect-title-regression-machine-learning-regressing-a-to-a-href-wiki-real-number-title-real-number-real-valued-a-labels-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-infty-infty-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-mo-stretchy-false-mo-mo-mo-mi-mathvariant-normal-mi-mo-mo-mi-mathvariant-normal-mi-mo-stretchy-false-mo-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-infty-infty-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-0c8c11c44279888c9e395eeb5f45d121348ae10a-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-838ex-width-9-299ex-height-2-843ex-alt-infty-infty-span-p-h2-span-class-mw-headline-id-choosing-hyperparameters-choosing-hyperparameters-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-27-title-edit-section-choosing-hyperparameters-edit-a-span-class-mw-editsection-bracket-span-span-h2-table-class-box-more-citations-needed-section-plainlinks-metadata-ambox-ambox-content-ambox-refimprove-role-presentation-tbody-tr-td-class-mbox-image-div-style-width-52px-a-href-wiki-file-question-book-new-svg-class-image-img-alt-src-upload-wikimedia-org-wikipedia-en-thumb-9-99-question-book-new-svg-50px-question-book-new-svg-png-decoding-async-width-50-height-39-srcset-upload-wikimedia-org-wikipedia-en-thumb-9-99-question-book-new-svg-75px-question-book-new-svg-png-1-5x-upload-wikimedia-org-wikipedia-en-thumb-9-99-question-book-new-svg-100px-question-book-new-svg-png-2x-data-file-width-512-data-file-height-399-a-div-td-td-class-mbox-text-div-class-mbox-text-span-this-section-b-needs-additional-citations-for-a-href-wiki-wikipedia-verifiability-title-wikipedia-verifiability-verification-a-b-span-class-hide-when-compact-please-help-a-class-external-text-href-en-wikipedia-org-w-index-php-title-convolutional-neural-network-action-edit-improve-this-article-a-by-a-href-wiki-help-introduction-to-referencing-with-wiki-markup-1-title-help-introduction-to-referencing-with-wiki-markup-1-adding-citations-to-reliable-sources-a-unsourced-material-may-be-challenged-and-removed-br-small-span-class-plainlinks-i-find-sources-i-a-rel-nofollow-class-external-text-href-www-google-com-search-as-eq-wikipedia-q-22convolutional-neural-network-22-convolutional-neural-network-a-a-rel-nofollow-class-external-text-href-www-google-com-search-tbm-nws-q-22convolutional-neural-network-22-wikipedia-news-a-b-b-a-rel-nofollow-class-external-text-href-www-google-com-search-q-22convolutional-neural-network-22-site-news-google-com-newspapers-source-newspapers-newspapers-a-b-b-a-rel-nofollow-class-external-text-href-www-google-com-search-tbs-bks-1-q-22convolutional-neural-network-22-wikipedia-books-a-b-b-a-rel-nofollow-class-external-text-href-scholar-google-com-scholar-q-22convolutional-neural-network-22-scholar-a-b-b-a-rel-nofollow-class-external-text-href-https-www-jstor-org-action-dobasicsearch-query-22convolutional-neural-network-22-acc-on-wc-on-jstor-a-span-small-span-small-class-date-container-i-span-class-date-june-2017-span-i-small-small-class-hide-when-compact-i-a-href-wiki-help-maintenance-template-removal-title-help-maintenance-template-removal-learn-how-and-when-to-remove-this-template-message-a-i-small-div-td-tr-tbody-table-p-cnns-use-more-a-href-wiki-hyperparameter-machine-learning-title-hyperparameter-machine-learning-hyperparameters-a-than-a-standard-multilayer-perceptron-mlp-while-the-usual-rules-for-a-href-wiki-learning-rate-title-learning-rate-learning-rates-a-and-a-href-wiki-regularization-mathematics-title-regularization-mathematics-regularization-a-constants-still-apply-the-following-should-be-kept-in-mind-when-optimizing-p-h3-span-class-mw-headline-id-number-of-filters-number-of-filters-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-28-title-edit-section-number-of-filters-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-since-feature-map-size-decreases-with-depth-layers-near-the-input-layer-will-tend-to-have-fewer-filters-while-higher-layers-can-have-more-to-equalize-computation-at-each-layer-the-product-of-feature-values-i-v-sub-a-sub-i-with-pixel-position-is-kept-roughly-constant-across-layers-preserving-more-information-about-the-input-would-require-keeping-the-total-number-of-activations-number-of-feature-maps-times-number-of-pixel-positions-non-decreasing-from-one-layer-to-the-next-p-p-the-number-of-feature-maps-directly-controls-the-capacity-and-depends-on-the-number-of-available-examples-and-task-complexity-p-h3-span-class-mw-headline-id-filter-shape-filter-shape-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-29-title-edit-section-filter-shape-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-common-filter-shapes-found-in-the-literature-vary-greatly-and-are-usually-chosen-based-on-the-dataset-p-p-the-challenge-is-thus-to-find-the-right-level-of-granularity-so-as-to-create-abstractions-at-the-proper-scale-given-a-particular-dataset-and-without-a-href-wiki-overfitting-title-overfitting-overfitting-a-p-h3-span-class-mw-headline-id-max-pooling-shape-max-pooling-shape-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-30-title-edit-section-max-pooling-shape-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-typical-values-are-2x2-very-large-input-volumes-may-warrant-4x4-pooling-in-the-lower-layers-sup-id-cite-ref-63-class-reference-a-href-cite-note-63-60-a-sup-however-choosing-larger-shapes-will-dramatically-a-href-wiki-dimensionality-reduction-title-dimensionality-reduction-reduce-the-dimension-a-of-the-signal-and-may-result-in-excess-a-href-wiki-data-loss-title-data-loss-information-loss-a-often-non-overlapping-pooling-windows-perform-best-sup-id-cite-ref-scherer-icann-2010-55-1-class-reference-a-href-cite-note-scherer-icann-2010-55-53-a-sup-p-h2-span-class-mw-headline-id-regularization-methods-regularization-methods-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-31-title-edit-section-regularization-methods-edit-a-span-class-mw-editsection-bracket-span-span-h2-div-role-note-class-hatnote-navigation-not-searchable-main-article-a-href-wiki-regularization-mathematics-title-regularization-mathematics-regularization-mathematics-a-div-table-class-box-more-citations-needed-section-plainlinks-metadata-ambox-ambox-content-ambox-refimprove-role-presentation-tbody-tr-td-class-mbox-image-div-style-width-52px-a-href-wiki-file-question-book-new-svg-class-image-img-alt-src-upload-wikimedia-org-wikipedia-en-thumb-9-99-question-book-new-svg-50px-question-book-new-svg-png-decoding-async-width-50-height-39-srcset-upload-wikimedia-org-wikipedia-en-thumb-9-99-question-book-new-svg-75px-question-book-new-svg-png-1-5x-upload-wikimedia-org-wikipedia-en-thumb-9-99-question-book-new-svg-100px-question-book-new-svg-png-2x-data-file-width-512-data-file-height-399-a-div-td-td-class-mbox-text-div-class-mbox-text-span-this-section-b-needs-additional-citations-for-a-href-wiki-wikipedia-verifiability-title-wikipedia-verifiability-verification-a-b-span-class-hide-when-compact-please-help-a-class-external-text-href-en-wikipedia-org-w-index-php-title-convolutional-neural-network-action-edit-improve-this-article-a-by-a-href-wiki-help-introduction-to-referencing-with-wiki-markup-1-title-help-introduction-to-referencing-with-wiki-markup-1-adding-citations-to-reliable-sources-a-unsourced-material-may-be-challenged-and-removed-br-small-span-class-plainlinks-i-find-sources-i-a-rel-nofollow-class-external-text-href-www-google-com-search-as-eq-wikipedia-q-22convolutional-neural-network-22-convolutional-neural-network-a-a-rel-nofollow-class-external-text-href-www-google-com-search-tbm-nws-q-22convolutional-neural-network-22-wikipedia-news-a-b-b-a-rel-nofollow-class-external-text-href-www-google-com-search-q-22convolutional-neural-network-22-site-news-google-com-newspapers-source-newspapers-newspapers-a-b-b-a-rel-nofollow-class-external-text-href-www-google-com-search-tbs-bks-1-q-22convolutional-neural-network-22-wikipedia-books-a-b-b-a-rel-nofollow-class-external-text-href-scholar-google-com-scholar-q-22convolutional-neural-network-22-scholar-a-b-b-a-rel-nofollow-class-external-text-href-https-www-jstor-org-action-dobasicsearch-query-22convolutional-neural-network-22-acc-on-wc-on-jstor-a-span-small-span-small-class-date-container-i-span-class-date-june-2017-span-i-small-small-class-hide-when-compact-i-a-href-wiki-help-maintenance-template-removal-title-help-maintenance-template-removal-learn-how-and-when-to-remove-this-template-message-a-i-small-div-td-tr-tbody-table-p-a-href-wiki-regularization-mathematics-title-regularization-mathematics-regularization-a-is-a-process-of-introducing-additional-information-to-solve-an-a-href-wiki-ill-posed-problem-class-mw-redirect-title-ill-posed-problem-ill-posed-problem-a-or-to-prevent-a-href-wiki-overfitting-title-overfitting-overfitting-a-cnns-use-various-types-of-regularization-p-h3-span-class-mw-headline-id-empirical-empirical-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-32-title-edit-section-empirical-edit-a-span-class-mw-editsection-bracket-span-span-h3-h4-span-class-mw-headline-id-dropout-dropout-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-33-title-edit-section-dropout-edit-a-span-class-mw-editsection-bracket-span-span-h4-p-because-a-fully-connected-layer-occupies-most-of-the-parameters-it-is-prone-to-a-href-wiki-overfitting-title-overfitting-overfitting-a-one-method-to-reduce-overfitting-is-a-href-wiki-dropout-neural-networks-title-dropout-neural-networks-dropout-a-sup-id-cite-ref-64-class-reference-a-href-cite-note-64-61-a-sup-sup-id-cite-ref-dlpatterns-65-0-class-reference-a-href-cite-note-dlpatterns-65-62-a-sup-at-each-training-stage-individual-nodes-are-either-dropped-out-of-the-net-with-probability-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-1-p-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-mn-1-mn-mo-mo-mi-p-mi-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-1-p-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-9633a8692121eedfa99cace406205e5d1511ef8d-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-671ex-width-5-172ex-height-2-509ex-alt-1-p-span-or-kept-with-probability-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-p-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-mi-p-mi-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-p-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-81eac1e205430d1f40810df36a0edffdc367af36-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-671ex-margin-left-0-089ex-width-1-259ex-height-2-009ex-alt-p-span-so-that-a-reduced-network-is-left-incoming-and-outgoing-edges-to-a-dropped-out-node-are-also-removed-only-the-reduced-network-is-trained-on-the-data-in-that-stage-the-removed-nodes-are-then-reinserted-into-the-network-with-their-original-weights-p-p-in-the-training-stages-the-probability-that-a-hidden-node-will-be-dropped-is-usually-0-5-for-input-nodes-this-should-be-much-lower-intuitively-because-information-is-directly-lost-when-input-nodes-are-ignored-p-p-at-testing-time-after-training-has-finished-we-would-ideally-like-to-find-a-sample-average-of-all-possible-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-2-n-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-msup-mn-2-mn-mrow-class-mjx-texatom-ord-mi-n-mi-mrow-msup-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-2-n-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-8226f30650ee4fe4e640c6d2798127e80e9c160d-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-338ex-width-2-381ex-height-2-343ex-alt-2-n-span-dropped-out-networks-unfortunately-this-is-unfeasible-for-large-values-of-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-n-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-mi-n-mi-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-n-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-a601995d55609f2d9f5e233e36fbe9ea26011b3b-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-338ex-width-1-395ex-height-1-676ex-alt-n-span-however-we-can-find-an-approximation-by-using-the-full-network-with-each-node-s-output-weighted-by-a-factor-of-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-p-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-mi-p-mi-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-p-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-81eac1e205430d1f40810df36a0edffdc367af36-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-671ex-margin-left-0-089ex-width-1-259ex-height-2-009ex-alt-p-span-so-the-a-href-wiki-expected-value-title-expected-value-expected-value-a-of-the-output-of-any-node-is-the-same-as-in-the-training-stages-this-is-the-biggest-contribution-of-the-dropout-method-although-it-effectively-generates-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-2-n-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-msup-mn-2-mn-mrow-class-mjx-texatom-ord-mi-n-mi-mrow-msup-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-2-n-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-8226f30650ee4fe4e640c6d2798127e80e9c160d-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-338ex-width-2-381ex-height-2-343ex-alt-2-n-span-neural-nets-and-as-such-allows-for-model-combination-at-test-time-only-a-single-network-needs-to-be-tested-p-p-by-avoiding-training-all-nodes-on-all-training-data-dropout-decreases-overfitting-the-method-also-significantly-improves-training-speed-this-makes-the-model-combination-practical-even-for-a-href-wiki-deep-neural-network-class-mw-redirect-title-deep-neural-network-deep-neural-networks-a-the-technique-seems-to-reduce-node-interactions-leading-them-to-learn-more-robust-features-sup-class-noprint-inline-template-style-margin-left-0-1em-white-space-nowrap-i-a-href-wiki-wikipedia-please-clarify-title-wikipedia-please-clarify-span-title-december-2018-clarification-needed-span-a-i-sup-that-better-generalize-to-new-data-p-h4-span-class-mw-headline-id-dropconnect-dropconnect-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-34-title-edit-section-dropconnect-edit-a-span-class-mw-editsection-bracket-span-span-h4-p-dropconnect-is-the-generalization-of-dropout-in-which-each-connection-rather-than-each-output-unit-can-be-dropped-with-probability-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-1-p-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-mn-1-mn-mo-mo-mi-p-mi-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-1-p-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-9633a8692121eedfa99cace406205e5d1511ef8d-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-671ex-width-5-172ex-height-2-509ex-alt-1-p-span-each-unit-thus-receives-input-from-a-random-subset-of-units-in-the-previous-layer-sup-id-cite-ref-66-class-reference-a-href-cite-note-66-63-a-sup-p-p-dropconnect-is-similar-to-dropout-as-it-introduces-dynamic-sparsity-within-the-model-but-differs-in-that-the-sparsity-is-on-the-weights-rather-than-the-output-vectors-of-a-layer-in-other-words-the-fully-connected-layer-with-dropconnect-becomes-a-sparsely-connected-layer-in-which-the-connections-are-chosen-at-random-during-the-training-stage-p-h4-span-class-mw-headline-id-stochastic-pooling-stochastic-pooling-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-35-title-edit-section-stochastic-pooling-edit-a-span-class-mw-editsection-bracket-span-span-h4-p-a-major-drawback-to-dropout-is-that-it-does-not-have-the-same-benefits-for-convolutional-layers-where-the-neurons-are-not-fully-connected-p-p-in-stochastic-pooling-sup-id-cite-ref-67-class-reference-a-href-cite-note-67-64-a-sup-the-conventional-a-href-wiki-deterministic-algorithm-title-deterministic-algorithm-deterministic-a-pooling-operations-are-replaced-with-a-stochastic-procedure-where-the-activation-within-each-pooling-region-is-picked-randomly-according-to-a-a-href-wiki-multinomial-distribution-title-multinomial-distribution-multinomial-distribution-a-given-by-the-activities-within-the-pooling-region-this-approach-is-free-of-hyperparameters-and-can-be-combined-with-other-regularization-approaches-such-as-dropout-and-data-augmentation-p-p-an-alternate-view-of-stochastic-pooling-is-that-it-is-equivalent-to-standard-max-pooling-but-with-many-copies-of-an-input-image-each-having-small-local-a-href-wiki-deformation-theory-title-deformation-theory-deformations-a-this-is-similar-to-explicit-a-href-wiki-elastic-deformation-class-mw-redirect-title-elastic-deformation-elastic-deformations-a-of-the-input-images-sup-id-cite-ref-3-68-0-class-reference-a-href-cite-note-3-68-65-a-sup-which-delivers-excellent-performance-on-the-a-href-wiki-mnist-database-title-mnist-database-mnist-data-set-a-sup-id-cite-ref-3-68-1-class-reference-a-href-cite-note-3-68-65-a-sup-using-stochastic-pooling-in-a-multilayer-model-gives-an-exponential-number-of-deformations-since-the-selections-in-higher-layers-are-independent-of-those-below-p-h4-span-class-mw-headline-id-artificial-data-artificial-data-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-36-title-edit-section-artificial-data-edit-a-span-class-mw-editsection-bracket-span-span-h4-p-since-the-degree-of-model-overfitting-is-determined-by-both-its-power-and-the-amount-of-training-it-receives-providing-a-convolutional-network-with-more-training-examples-can-reduce-overfitting-since-these-networks-are-usually-trained-with-all-available-data-one-approach-is-to-either-generate-new-data-from-scratch-if-possible-or-perturb-existing-data-to-create-new-ones-for-example-input-images-could-be-asymmetrically-cropped-by-a-few-percent-to-create-new-examples-with-the-same-label-as-the-original-sup-id-cite-ref-69-class-reference-a-href-cite-note-69-66-a-sup-p-h3-span-class-mw-headline-id-explicit-explicit-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-37-title-edit-section-explicit-edit-a-span-class-mw-editsection-bracket-span-span-h3-h4-span-class-mw-headline-id-early-stopping-early-stopping-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-38-title-edit-section-early-stopping-edit-a-span-class-mw-editsection-bracket-span-span-h4-div-role-note-class-hatnote-navigation-not-searchable-main-article-a-href-wiki-early-stopping-title-early-stopping-early-stopping-a-div-p-one-of-the-simplest-methods-to-prevent-overfitting-of-a-network-is-to-simply-stop-the-training-before-overfitting-has-had-a-chance-to-occur-it-comes-with-the-disadvantage-that-the-learning-process-is-halted-p-h4-span-class-mw-headline-id-number-of-parameters-number-of-parameters-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-39-title-edit-section-number-of-parameters-edit-a-span-class-mw-editsection-bracket-span-span-h4-p-another-simple-way-to-prevent-overfitting-is-to-limit-the-number-of-parameters-typically-by-limiting-the-number-of-hidden-units-in-each-layer-or-limiting-network-depth-for-convolutional-networks-the-filter-size-also-affects-the-number-of-parameters-limiting-the-number-of-parameters-restricts-the-predictive-power-of-the-network-directly-reducing-the-complexity-of-the-function-that-it-can-perform-on-the-data-and-thus-limits-the-amount-of-overfitting-this-is-equivalent-to-a-a-href-wiki-zero-norm-class-mw-redirect-title-zero-norm-zero-norm-a-p-h4-span-class-mw-headline-id-weight-decay-weight-decay-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-40-title-edit-section-weight-decay-edit-a-span-class-mw-editsection-bracket-span-span-h4-p-a-simple-form-of-added-regularizer-is-weight-decay-which-simply-adds-an-additional-error-proportional-to-the-sum-of-weights-a-href-wiki-l1-norm-class-mw-redirect-title-l1-norm-l1-norm-a-or-squared-magnitude-a-href-wiki-l2-norm-class-mw-redirect-title-l2-norm-l2-norm-a-of-the-weight-vector-to-the-error-at-each-node-the-level-of-acceptable-model-complexity-can-be-reduced-by-increasing-the-proportionality-constant-thus-increasing-the-penalty-for-large-weight-vectors-p-p-l2-regularization-is-the-most-common-form-of-regularization-it-can-be-implemented-by-penalizing-the-squared-magnitude-of-all-parameters-directly-in-the-objective-the-l2-regularization-has-the-intuitive-interpretation-of-heavily-penalizing-peaky-weight-vectors-and-preferring-diffuse-weight-vectors-due-to-multiplicative-interactions-between-weights-and-inputs-this-has-the-useful-property-of-encouraging-the-network-to-use-all-of-its-inputs-a-little-rather-than-some-of-its-inputs-a-lot-p-p-l1-regularization-is-another-common-form-it-is-possible-to-combine-l1-with-l2-regularization-this-is-called-a-href-wiki-elastic-net-regularization-title-elastic-net-regularization-elastic-net-regularization-a-the-l1-regularization-leads-the-weight-vectors-to-become-sparse-during-optimization-in-other-words-neurons-with-l1-regularization-end-up-using-only-a-sparse-subset-of-their-most-important-inputs-and-become-nearly-invariant-to-the-noisy-inputs-p-h4-span-class-mw-headline-id-max-norm-constraints-max-norm-constraints-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-41-title-edit-section-max-norm-constraints-edit-a-span-class-mw-editsection-bracket-span-span-h4-p-another-form-of-regularization-is-to-enforce-an-absolute-upper-bound-on-the-magnitude-of-the-weight-vector-for-every-neuron-and-use-a-href-wiki-sparse-approximation-projected-gradient-descent-title-sparse-approximation-projected-gradient-descent-a-to-enforce-the-constraint-in-practice-this-corresponds-to-performing-the-parameter-update-as-normal-and-then-enforcing-the-constraint-by-clamping-the-weight-vector-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-vec-w-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-mrow-class-mjx-texatom-ord-mrow-class-mjx-texatom-ord-mover-mi-w-mi-mo-stretchy-false-mo-mover-mrow-mrow-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-vec-w-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-8b6c48cdaecf8d81481ea21b1d0c046bf34b68ec-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-338ex-width-1-664ex-height-2-343ex-alt-vec-w-span-of-every-neuron-to-satisfy-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-vec-w-2-c-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-mo-fence-false-stretchy-false-mo-mrow-class-mjx-texatom-ord-mrow-class-mjx-texatom-ord-mover-mi-w-mi-mo-stretchy-false-mo-mover-mrow-mrow-msub-mo-fence-false-stretchy-false-mo-mrow-class-mjx-texatom-ord-mn-2-mn-mrow-msub-mo-mo-mi-c-mi-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-vec-w-2-c-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-a1a47ec91922a4d2bc7122e90cff40c4b8b73d66-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-838ex-width-9-149ex-height-2-843ex-alt-displaystyle-vec-w-2-c-span-typical-values-of-span-class-mwe-math-element-span-class-mwe-math-mathml-inline-mwe-math-mathml-a11y-style-display-none-math-xmlns-http-www-w3-org-1998-math-mathml-alttext-displaystyle-c-semantics-mrow-class-mjx-texatom-ord-mstyle-displaystyle-true-scriptlevel-0-mi-c-mi-mstyle-mrow-annotation-encoding-application-x-tex-displaystyle-c-annotation-semantics-math-span-img-src-https-wikimedia-org-api-rest-v1-media-math-render-svg-86a67b81c2de995bd608d5b2df50cd8cd7d92455-class-mwe-math-fallback-image-inline-aria-hidden-true-style-vertical-align-0-338ex-width-1-007ex-height-1-676ex-alt-c-span-are-order-of-3-4-some-papers-report-improvements-sup-id-cite-ref-70-class-reference-a-href-cite-note-70-67-a-sup-when-using-this-form-of-regularization-p-h2-span-class-mw-headline-id-hierarchical-coordinate-frames-hierarchical-coordinate-frames-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-42-title-edit-section-hierarchical-coordinate-frames-edit-a-span-class-mw-editsection-bracket-span-span-h2-p-pooling-loses-the-precise-spatial-relationships-between-high-level-parts-such-as-nose-and-mouth-in-a-face-image-these-relationships-are-needed-for-identity-recognition-overlapping-the-pools-so-that-each-feature-occurs-in-multiple-pools-helps-retain-the-information-translation-alone-cannot-extrapolate-the-understanding-of-geometric-relationships-to-a-radically-new-viewpoint-such-as-a-different-orientation-or-scale-on-the-other-hand-people-are-very-good-at-extrapolating-after-seeing-a-new-shape-once-they-can-recognize-it-from-a-different-viewpoint-sup-id-cite-ref-71-class-reference-a-href-cite-note-71-68-a-sup-p-p-currently-the-common-way-to-deal-with-this-problem-is-to-train-the-network-on-transformed-data-in-different-orientations-scales-lighting-etc-so-that-the-network-can-cope-with-these-variations-this-is-computationally-intensive-for-large-data-sets-the-alternative-is-to-use-a-hierarchy-of-coordinate-frames-and-to-use-a-group-of-neurons-to-represent-a-conjunction-of-the-shape-of-the-feature-and-its-pose-relative-to-the-a-href-wiki-retina-title-retina-retina-a-the-pose-relative-to-retina-is-the-relationship-between-the-coordinate-frame-of-the-retina-and-the-intrinsic-features-coordinate-frame-sup-id-cite-ref-72-class-reference-a-href-cite-note-72-69-a-sup-p-p-thus-one-way-of-representing-something-is-to-embed-the-coordinate-frame-within-it-once-this-is-done-large-features-can-be-recognized-by-using-the-consistency-of-the-poses-of-their-parts-e-g-nose-and-mouth-poses-make-a-consistent-prediction-of-the-pose-of-the-whole-face-using-this-approach-ensures-that-the-higher-level-entity-e-g-face-is-present-when-the-lower-level-e-g-nose-and-mouth-agree-on-its-prediction-of-the-pose-the-vectors-of-neuronal-activity-that-represent-pose-pose-vectors-allow-spatial-transformations-modeled-as-linear-operations-that-make-it-easier-for-the-network-to-learn-the-hierarchy-of-visual-entities-and-generalize-across-viewpoints-this-is-similar-to-the-way-the-human-a-href-wiki-visual-system-title-visual-system-visual-system-a-imposes-coordinate-frames-in-order-to-represent-shapes-sup-id-cite-ref-73-class-reference-a-href-cite-note-73-70-a-sup-p-h2-span-class-mw-headline-id-applications-applications-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-43-title-edit-section-applications-edit-a-span-class-mw-editsection-bracket-span-span-h2-h3-span-class-mw-headline-id-image-recognition-image-recognition-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-44-title-edit-section-image-recognition-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-cnns-are-often-used-in-a-href-wiki-image-recognition-class-mw-redirect-title-image-recognition-image-recognition-a-systems-in-2012-an-a-href-wiki-per-comparison-error-rate-title-per-comparison-error-rate-error-rate-a-of-0-23-percent-on-the-a-href-wiki-mnist-database-title-mnist-database-mnist-database-a-was-reported-sup-id-cite-ref-mcdns-14-3-class-reference-a-href-cite-note-mcdns-14-14-a-sup-another-paper-on-using-cnn-for-image-classification-reported-that-the-learning-process-was-surprisingly-fast-in-the-same-paper-the-best-published-results-as-of-2011-were-achieved-in-the-mnist-database-and-the-norb-database-sup-id-cite-ref-flexible-12-2-class-reference-a-href-cite-note-flexible-12-12-a-sup-subsequently-a-similar-cnn-called-a-href-wiki-alexnet-title-alexnet-alexnet-a-sup-id-cite-ref-quartz-74-0-class-reference-a-href-cite-note-quartz-74-71-a-sup-won-the-a-href-wiki-imagenet-large-scale-visual-recognition-challenge-class-mw-redirect-title-imagenet-large-scale-visual-recognition-challenge-imagenet-large-scale-visual-recognition-challenge-a-2012-p-p-when-applied-to-a-href-wiki-facial-recognition-system-title-facial-recognition-system-facial-recognition-a-cnns-achieved-a-large-decrease-in-error-rate-sup-id-cite-ref-75-class-reference-a-href-cite-note-75-72-a-sup-another-paper-reported-a-97-6-percent-recognition-rate-on-5600-still-images-of-more-than-10-subjects-sup-id-cite-ref-robust-face-detection-6-1-class-reference-a-href-cite-note-robust-face-detection-6-6-a-sup-cnns-were-used-to-assess-a-href-wiki-video-quality-title-video-quality-video-quality-a-in-an-objective-way-after-manual-training-the-resulting-system-had-a-very-low-a-href-wiki-root-mean-square-error-class-mw-redirect-title-root-mean-square-error-root-mean-square-error-a-sup-id-cite-ref-video-quality-26-1-class-reference-a-href-cite-note-video-quality-26-26-a-sup-p-p-the-a-href-wiki-imagenet-large-scale-visual-recognition-challenge-class-mw-redirect-title-imagenet-large-scale-visual-recognition-challenge-imagenet-large-scale-visual-recognition-challenge-a-is-a-benchmark-in-object-classification-and-detection-with-millions-of-images-and-hundreds-of-object-classes-in-the-ilsvrc-2014-sup-id-cite-ref-ilsvrc2014-76-0-class-reference-a-href-cite-note-ilsvrc2014-76-73-a-sup-a-large-scale-visual-recognition-challenge-almost-every-highly-ranked-team-used-cnn-as-their-basic-framework-the-winner-a-href-w-index-php-title-googlenet-action-edit-redlink-1-class-new-title-googlenet-page-does-not-exist-googlenet-a-sup-id-cite-ref-googlenet-77-0-class-reference-a-href-cite-note-googlenet-77-74-a-sup-the-foundation-of-a-href-wiki-deepdream-title-deepdream-deepdream-a-increased-the-mean-average-a-href-wiki-precision-and-recall-title-precision-and-recall-precision-a-of-object-detection-to-0-439329-and-reduced-classification-error-to-0-06656-the-best-result-to-date-its-network-applied-more-than-30-layers-that-performance-of-convolutional-neural-networks-on-the-imagenet-tests-was-close-to-that-of-humans-sup-id-cite-ref-78-class-reference-a-href-cite-note-78-75-a-sup-the-best-algorithms-still-struggle-with-objects-that-are-small-or-thin-such-as-a-small-ant-on-a-stem-of-a-flower-or-a-person-holding-a-quill-in-their-hand-they-also-have-trouble-with-images-that-have-been-distorted-with-filters-an-increasingly-common-phenomenon-with-modern-digital-cameras-by-contrast-those-kinds-of-images-rarely-trouble-humans-humans-however-tend-to-have-trouble-with-other-issues-for-example-they-are-not-good-at-classifying-objects-into-fine-grained-categories-such-as-the-particular-breed-of-dog-or-species-of-bird-whereas-convolutional-neural-networks-handle-this-sup-class-noprint-inline-template-template-fact-style-white-space-nowrap-i-a-href-wiki-wikipedia-citation-needed-title-wikipedia-citation-needed-span-title-this-claim-needs-references-to-reliable-sources-june-2019-citation-needed-span-a-i-sup-p-p-in-2015-a-many-layered-cnn-demonstrated-the-ability-to-spot-faces-from-a-wide-range-of-angles-including-upside-down-even-when-partially-occluded-with-competitive-performance-the-network-was-trained-on-a-database-of-200000-images-that-included-faces-at-various-angles-and-orientations-and-a-further-20-million-images-without-faces-they-used-batches-of-128-images-over-50000-iterations-sup-id-cite-ref-79-class-reference-a-href-cite-note-79-76-a-sup-p-h3-span-class-mw-headline-id-video-analysis-video-analysis-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-45-title-edit-section-video-analysis-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-compared-to-image-data-domains-there-is-relatively-little-work-on-applying-cnns-to-video-classification-video-is-more-complex-than-images-since-it-has-another-temporal-dimension-however-some-extensions-of-cnns-into-the-video-domain-have-been-explored-one-approach-is-to-treat-space-and-time-as-equivalent-dimensions-of-the-input-and-perform-convolutions-in-both-time-and-space-sup-id-cite-ref-80-class-reference-a-href-cite-note-80-77-a-sup-sup-id-cite-ref-81-class-reference-a-href-cite-note-81-78-a-sup-another-way-is-to-fuse-the-features-of-two-convolutional-neural-networks-one-for-the-spatial-and-one-for-the-temporal-stream-sup-id-cite-ref-82-class-reference-a-href-cite-note-82-79-a-sup-sup-id-cite-ref-83-class-reference-a-href-cite-note-83-80-a-sup-sup-id-cite-ref-84-class-reference-a-href-cite-note-84-81-a-sup-a-href-wiki-long-short-term-memory-title-long-short-term-memory-long-short-term-memory-a-lstm-a-href-wiki-recurrent-neural-network-title-recurrent-neural-network-recurrent-a-units-are-typically-incorporated-after-the-cnn-to-account-for-inter-frame-or-inter-clip-dependencies-sup-id-cite-ref-wang-duan-zhang-niu-p-1657-85-0-class-reference-a-href-cite-note-wang-duan-zhang-niu-p-1657-85-82-a-sup-sup-id-cite-ref-duan-wang-zhai-zheng-2018-p-86-0-class-reference-a-href-cite-note-duan-wang-zhai-zheng-2018-p-86-83-a-sup-a-href-wiki-unsupervised-learning-title-unsupervised-learning-unsupervised-learning-a-schemes-for-training-spatio-temporal-features-have-been-introduced-based-on-convolutional-gated-restricted-a-href-wiki-boltzmann-machine-title-boltzmann-machine-boltzmann-machines-a-sup-id-cite-ref-87-class-reference-a-href-cite-note-87-84-a-sup-and-independent-subspace-analysis-sup-id-cite-ref-88-class-reference-a-href-cite-note-88-85-a-sup-p-h3-span-class-mw-headline-id-natural-language-processing-natural-language-processing-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-46-title-edit-section-natural-language-processing-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-cnns-have-also-been-explored-for-a-href-wiki-natural-language-processing-title-natural-language-processing-natural-language-processing-a-cnn-models-are-effective-for-various-nlp-problems-and-achieved-excellent-results-in-a-href-wiki-semantic-parsing-title-semantic-parsing-semantic-parsing-a-sup-id-cite-ref-89-class-reference-a-href-cite-note-89-86-a-sup-search-query-retrieval-sup-id-cite-ref-90-class-reference-a-href-cite-note-90-87-a-sup-sentence-modeling-sup-id-cite-ref-91-class-reference-a-href-cite-note-91-88-a-sup-classification-sup-id-cite-ref-92-class-reference-a-href-cite-note-92-89-a-sup-prediction-sup-id-cite-ref-93-class-reference-a-href-cite-note-93-90-a-sup-and-other-traditional-nlp-tasks-sup-id-cite-ref-94-class-reference-a-href-cite-note-94-91-a-sup-p-h3-span-class-mw-headline-id-drug-discovery-drug-discovery-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-47-title-edit-section-drug-discovery-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-cnns-have-been-used-in-a-href-wiki-drug-discovery-title-drug-discovery-drug-discovery-a-predicting-the-interaction-between-molecules-and-biological-a-href-wiki-proteins-class-mw-redirect-title-proteins-proteins-a-can-identify-potential-treatments-in-2015-atomwise-introduced-atomnet-the-first-deep-learning-neural-network-for-structure-based-a-href-wiki-drug-design-title-drug-design-rational-drug-design-a-sup-id-cite-ref-95-class-reference-a-href-cite-note-95-92-a-sup-the-system-trains-directly-on-3-dimensional-representations-of-chemical-interactions-similar-to-how-image-recognition-networks-learn-to-compose-smaller-spatially-proximate-features-into-larger-complex-structures-sup-id-cite-ref-96-class-reference-a-href-cite-note-96-93-a-sup-atomnet-discovers-chemical-features-such-as-a-href-wiki-aromaticity-title-aromaticity-aromaticity-a-a-href-wiki-orbital-hybridisation-title-orbital-hybridisation-sp-sup-3-sup-carbons-a-and-a-href-wiki-hydrogen-bond-title-hydrogen-bond-hydrogen-bonding-a-subsequently-atomnet-was-used-to-predict-novel-candidate-a-href-wiki-biomolecule-title-biomolecule-biomolecules-a-for-multiple-disease-targets-most-notably-treatments-for-the-a-href-wiki-ebola-virus-class-mw-redirect-title-ebola-virus-ebola-virus-a-sup-id-cite-ref-97-class-reference-a-href-cite-note-97-94-a-sup-and-a-href-wiki-multiple-sclerosis-title-multiple-sclerosis-multiple-sclerosis-a-sup-id-cite-ref-98-class-reference-a-href-cite-note-98-95-a-sup-p-h3-span-class-mw-headline-id-health-risk-assessment-and-biomarkers-of-aging-discovery-health-risk-assessment-and-biomarkers-of-aging-discovery-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-48-title-edit-section-health-risk-assessment-and-biomarkers-of-aging-discovery-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-cnns-can-be-naturally-tailored-to-analyze-a-sufficiently-large-collection-of-a-href-wiki-time-series-title-time-series-time-series-a-data-representing-one-week-long-human-physical-activity-streams-augmented-by-the-rich-clinical-data-including-the-death-register-as-provided-by-e-g-the-a-href-wiki-national-health-and-nutrition-examination-survey-title-national-health-and-nutrition-examination-survey-nhanes-a-study-a-simple-cnn-was-combined-with-cox-gompertz-a-href-wiki-proportional-hazards-model-title-proportional-hazards-model-proportional-hazards-model-a-and-used-to-produce-a-proof-of-concept-example-of-digital-a-href-wiki-biomarkers-of-aging-title-biomarkers-of-aging-biomarkers-of-aging-a-in-the-form-of-all-causes-mortality-predictor-sup-id-cite-ref-pmid-29581467-99-0-class-reference-a-href-cite-note-pmid-29581467-99-96-a-sup-p-h3-span-class-mw-headline-id-checkers-game-checkers-game-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-49-title-edit-section-checkers-game-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-cnns-have-been-used-in-the-game-of-a-href-wiki-draughts-title-draughts-checkers-a-from-1999-to-2001-a-href-wiki-david-b-fogel-title-david-b-fogel-fogel-a-and-chellapilla-published-papers-showing-how-a-convolutional-neural-network-could-learn-to-play-checkers-using-co-evolution-the-learning-process-did-not-use-prior-human-professional-games-but-rather-focused-on-a-minimal-set-of-information-contained-in-the-checkerboard-the-location-and-type-of-pieces-and-the-difference-in-number-of-pieces-between-the-two-sides-ultimately-the-program-a-href-wiki-blondie24-title-blondie24-blondie24-a-was-tested-on-165-games-against-players-and-ranked-in-the-highest-0-4-sup-id-cite-ref-100-class-reference-a-href-cite-note-100-97-a-sup-sup-id-cite-ref-101-class-reference-a-href-cite-note-101-98-a-sup-it-also-earned-a-win-against-the-program-a-href-wiki-chinook-draughts-player-title-chinook-draughts-player-chinook-a-at-its-expert-level-of-play-sup-id-cite-ref-102-class-reference-a-href-cite-note-102-99-a-sup-p-h3-span-class-mw-headline-id-go-go-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-50-title-edit-section-go-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-cnns-have-been-used-in-a-href-wiki-computer-go-title-computer-go-computer-go-a-in-december-2014-clark-and-storkey-published-a-paper-showing-that-a-cnn-trained-by-supervised-learning-from-a-database-of-human-professional-games-could-outperform-a-href-wiki-gnu-go-title-gnu-go-gnu-go-a-and-win-some-games-against-a-href-wiki-monte-carlo-tree-search-title-monte-carlo-tree-search-monte-carlo-tree-search-a-fuego-1-1-in-a-fraction-of-the-time-it-took-fuego-to-play-sup-id-cite-ref-103-class-reference-a-href-cite-note-103-100-a-sup-later-it-was-announced-that-a-large-12-layer-convolutional-neural-network-had-correctly-predicted-the-professional-move-in-55-of-positions-equalling-the-accuracy-of-a-a-href-wiki-go-ranks-and-ratings-title-go-ranks-and-ratings-6-dan-a-human-player-when-the-trained-convolutional-network-was-used-directly-to-play-games-of-go-without-any-search-it-beat-the-traditional-search-program-a-href-wiki-gnu-go-title-gnu-go-gnu-go-a-in-97-of-games-and-matched-the-performance-of-the-a-href-wiki-monte-carlo-tree-search-title-monte-carlo-tree-search-monte-carlo-tree-search-a-program-fuego-simulating-ten-thousand-playouts-about-a-million-positions-per-move-sup-id-cite-ref-104-class-reference-a-href-cite-note-104-101-a-sup-p-p-a-couple-of-cnns-for-choosing-moves-to-try-policy-network-and-evaluating-positions-value-network-driving-mcts-were-used-by-a-href-wiki-alphago-title-alphago-alphago-a-the-first-to-beat-the-best-human-player-at-the-time-sup-id-cite-ref-105-class-reference-a-href-cite-note-105-102-a-sup-p-h2-span-class-mw-headline-id-fine-tuning-fine-tuning-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-51-title-edit-section-fine-tuning-edit-a-span-class-mw-editsection-bracket-span-span-h2-p-for-many-applications-little-training-data-is-available-convolutional-neural-networks-usually-require-a-large-amount-of-training-data-in-order-to-avoid-a-href-wiki-overfitting-title-overfitting-overfitting-a-a-common-technique-is-to-train-the-network-on-a-larger-data-set-from-a-related-domain-once-the-network-parameters-have-converged-an-additional-training-step-is-performed-using-the-in-domain-data-to-fine-tune-the-network-weights-this-allows-convolutional-networks-to-be-successfully-applied-to-problems-with-small-training-sets-sup-id-cite-ref-106-class-reference-a-href-cite-note-106-103-a-sup-p-h2-span-class-mw-headline-id-human-interpretable-explanations-human-interpretable-explanations-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-52-title-edit-section-human-interpretable-explanations-edit-a-span-class-mw-editsection-bracket-span-span-h2-p-end-to-end-training-and-prediction-are-common-practice-in-a-href-wiki-computer-vision-title-computer-vision-computer-vision-a-however-human-interpretable-explanations-are-required-for-a-href-wiki-safety-critical-system-title-safety-critical-system-critical-systems-a-such-as-a-a-href-wiki-self-driving-car-class-mw-redirect-title-self-driving-car-self-driving-cars-a-sup-id-cite-ref-interpretable-ml-symposium-2017-107-0-class-reference-a-href-cite-note-interpretable-ml-symposium-2017-107-104-a-sup-with-recent-advances-in-a-href-wiki-salience-neuroscience-title-salience-neuroscience-visual-salience-a-a-href-wiki-visual-spatial-attention-title-visual-spatial-attention-spatial-a-and-a-href-wiki-visual-temporal-attention-title-visual-temporal-attention-temporal-attention-a-the-most-critical-spatial-regions-temporal-instants-could-be-visualized-to-justify-the-cnn-predictions-sup-id-cite-ref-zang-wang-liu-zhang-2018-pp-97-108-108-0-class-reference-a-href-cite-note-zang-wang-liu-zhang-2018-pp-97-108-108-105-a-sup-sup-id-cite-ref-wang-zang-zhang-niu-p-1979-109-0-class-reference-a-href-cite-note-wang-zang-zhang-niu-p-1979-109-106-a-sup-p-h2-span-class-mw-headline-id-related-architectures-related-architectures-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-53-title-edit-section-related-architectures-edit-a-span-class-mw-editsection-bracket-span-span-h2-h3-span-class-mw-headline-id-deep-q-networks-deep-q-networks-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-54-title-edit-section-deep-q-networks-edit-a-span-class-mw-editsection-bracket-span-span-h3-p-a-deep-q-network-dqn-is-a-type-of-deep-learning-model-that-combines-a-deep-cnn-with-a-href-wiki-q-learning-title-q-learning-q-learning-a-a-form-of-a-href-wiki-reinforcement-learning-title-reinforcement-learning-reinforcement-learning-a-unlike-earlier-reinforcement-learning-agents-dqns-can-learn-directly-from-high-dimensional-sensory-inputs-sup-class-noprint-inline-template-template-fact-style-white-space-nowrap-i-a-href-wiki-wikipedia-citation-needed-title-wikipedia-citation-needed-span-title-this-claim-needs-references-to-reliable-sources-june-2019-citation-needed-span-a-i-sup-p-p-preliminary-results-were-presented-in-2014-with-an-accompanying-paper-in-february-2015-sup-id-cite-ref-dqn-110-0-class-reference-a-href-cite-note-dqn-110-107-a-sup-the-research-described-an-application-to-a-href-wiki-atari-2600-title-atari-2600-atari-2600-a-gaming-other-deep-reinforcement-learning-models-preceded-it-sup-id-cite-ref-111-class-reference-a-href-cite-note-111-108-a-sup-p-h3-span-class-mw-headline-id-deep-belief-networks-deep-belief-networks-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-55-title-edit-section-deep-belief-networks-edit-a-span-class-mw-editsection-bracket-span-span-h3-div-role-note-class-hatnote-navigation-not-searchable-main-article-a-href-wiki-deep-belief-network-title-deep-belief-network-deep-belief-network-a-div-p-convolutional-deep-belief-networks-cdbn-have-structure-very-similar-to-convolutional-neural-networks-and-are-trained-similarly-to-deep-belief-networks-therefore-they-exploit-the-2d-structure-of-images-like-cnns-do-and-make-use-of-pre-training-like-a-href-wiki-deep-belief-network-title-deep-belief-network-deep-belief-networks-a-they-provide-a-generic-structure-that-can-be-used-in-many-image-and-signal-processing-tasks-benchmark-results-on-standard-image-datasets-like-cifar-sup-id-cite-ref-cdbn-cifar-112-0-class-reference-a-href-cite-note-cdbn-cifar-112-109-a-sup-have-been-obtained-using-cdbns-sup-id-cite-ref-cdbn-113-0-class-reference-a-href-cite-note-cdbn-113-110-a-sup-p-h2-span-class-mw-headline-id-notable-libraries-notable-libraries-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-56-title-edit-section-notable-libraries-edit-a-span-class-mw-editsection-bracket-span-span-h2-ul-li-a-href-wiki-caffe-software-title-caffe-software-caffe-a-a-library-for-convolutional-neural-networks-created-by-the-berkeley-vision-and-learning-center-bvlc-it-supports-both-cpu-and-gpu-developed-in-a-href-wiki-c-2b-2b-title-c-c-a-and-has-a-href-wiki-python-programming-language-title-python-programming-language-python-a-and-a-href-wiki-matlab-title-matlab-matlab-a-wrappers-li-li-a-href-wiki-deeplearning4j-title-deeplearning4j-deeplearning4j-a-deep-learning-in-a-href-wiki-java-programming-language-title-java-programming-language-java-a-and-a-href-wiki-scala-programming-language-title-scala-programming-language-scala-a-on-multi-gpu-enabled-a-href-wiki-apache-spark-title-apache-spark-spark-a-a-general-purpose-deep-learning-library-for-the-jvm-production-stack-running-on-a-c-scientific-computing-engine-allows-the-creation-of-custom-layers-integrates-with-hadoop-and-kafka-li-li-a-href-wiki-dlib-title-dlib-dlib-a-a-toolkit-for-making-real-world-machine-learning-and-data-analysis-applications-in-c-li-li-a-href-wiki-microsoft-cognitive-toolkit-title-microsoft-cognitive-toolkit-microsoft-cognitive-toolkit-a-a-deep-learning-toolkit-written-by-microsoft-with-several-unique-features-enhancing-scalability-over-multiple-nodes-it-supports-full-fledged-interfaces-for-training-in-c-and-python-and-with-additional-support-for-model-inference-in-a-href-wiki-c-sharp-programming-language-title-c-sharp-programming-language-c-a-and-java-li-li-a-href-wiki-tensorflow-title-tensorflow-tensorflow-a-a-href-wiki-apache-license-version-2-0-title-apache-license-apache-2-0-a-licensed-theano-like-library-with-support-for-cpu-gpu-google-s-proprietary-a-href-wiki-tensor-processing-unit-title-tensor-processing-unit-tensor-processing-unit-a-tpu-sup-id-cite-ref-114-class-reference-a-href-cite-note-114-111-a-sup-and-mobile-devices-li-li-a-href-wiki-theano-software-title-theano-software-theano-a-the-reference-deep-learning-library-for-python-with-an-api-largely-compatible-with-the-popular-a-href-wiki-numpy-title-numpy-numpy-a-library-allows-user-to-write-symbolic-mathematical-expressions-then-automatically-generates-their-derivatives-saving-the-user-from-having-to-code-gradients-or-backpropagation-these-symbolic-expressions-are-automatically-compiled-to-a-href-wiki-cuda-title-cuda-cuda-a-code-for-a-fast-a-href-wiki-compute-kernel-title-compute-kernel-on-the-gpu-a-implementation-li-li-a-href-wiki-torch-machine-learning-title-torch-machine-learning-torch-a-a-a-href-wiki-scientific-computing-class-mw-redirect-title-scientific-computing-scientific-computing-a-framework-with-wide-support-for-machine-learning-algorithms-written-in-a-href-wiki-c-programming-language-title-c-programming-language-c-a-and-a-href-wiki-lua-programming-language-title-lua-programming-language-lua-a-the-main-author-is-ronan-collobert-and-it-is-now-used-at-facebook-ai-research-and-twitter-li-ul-h2-span-class-mw-headline-id-notable-apis-notable-apis-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-57-title-edit-section-notable-apis-edit-a-span-class-mw-editsection-bracket-span-span-h2-ul-li-a-href-wiki-keras-title-keras-keras-a-a-high-level-api-written-in-a-href-wiki-python-programming-language-title-python-programming-language-python-a-for-a-href-wiki-tensorflow-title-tensorflow-tensorflow-a-and-a-href-wiki-theano-software-title-theano-software-theano-a-convolutional-neural-networks-sup-id-cite-ref-115-class-reference-a-href-cite-note-115-112-a-sup-li-ul-h2-span-class-mw-headline-id-see-also-see-also-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-58-title-edit-section-see-also-edit-a-span-class-mw-editsection-bracket-span-span-h2-ul-li-a-href-wiki-convolution-title-convolution-convolution-a-li-li-a-href-wiki-deep-learning-title-deep-learning-deep-learning-a-li-li-a-href-wiki-natural-language-processing-class-mw-redirect-title-natural-language-processing-natural-language-processing-a-li-li-a-href-wiki-neocognitron-title-neocognitron-neocognitron-a-li-li-a-href-wiki-scale-invariant-feature-transform-title-scale-invariant-feature-transform-scale-invariant-feature-transform-a-li-li-a-href-wiki-time-delay-neural-network-title-time-delay-neural-network-time-delay-neural-network-a-li-li-a-href-wiki-vision-processing-unit-title-vision-processing-unit-vision-processing-unit-a-li-ul-h2-span-class-mw-headline-id-notes-notes-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-59-title-edit-section-notes-edit-a-span-class-mw-editsection-bracket-span-span-h2-div-class-reflist-style-list-style-type-decimal-div-class-mw-references-wrap-ol-class-references-li-id-cite-note-52-span-class-mw-cite-backlink-b-a-href-cite-ref-52-a-b-span-span-class-reference-text-when-applied-to-other-types-of-data-than-image-data-such-as-sound-data-spatial-position-may-variously-correspond-to-different-points-in-the-a-href-wiki-time-domain-title-time-domain-time-domain-a-a-href-wiki-frequency-domain-title-frequency-domain-frequency-domain-a-or-other-a-href-wiki-space-mathematics-title-space-mathematics-mathematical-spaces-a-span-li-li-id-cite-note-54-span-class-mw-cite-backlink-b-a-href-cite-ref-54-a-b-span-span-class-reference-text-hence-the-name-b-convolutional-layer-b-span-li-li-id-cite-note-62-span-class-mw-cite-backlink-b-a-href-cite-ref-62-a-b-span-span-class-reference-text-so-called-a-href-wiki-categorical-data-class-mw-redirect-title-categorical-data-categorical-data-a-span-li-ol-div-div-h2-span-class-mw-headline-id-references-references-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-60-title-edit-section-references-edit-a-span-class-mw-editsection-bracket-span-span-h2-div-class-reflist-columns-references-column-width-style-moz-column-width-30em-webkit-column-width-30em-column-width-30em-list-style-type-decimal-ol-class-references-li-id-cite-note-0-1-span-class-mw-cite-backlink-a-href-cite-ref-0-1-0-sup-i-b-a-b-i-sup-a-a-href-cite-ref-0-1-1-sup-i-b-b-b-i-sup-a-span-span-class-reference-text-cite-class-citation-journal-zhang-wei-1988-a-rel-nofollow-class-external-text-href-https-drive-google-com-file-d-0b65v6wo67tk5zm03tm1kaediyke-view-usp-sharing-shift-invariant-pattern-recognition-neural-network-and-its-optical-architecture-a-i-proceedings-of-annual-conference-of-the-japan-society-of-applied-physics-i-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-proceedings-of-annual-conference-of-the-japan-society-of-applied-physics-rft-atitle-shift-invariant-pattern-recognition-neural-network-and-its-optical-architecture-rft-date-1988-rft-aulast-zhang-rft-aufirst-wei-rft-id-https-3a-2f-2fdrive-google-com-2ffile-2fd-2f0b65v6wo67tk5zm03tm1kaediyke-2fview-3fusp-3dsharing-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-style-data-mw-deduplicate-templatestyles-r886058088-mw-parser-output-cite-citation-font-style-inherit-mw-parser-output-citation-q-quotes-mw-parser-output-citation-cs1-lock-free-a-background-url-upload-wikimedia-org-wikipedia-commons-thumb-6-65-lock-green-svg-9px-lock-green-svg-png-no-repeat-background-position-right-1em-center-mw-parser-output-citation-cs1-lock-limited-a-mw-parser-output-citation-cs1-lock-registration-a-background-url-upload-wikimedia-org-wikipedia-commons-thumb-d-d6-lock-gray-alt-2-svg-9px-lock-gray-alt-2-svg-png-no-repeat-background-position-right-1em-center-mw-parser-output-citation-cs1-lock-subscription-a-background-url-upload-wikimedia-org-wikipedia-commons-thumb-a-aa-lock-red-alt-2-svg-9px-lock-red-alt-2-svg-png-no-repeat-background-position-right-1em-center-mw-parser-output-cs1-subscription-mw-parser-output-cs1-registration-color-555-mw-parser-output-cs1-subscription-span-mw-parser-output-cs1-registration-span-border-bottom-1px-dotted-cursor-help-mw-parser-output-cs1-ws-icon-a-background-url-upload-wikimedia-org-wikipedia-commons-thumb-4-4c-wikisource-logo-svg-12px-wikisource-logo-svg-png-no-repeat-background-position-right-1em-center-mw-parser-output-code-cs1-code-color-inherit-background-inherit-border-inherit-padding-inherit-mw-parser-output-cs1-hidden-error-display-none-font-size-100-mw-parser-output-cs1-visible-error-font-size-100-mw-parser-output-cs1-maint-display-none-color-33aa33-margin-left-0-3em-mw-parser-output-cs1-subscription-mw-parser-output-cs1-registration-mw-parser-output-cs1-format-font-size-95-mw-parser-output-cs1-kern-left-mw-parser-output-cs1-kern-wl-left-padding-left-0-2em-mw-parser-output-cs1-kern-right-mw-parser-output-cs1-kern-wl-right-padding-right-0-2em-style-span-li-li-id-cite-note-1-2-span-class-mw-cite-backlink-a-href-cite-ref-1-2-0-sup-i-b-a-b-i-sup-a-a-href-cite-ref-1-2-1-sup-i-b-b-b-i-sup-a-span-span-class-reference-text-cite-class-citation-journal-zhang-wei-1990-a-rel-nofollow-class-external-text-href-https-drive-google-com-file-d-0b65v6wo67tk5odrzzmhsr29vedg-view-usp-sharing-parallel-distributed-processing-model-with-local-space-invariant-interconnections-and-its-optical-architecture-a-i-applied-optics-i-b-29-b-32-4790-7-a-href-wiki-bibcode-title-bibcode-bibcode-a-a-rel-nofollow-class-external-text-href-http-adsabs-harvard-edu-abs-1990apopt-29-4790z-1990apopt-29-4790z-a-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1364-2fao-29-004790-10-1364-ao-29-004790-a-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-20577468-20577468-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-applied-optics-rft-atitle-parallel-distributed-processing-model-with-local-space-invariant-interconnections-and-its-optical-architecture-rft-volume-29-rft-issue-32-rft-pages-4790-7-rft-date-1990-rft-id-info-3apmid-2f20577468-rft-id-info-3adoi-2f10-1364-2fao-29-004790-rft-id-info-3abibcode-2f1990apopt-29-4790z-rft-aulast-zhang-rft-aufirst-wei-rft-id-https-3a-2f-2fdrive-google-com-2ffile-2fd-2f0b65v6wo67tk5odrzzmhsr29vedg-2fview-3fusp-3dsharing-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-fukuneoscholar-3-span-class-mw-cite-backlink-a-href-cite-ref-fukuneoscholar-3-0-sup-i-b-a-b-i-sup-a-a-href-cite-ref-fukuneoscholar-3-1-sup-i-b-b-b-i-sup-a-a-href-cite-ref-fukuneoscholar-3-2-sup-i-b-c-b-i-sup-a-span-span-class-reference-text-cite-class-citation-journal-fukushima-k-2007-neocognitron-i-scholarpedia-i-b-2-b-1-1717-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-4249-2fscholarpedia-1717-10-4249-scholarpedia-1717-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-scholarpedia-rft-atitle-neocognitron-rft-volume-2-rft-issue-1-rft-pages-1717-rft-date-2007-rft-id-info-3adoi-2f10-4249-2fscholarpedia-1717-rft-aulast-fukushima-rft-aufirst-k-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-hubelwiesel1968-4-span-class-mw-cite-backlink-a-href-cite-ref-hubelwiesel1968-4-0-sup-i-b-a-b-i-sup-a-a-href-cite-ref-hubelwiesel1968-4-1-sup-i-b-b-b-i-sup-a-span-span-class-reference-text-cite-class-citation-journal-hubel-d-h-wiesel-t-n-1968-03-01-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pmc-articles-pmc1557912-receptive-fields-and-functional-architecture-of-monkey-striate-cortex-a-i-the-journal-of-physiology-i-b-195-b-1-215-243-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1113-2fjphysiol-1968-sp008455-10-1113-jphysiol-1968-sp008455-a-a-href-wiki-international-standard-serial-number-title-international-standard-serial-number-issn-a-a-rel-nofollow-class-external-text-href-www-worldcat-org-issn-0022-3751-0022-3751-a-a-href-wiki-pubmed-central-title-pubmed-central-pmc-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pmc-articles-pmc1557912-1557912-a-span-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-4966457-4966457-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-the-journal-of-physiology-rft-atitle-receptive-fields-and-functional-architecture-of-monkey-striate-cortex-rft-volume-195-rft-issue-1-rft-pages-215-243-rft-date-1968-03-01-rft-id-2f-2fwww-ncbi-nlm-nih-gov-2fpmc-2farticles-2fpmc1557912-rft-issn-0022-3751-rft-id-info-3apmid-2f4966457-rft-id-info-3adoi-2f10-1113-2fjphysiol-1968-sp008455-rft-aulast-hubel-rft-aufirst-d-h-rft-au-wiesel-2c-t-n-rft-id-2f-2fwww-ncbi-nlm-nih-gov-2fpmc-2farticles-2fpmc1557912-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-intro-5-span-class-mw-cite-backlink-a-href-cite-ref-intro-5-0-sup-i-b-a-b-i-sup-a-a-href-cite-ref-intro-5-1-sup-i-b-b-b-i-sup-a-span-span-class-reference-text-cite-class-citation-journal-fukushima-kunihiko-1980-a-rel-nofollow-class-external-text-href-http-www-cs-princeton-edu-courses-archive-spr08-cos598b-readings-fukushima1980-pdf-neocognitron-a-self-organizing-neural-network-model-for-a-mechanism-of-pattern-recognition-unaffected-by-shift-in-position-a-span-class-cs1-format-pdf-span-i-biological-cybernetics-i-b-36-b-4-193-202-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1007-2fbf00344251-10-1007-bf00344251-a-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-7370364-7370364-a-span-class-reference-accessdate-retrieved-span-class-nowrap-16-november-span-2013-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-biological-cybernetics-rft-atitle-neocognitron-3a-a-self-organizing-neural-network-model-for-a-mechanism-of-pattern-recognition-unaffected-by-shift-in-position-rft-volume-36-rft-issue-4-rft-pages-193-202-rft-date-1980-rft-id-info-3adoi-2f10-1007-2fbf00344251-rft-id-info-3apmid-2f7370364-rft-aulast-fukushima-rft-aufirst-kunihiko-rft-id-http-3a-2f-2fwww-cs-princeton-edu-2fcourses-2farchive-2fspr08-2fcos598b-2freadings-2ffukushima1980-pdf-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-robust-face-detection-6-span-class-mw-cite-backlink-a-href-cite-ref-robust-face-detection-6-0-sup-i-b-a-b-i-sup-a-a-href-cite-ref-robust-face-detection-6-1-sup-i-b-b-b-i-sup-a-span-span-class-reference-text-cite-class-citation-journal-matusugu-masakazu-katsuhiko-mori-yusuke-mitari-yuji-kaneda-2003-a-rel-nofollow-class-external-text-href-http-www-iro-umontreal-ca-pift6080-h09-documents-papers-sparse-matsugo-etal-face-expression-conv-nnet-pdf-subject-independent-facial-expression-recognition-with-robust-face-detection-using-a-convolutional-neural-network-a-span-class-cs1-format-pdf-span-i-neural-networks-i-b-16-b-5-555-559-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1016-2fs0893-6080-2803-2900115-1-10-1016-s0893-6080-03-00115-1-a-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-12850007-12850007-a-span-class-reference-accessdate-retrieved-span-class-nowrap-17-november-span-2013-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-neural-networks-rft-atitle-subject-independent-facial-expression-recognition-with-robust-face-detection-using-a-convolutional-neural-network-rft-volume-16-rft-issue-5-rft-pages-555-559-rft-date-2003-rft-id-info-3adoi-2f10-1016-2fs0893-6080-2803-2900115-1-rft-id-info-3apmid-2f12850007-rft-aulast-matusugu-rft-aufirst-masakazu-rft-au-katsuhiko-mori-rft-au-yusuke-mitari-rft-au-yuji-kaneda-rft-id-http-3a-2f-2fwww-iro-umontreal-ca-2f-pift6080-2fh09-2fdocuments-2fpapers-2fsparse-2fmatsugo-etal-face-expression-conv-nnet-pdf-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-7-span-class-mw-cite-backlink-b-a-href-cite-ref-7-a-b-span-span-class-reference-text-cite-class-citation-book-van-den-oord-aaron-dieleman-sander-schrauwen-benjamin-2013-01-01-burges-c-j-c-bottou-l-welling-m-ghahramani-z-weinberger-k-q-eds-a-rel-nofollow-class-external-text-href-http-papers-nips-cc-paper-5004-deep-content-based-music-recommendation-pdf-i-deep-content-based-music-recommendation-i-a-span-class-cs1-format-pdf-span-curran-associates-inc-pp-2643-2651-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-book-rft-btitle-deep-content-based-music-recommendation-rft-pages-2643-2651-rft-pub-curran-associates-2c-inc-rft-date-2013-01-01-rft-aulast-van-den-oord-rft-aufirst-aaron-rft-au-dieleman-2c-sander-rft-au-schrauwen-2c-benjamin-rft-id-http-3a-2f-2fpapers-nips-cc-2fpaper-2f5004-deep-content-based-music-recommendation-pdf-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-8-span-class-mw-cite-backlink-b-a-href-cite-ref-8-a-b-span-span-class-reference-text-cite-class-citation-book-collobert-ronan-weston-jason-2008-01-01-i-a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning-i-i-proceedings-of-the-25th-international-conference-on-machine-learning-i-icml-08-new-york-ny-usa-acm-pp-160-167-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1145-2f1390156-1390177-10-1145-1390156-1390177-a-a-href-wiki-international-standard-book-number-title-international-standard-book-number-isbn-a-a-href-wiki-special-booksources-978-1-60558-205-4-title-special-booksources-978-1-60558-205-4-bdi-978-1-60558-205-4-bdi-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-book-rft-btitle-a-unified-architecture-for-natural-language-processing-3a-deep-neural-networks-with-multitask-learning-rft-place-new-york-2c-ny-2c-usa-rft-series-icml-2708-rft-pages-160-167-rft-pub-acm-rft-date-2008-01-01-rft-id-info-3adoi-2f10-1145-2f1390156-1390177-rft-isbn-978-1-60558-205-4-rft-aulast-collobert-rft-aufirst-ronan-rft-au-weston-2c-jason-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-9-span-class-mw-cite-backlink-b-a-href-cite-ref-9-a-b-span-span-class-reference-text-cite-class-citation-web-a-rel-nofollow-class-external-text-href-https-cs231n-github-io-convolutional-networks-cs231n-convolutional-neural-networks-for-visual-recognition-a-i-cs231n-github-io-i-span-class-reference-accessdate-retrieved-span-class-nowrap-2018-12-13-span-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-unknown-rft-jtitle-cs231n-github-io-rft-atitle-cs231n-convolutional-neural-networks-for-visual-recognition-rft-id-https-3a-2f-2fcs231n-github-io-2fconvolutional-networks-2f-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-deeplearning-10-span-class-mw-cite-backlink-b-a-href-cite-ref-deeplearning-10-0-a-b-span-span-class-reference-text-cite-class-citation-web-a-rel-nofollow-class-external-text-href-http-deeplearning-net-tutorial-lenet-html-convolutional-neural-networks-lenet-deeplearning-0-1-documentation-a-i-deeplearning-0-1-i-lisa-lab-span-class-reference-accessdate-retrieved-span-class-nowrap-31-august-span-2013-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-unknown-rft-jtitle-deeplearning-0-1-rft-atitle-convolutional-neural-networks-28lenet-29-e2-80-93-deeplearning-0-1-documentation-rft-id-http-3a-2f-2fdeeplearning-net-2ftutorial-2flenet-html-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-11-span-class-mw-cite-backlink-b-a-href-cite-ref-11-a-b-span-span-class-reference-text-cite-class-citation-book-habibi-aghdam-hamed-2017-05-30-i-guide-to-convolutional-neural-networks-a-practical-application-to-traffic-sign-detection-and-classification-i-heravi-elnaz-jahani-cham-switzerland-a-href-wiki-international-standard-book-number-title-international-standard-book-number-isbn-a-a-href-wiki-special-booksources-9783319575490-title-special-booksources-9783319575490-bdi-9783319575490-bdi-a-a-href-wiki-oclc-title-oclc-oclc-a-a-rel-nofollow-class-external-text-href-www-worldcat-org-oclc-987790957-987790957-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-book-rft-btitle-guide-to-convolutional-neural-networks-3a-a-practical-application-to-traffic-sign-detection-and-classification-rft-place-cham-2c-switzerland-rft-date-2017-05-30-rft-id-info-3aoclcnum-2f987790957-rft-isbn-9783319575490-rft-aulast-habibi-rft-aufirst-aghdam-2c-hamed-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-flexible-12-span-class-mw-cite-backlink-a-href-cite-ref-flexible-12-0-sup-i-b-a-b-i-sup-a-a-href-cite-ref-flexible-12-1-sup-i-b-b-b-i-sup-a-a-href-cite-ref-flexible-12-2-sup-i-b-c-b-i-sup-a-span-span-class-reference-text-cite-class-citation-journal-ciresan-dan-ueli-meier-jonathan-masci-luca-m-gambardella-jurgen-schmidhuber-2011-a-rel-nofollow-class-external-text-href-http-www-idsia-ch-juergen-ijcai2011-pdf-flexible-high-performance-convolutional-neural-networks-for-image-classification-a-span-class-cs1-format-pdf-span-i-proceedings-of-the-twenty-second-international-joint-conference-on-artificial-intelligence-volume-volume-two-i-b-2-b-1237-1242-span-class-reference-accessdate-retrieved-span-class-nowrap-17-november-span-2013-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-proceedings-of-the-twenty-second-international-joint-conference-on-artificial-intelligence-volume-volume-two-rft-atitle-flexible-2c-high-performance-convolutional-neural-networks-for-image-classification-rft-volume-2-rft-pages-1237-1242-rft-date-2011-rft-aulast-ciresan-rft-aufirst-dan-rft-au-ueli-meier-rft-au-jonathan-masci-rft-au-luca-m-gambardella-rft-au-jurgen-schmidhuber-rft-id-http-3a-2f-2fwww-idsia-ch-2f-juergen-2fijcai2011-pdf-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-13-span-class-mw-cite-backlink-b-a-href-cite-ref-13-a-b-span-span-class-reference-text-cite-class-citation-web-a-href-w-index-php-title-alex-krizhevsky-action-edit-redlink-1-class-new-title-alex-krizhevsky-page-does-not-exist-krizhevsky-a-alex-a-rel-nofollow-class-external-text-href-http-www-image-net-org-challenges-lsvrc-2012-supervision-pdf-imagenet-classification-with-deep-convolutional-neural-networks-a-span-class-cs1-format-pdf-span-span-class-reference-accessdate-retrieved-span-class-nowrap-17-november-span-2013-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-unknown-rft-btitle-imagenet-classification-with-deep-convolutional-neural-networks-rft-aulast-krizhevsky-rft-aufirst-alex-rft-id-http-3a-2f-2fwww-image-net-org-2fchallenges-2flsvrc-2f2012-2fsupervision-pdf-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-mcdns-14-span-class-mw-cite-backlink-a-href-cite-ref-mcdns-14-0-sup-i-b-a-b-i-sup-a-a-href-cite-ref-mcdns-14-1-sup-i-b-b-b-i-sup-a-a-href-cite-ref-mcdns-14-2-sup-i-b-c-b-i-sup-a-a-href-cite-ref-mcdns-14-3-sup-i-b-d-b-i-sup-a-span-span-class-reference-text-cite-class-citation-book-ciresan-dan-meier-ueli-schmidhuber-jurgen-june-2012-i-multi-column-deep-neural-networks-for-image-classification-i-i-2012-ieee-conference-on-computer-vision-and-pattern-recognition-i-new-york-ny-a-href-wiki-institute-of-electrical-and-electronics-engineers-title-institute-of-electrical-and-electronics-engineers-institute-of-electrical-and-electronics-engineers-a-ieee-pp-3642-3649-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1202-2745-1202-2745-a-span-a-href-wiki-citeseerx-title-citeseerx-citeseerx-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-citeseerx-ist-psu-edu-viewdoc-summary-doi-10-1-1-300-3283-10-1-1-300-3283-a-span-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1109-2fcvpr-2012-6248110-10-1109-cvpr-2012-6248110-a-a-href-wiki-international-standard-book-number-title-international-standard-book-number-isbn-a-a-href-wiki-special-booksources-978-1-4673-1226-4-title-special-booksources-978-1-4673-1226-4-bdi-978-1-4673-1226-4-bdi-a-a-href-wiki-oclc-title-oclc-oclc-a-a-rel-nofollow-class-external-text-href-www-worldcat-org-oclc-812295155-812295155-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-book-rft-btitle-multi-column-deep-neural-networks-for-image-classification-rft-place-new-york-2c-ny-rft-pages-3642-3649-rft-pub-institute-of-electrical-and-electronics-engineers-28ieee-29-rft-date-2012-06-rft-id-info-3adoi-2f10-1109-2fcvpr-2012-6248110-rft-id-2f-2fciteseerx-ist-psu-edu-2fviewdoc-2fsummary-3fdoi-3d10-1-1-300-3283-rft-id-info-3aarxiv-2f1202-2745-rft-id-info-3aoclcnum-2f812295155-rft-isbn-978-1-4673-1226-4-rft-aulast-ciresan-rft-aufirst-dan-rft-au-meier-2c-ueli-rft-au-schmidhuber-2c-j-c3-bcrgen-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-cnnbackground-15-span-class-mw-cite-backlink-b-a-href-cite-ref-cnnbackground-15-0-a-b-span-span-class-reference-text-a-rel-nofollow-class-external-text-href-https-www-academia-edu-37491583-a-survey-of-fpga-based-accelerators-for-convolutional-neural-networks-a-survey-of-fpga-based-accelerators-for-convolutional-neural-networks-a-ncaa-2018-span-li-li-id-cite-note-lecun-16-span-class-mw-cite-backlink-b-a-href-cite-ref-lecun-16-0-a-b-span-span-class-reference-text-cite-class-citation-web-lecun-yann-a-rel-nofollow-class-external-text-href-http-yann-lecun-com-exdb-lenet-lenet-5-convolutional-neural-networks-a-span-class-reference-accessdate-retrieved-span-class-nowrap-16-november-span-2013-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-unknown-rft-btitle-lenet-5-2c-convolutional-neural-networks-rft-aulast-lecun-rft-aufirst-yann-rft-id-http-3a-2f-2fyann-lecun-com-2fexdb-2flenet-2f-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-17-span-class-mw-cite-backlink-b-a-href-cite-ref-17-a-b-span-span-class-reference-text-cite-class-citation-book-david-h-hubel-and-torsten-n-wiesel-2005-a-rel-nofollow-class-external-text-href-https-books-google-com-books-id-8yrxwojxua4c-pg-pa106-i-brain-and-visual-perception-the-story-of-a-25-year-collaboration-i-a-oxford-university-press-us-p-106-a-href-wiki-international-standard-book-number-title-international-standard-book-number-isbn-a-a-href-wiki-special-booksources-978-0-19-517618-6-title-special-booksources-978-0-19-517618-6-bdi-978-0-19-517618-6-bdi-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-book-rft-btitle-brain-and-visual-perception-3a-the-story-of-a-25-year-collaboration-rft-pages-106-rft-pub-oxford-university-press-us-rft-date-2005-rft-isbn-978-0-19-517618-6-rft-au-david-h-hubel-and-torsten-n-wiesel-rft-id-https-3a-2f-2fbooks-google-com-2fbooks-3fid-3d8yrxwojxua4c-26pg-3dpa106-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-18-span-class-mw-cite-backlink-b-a-href-cite-ref-18-a-b-span-span-class-reference-text-cite-class-citation-journal-hubel-dh-wiesel-tn-october-1959-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pmc-articles-pmc1363130-receptive-fields-of-single-neurones-in-the-cat-s-striate-cortex-a-i-j-physiol-i-b-148-b-3-574-91-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1113-2fjphysiol-1959-sp006308-10-1113-jphysiol-1959-sp006308-a-a-href-wiki-pubmed-central-title-pubmed-central-pmc-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pmc-articles-pmc1363130-1363130-a-span-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-14403679-14403679-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-j-physiol-rft-atitle-receptive-fields-of-single-neurones-in-the-cat-27s-striate-cortex-rft-volume-148-rft-issue-3-rft-pages-574-91-rft-date-1959-10-rft-id-2f-2fwww-ncbi-nlm-nih-gov-2fpmc-2farticles-2fpmc1363130-rft-id-info-3apmid-2f14403679-rft-id-info-3adoi-2f10-1113-2fjphysiol-1959-sp006308-rft-aulast-hubel-rft-aufirst-dh-rft-au-wiesel-2c-tn-rft-id-2f-2fwww-ncbi-nlm-nih-gov-2fpmc-2farticles-2fpmc1363130-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-19-span-class-mw-cite-backlink-b-a-href-cite-ref-19-a-b-span-span-class-reference-text-cite-class-citation-journal-lecun-yann-bengio-yoshua-hinton-geoffrey-2015-deep-learning-i-nature-i-b-521-b-7553-436-444-a-href-wiki-bibcode-title-bibcode-bibcode-a-a-rel-nofollow-class-external-text-href-http-adsabs-harvard-edu-abs-2015natur-521-436l-2015natur-521-436l-a-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1038-2fnature14539-10-1038-nature14539-a-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-26017442-26017442-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-nature-rft-atitle-deep-learning-rft-volume-521-rft-issue-7553-rft-pages-436-444-rft-date-2015-rft-id-info-3apmid-2f26017442-rft-id-info-3adoi-2f10-1038-2fnature14539-rft-id-info-3abibcode-2f2015natur-521-436l-rft-aulast-lecun-rft-aufirst-yann-rft-au-bengio-2c-yoshua-rft-au-hinton-2c-geoffrey-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-weng1993-20-span-class-mw-cite-backlink-b-a-href-cite-ref-weng1993-20-0-a-b-span-span-class-reference-text-cite-class-citation-journal-weng-j-ahuja-n-huang-ts-1993-learning-recognition-and-segmentation-of-3-d-objects-from-2-d-images-i-proc-4th-international-conf-computer-vision-i-121-128-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-proc-4th-international-conf-computer-vision-rft-atitle-learning-recognition-and-segmentation-of-3-d-objects-from-2-d-images-rft-pages-121-128-rft-date-1993-rft-aulast-weng-rft-aufirst-j-rft-au-ahuja-2c-n-rft-au-huang-2c-ts-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-schdeepscholar-21-span-class-mw-cite-backlink-a-href-cite-ref-schdeepscholar-21-0-sup-i-b-a-b-i-sup-a-a-href-cite-ref-schdeepscholar-21-1-sup-i-b-b-b-i-sup-a-a-href-cite-ref-schdeepscholar-21-2-sup-i-b-c-b-i-sup-a-span-span-class-reference-text-cite-class-citation-journal-schmidhuber-jurgen-2015-a-rel-nofollow-class-external-text-href-http-www-scholarpedia-org-article-deep-learning-deep-learning-a-i-scholarpedia-i-b-10-b-11-1527-54-a-href-wiki-citeseerx-title-citeseerx-citeseerx-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-citeseerx-ist-psu-edu-viewdoc-summary-doi-10-1-1-76-1541-10-1-1-76-1541-a-span-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1162-2fneco-2006-18-7-1527-10-1162-neco-2006-18-7-1527-a-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-16764513-16764513-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-scholarpedia-rft-atitle-deep-learning-rft-volume-10-rft-issue-11-rft-pages-1527-54-rft-date-2015-rft-id-2f-2fciteseerx-ist-psu-edu-2fviewdoc-2fsummary-3fdoi-3d10-1-1-76-1541-rft-id-info-3apmid-2f16764513-rft-id-info-3adoi-2f10-1162-2fneco-2006-18-7-1527-rft-aulast-schmidhuber-rft-aufirst-j-c3-bcrgen-rft-id-http-3a-2f-2fwww-scholarpedia-org-2farticle-2fdeep-learning-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-22-span-class-mw-cite-backlink-b-a-href-cite-ref-22-a-b-span-span-class-reference-text-cite-class-citation-journal-homma-toshiteru-les-atlas-robert-marks-ii-1988-a-rel-nofollow-class-external-text-href-http-papers-nips-cc-paper-20-an-artificial-neural-network-for-spatio-temporal-bipolar-patterns-application-to-phoneme-classification-pdf-an-artificial-neural-network-for-spatio-temporal-bipolar-patters-application-to-phoneme-classification-a-span-class-cs1-format-pdf-span-i-advances-in-neural-information-processing-systems-i-b-1-b-31-40-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-advances-in-neural-information-processing-systems-rft-atitle-an-artificial-neural-network-for-spatio-temporal-bipolar-patters-3a-application-to-phoneme-classification-rft-volume-1-rft-pages-31-40-rft-date-1988-rft-aulast-homma-rft-aufirst-toshiteru-rft-au-les-atlas-rft-au-robert-marks-ii-rft-id-http-3a-2f-2fpapers-nips-cc-2fpaper-2f20-an-artificial-neural-network-for-spatio-temporal-bipolar-patterns-application-to-phoneme-classification-pdf-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-waibel1987-23-span-class-mw-cite-backlink-a-href-cite-ref-waibel1987-23-0-sup-i-b-a-b-i-sup-a-a-href-cite-ref-waibel1987-23-1-sup-i-b-b-b-i-sup-a-span-span-class-reference-text-cite-class-citation-conference-waibel-alex-december-1987-i-phoneme-recognition-using-time-delay-neural-networks-i-meeting-of-the-institute-of-electrical-information-and-communication-engineers-ieice-tokyo-japan-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-conference-rft-btitle-phoneme-recognition-using-time-delay-neural-networks-rft-place-tokyo-2c-japan-rft-date-1987-12-rft-aulast-waibel-rft-aufirst-alex-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-speechsignal-24-span-class-mw-cite-backlink-a-href-cite-ref-speechsignal-24-0-sup-i-b-a-b-i-sup-a-a-href-cite-ref-speechsignal-24-1-sup-i-b-b-b-i-sup-a-span-span-class-reference-text-a-href-wiki-alex-waibel-title-alex-waibel-alexander-waibel-a-et-al-i-phoneme-recognition-using-time-delay-neural-networks-i-ieee-transactions-on-acoustics-speech-and-signal-processing-volume-37-no-3-pp-328-339-march-1989-span-li-li-id-cite-note-25-span-class-mw-cite-backlink-b-a-href-cite-ref-25-a-b-span-span-class-reference-text-cite-class-citation-encyclopaedia-lecun-yann-bengio-yoshua-1995-convolutional-networks-for-images-speech-and-time-series-in-arbib-michael-a-ed-i-the-handbook-of-brain-theory-and-neural-networks-i-second-ed-the-mit-press-pp-276-278-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-bookitem-rft-atitle-convolutional-networks-for-images-2c-speech-2c-and-time-series-rft-btitle-the-handbook-of-brain-theory-and-neural-networks-rft-pages-276-278-rft-edition-second-rft-pub-the-mit-press-rft-date-1995-rft-aulast-lecun-rft-aufirst-yann-rft-au-bengio-2c-yoshua-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-video-quality-26-span-class-mw-cite-backlink-a-href-cite-ref-video-quality-26-0-sup-i-b-a-b-i-sup-a-a-href-cite-ref-video-quality-26-1-sup-i-b-b-b-i-sup-a-span-span-class-reference-text-cite-class-citation-journal-le-callet-patrick-christian-viard-gaudin-dominique-barba-2006-a-rel-nofollow-class-external-text-href-http-hal-univ-nantes-fr-docs-00-28-74-26-pdf-a-convolutional-neural-network-approach-for-objective-video-quality-assessment-completefinal-manuscript-pdf-a-convolutional-neural-network-approach-for-objective-video-quality-assessment-a-span-class-cs1-format-pdf-span-i-ieee-transactions-on-neural-networks-i-b-17-b-5-1316-1327-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1109-2ftnn-2006-879766-10-1109-tnn-2006-879766-a-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-17001990-17001990-a-span-class-reference-accessdate-retrieved-span-class-nowrap-17-november-span-2013-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-ieee-transactions-on-neural-networks-rft-atitle-a-convolutional-neural-network-approach-for-objective-video-quality-assessment-rft-volume-17-rft-issue-5-rft-pages-1316-1327-rft-date-2006-rft-id-info-3adoi-2f10-1109-2ftnn-2006-879766-rft-id-info-3apmid-2f17001990-rft-aulast-le-callet-rft-aufirst-patrick-rft-au-christian-viard-gaudin-rft-au-dominique-barba-rft-id-http-3a-2f-2fhal-univ-nantes-fr-2fdocs-2f00-2f28-2f74-2f26-2fpdf-2fa-convolutional-neural-network-approach-for-objective-video-quality-assessment-completefinal-manuscript-pdf-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-ko2017-27-span-class-mw-cite-backlink-b-a-href-cite-ref-ko2017-27-0-a-b-span-span-class-reference-text-cite-class-citation-conference-ko-tom-peddinti-vijayaditya-povey-daniel-seltzer-michael-l-khudanpur-sanjeev-march-2018-i-a-study-on-data-augmentation-of-reverberant-speech-for-robust-speech-recognition-i-the-42nd-ieee-international-conference-on-acoustics-speech-and-signal-processing-icassp-2017-new-orleans-la-usa-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-conference-rft-btitle-a-study-on-data-augmentation-of-reverberant-speech-for-robust-speech-recognition-rft-place-new-orleans-2c-la-2c-usa-rft-date-2018-03-rft-aulast-ko-rft-aufirst-tom-rft-au-peddinti-2c-vijayaditya-rft-au-povey-2c-daniel-rft-au-seltzer-2c-michael-l-rft-au-khudanpur-2c-sanjeev-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-28-span-class-mw-cite-backlink-b-a-href-cite-ref-28-a-b-span-span-class-reference-text-denker-j-s-gardner-w-r-graf-h-p-henderson-d-howard-r-e-hubbard-w-jackel-l-d-baird-h-s-and-guyon-1989-a-rel-nofollow-class-external-text-href-http-citeseerx-ist-psu-edu-viewdoc-download-doi-10-1-1-852-5499-rep-rep1-type-pdf-neural-network-recognizer-for-hand-written-zip-code-digits-a-at-t-bell-laboratories-span-li-li-id-cite-note-2-29-span-class-mw-cite-backlink-a-href-cite-ref-2-29-0-sup-i-b-a-b-i-sup-a-a-href-cite-ref-2-29-1-sup-i-b-b-b-i-sup-a-span-span-class-reference-text-y-lecun-b-boser-j-s-denker-d-henderson-r-e-howard-w-hubbard-l-d-jackel-a-rel-nofollow-class-external-text-href-http-yann-lecun-com-exdb-publis-pdf-lecun-89e-pdf-backpropagation-applied-to-handwritten-zip-code-recognition-a-at-t-bell-laboratories-span-li-li-id-cite-note-lecun98-30-span-class-mw-cite-backlink-b-a-href-cite-ref-lecun98-30-0-a-b-span-span-class-reference-text-cite-class-citation-journal-lecun-yann-leon-bottou-yoshua-bengio-patrick-haffner-1998-a-rel-nofollow-class-external-text-href-http-yann-lecun-com-exdb-publis-pdf-lecun-01a-pdf-gradient-based-learning-applied-to-document-recognition-a-span-class-cs1-format-pdf-span-i-proceedings-of-the-ieee-i-b-86-b-11-2278-2324-a-href-wiki-citeseerx-title-citeseerx-citeseerx-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-citeseerx-ist-psu-edu-viewdoc-summary-doi-10-1-1-32-9552-10-1-1-32-9552-a-span-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1109-2f5-726791-10-1109-5-726791-a-span-class-reference-accessdate-retrieved-span-class-nowrap-october-7-span-2016-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-proceedings-of-the-ieee-rft-atitle-gradient-based-learning-applied-to-document-recognition-rft-volume-86-rft-issue-11-rft-pages-2278-2324-rft-date-1998-rft-id-2f-2fciteseerx-ist-psu-edu-2fviewdoc-2fsummary-3fdoi-3d10-1-1-32-9552-rft-id-info-3adoi-2f10-1109-2f5-726791-rft-aulast-lecun-rft-aufirst-yann-rft-au-l-c3-a9on-bottou-rft-au-yoshua-bengio-rft-au-patrick-haffner-rft-id-http-3a-2f-2fyann-lecun-com-2fexdb-2fpublis-2fpdf-2flecun-01a-pdf-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-31-span-class-mw-cite-backlink-b-a-href-cite-ref-31-a-b-span-span-class-reference-text-cite-class-citation-journal-zhang-wei-1991-a-rel-nofollow-class-external-text-href-https-drive-google-com-file-d-0b65v6wo67tk5dkjtcemtu2c5znc-view-usp-sharing-error-back-propagation-with-minimum-entropy-weights-a-technique-for-better-generalization-of-2-d-shift-invariant-nns-a-i-proceedings-of-the-international-joint-conference-on-neural-networks-i-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-proceedings-of-the-international-joint-conference-on-neural-networks-rft-atitle-error-back-propagation-with-minimum-entropy-weights-3a-a-technique-for-better-generalization-of-2-d-shift-invariant-nns-rft-date-1991-rft-aulast-zhang-rft-aufirst-wei-rft-id-https-3a-2f-2fdrive-google-com-2ffile-2fd-2f0b65v6wo67tk5dkjtcemtu2c5znc-2fview-3fusp-3dsharing-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-32-span-class-mw-cite-backlink-b-a-href-cite-ref-32-a-b-span-span-class-reference-text-cite-class-citation-journal-zhang-wei-1991-a-rel-nofollow-class-external-text-href-https-drive-google-com-file-d-0b65v6wo67tk5cm5dtlngd0npumm-view-usp-sharing-image-processing-of-human-corneal-endothelium-based-on-a-learning-network-a-i-applied-optics-i-b-30-b-29-4211-7-a-href-wiki-bibcode-title-bibcode-bibcode-a-a-rel-nofollow-class-external-text-href-http-adsabs-harvard-edu-abs-1991apopt-30-4211z-1991apopt-30-4211z-a-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1364-2fao-30-004211-10-1364-ao-30-004211-a-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-20706526-20706526-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-applied-optics-rft-atitle-image-processing-of-human-corneal-endothelium-based-on-a-learning-network-rft-volume-30-rft-issue-29-rft-pages-4211-7-rft-date-1991-rft-id-info-3apmid-2f20706526-rft-id-info-3adoi-2f10-1364-2fao-30-004211-rft-id-info-3abibcode-2f1991apopt-30-4211z-rft-aulast-zhang-rft-aufirst-wei-rft-id-https-3a-2f-2fdrive-google-com-2ffile-2fd-2f0b65v6wo67tk5cm5dtlngd0npumm-2fview-3fusp-3dsharing-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-33-span-class-mw-cite-backlink-b-a-href-cite-ref-33-a-b-span-span-class-reference-text-cite-class-citation-journal-zhang-wei-1994-a-rel-nofollow-class-external-text-href-https-drive-google-com-file-d-0b65v6wo67tk5ml9qew5nq3povtq-view-usp-sharing-computerized-detection-of-clustered-microcalcifications-in-digital-mammograms-using-a-shift-invariant-artificial-neural-network-a-i-medical-physics-i-b-21-b-4-517-24-a-href-wiki-bibcode-title-bibcode-bibcode-a-a-rel-nofollow-class-external-text-href-http-adsabs-harvard-edu-abs-1994medph-21-517z-1994medph-21-517z-a-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1118-2f1-597177-10-1118-1-597177-a-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-8058017-8058017-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-medical-physics-rft-atitle-computerized-detection-of-clustered-microcalcifications-in-digital-mammograms-using-a-shift-invariant-artificial-neural-network-rft-volume-21-rft-issue-4-rft-pages-517-24-rft-date-1994-rft-id-info-3apmid-2f8058017-rft-id-info-3adoi-2f10-1118-2f1-597177-rft-id-info-3abibcode-2f1994medph-21-517z-rft-aulast-zhang-rft-aufirst-wei-rft-id-https-3a-2f-2fdrive-google-com-2ffile-2fd-2f0b65v6wo67tk5ml9qew5nq3povtq-2fview-3fusp-3dsharing-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-34-span-class-mw-cite-backlink-b-a-href-cite-ref-34-a-b-span-span-class-reference-text-daniel-graupe-ruey-wen-liu-george-s-moschytz-applications-of-neural-networks-to-medical-signal-processing-in-proc-27th-ieee-decision-and-control-conf-pp-343-347-1988-span-li-li-id-cite-note-35-span-class-mw-cite-backlink-b-a-href-cite-ref-35-a-b-span-span-class-reference-text-daniel-graupe-boris-vern-g-gruener-aaron-field-and-qiu-huang-decomposition-of-surface-emg-signals-into-single-fiber-action-potentials-by-means-of-neural-network-proc-ieee-international-symp-on-circuits-and-systems-pp-1008-1011-1989-span-li-li-id-cite-note-36-span-class-mw-cite-backlink-b-a-href-cite-ref-36-a-b-span-span-class-reference-text-qiu-huang-daniel-graupe-yi-fang-huang-ruey-wen-liu-identification-of-firing-patterns-of-neuronal-signals-in-proc-28th-ieee-decision-and-control-conf-pp-266-271-1989-span-li-li-id-cite-note-37-span-class-mw-cite-backlink-b-a-href-cite-ref-37-a-b-span-span-class-reference-text-cite-class-citation-book-behnke-sven-2003-a-rel-nofollow-class-external-text-href-https-www-ais-uni-bonn-de-books-lncs2766-pdf-i-hierarchical-neural-networks-for-image-interpretation-i-a-span-class-cs1-format-pdf-span-lecture-notes-in-computer-science-b-2766-b-springer-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1007-2fb11963-10-1007-b11963-a-a-href-wiki-international-standard-book-number-title-international-standard-book-number-isbn-a-a-href-wiki-special-booksources-978-3-540-40722-5-title-special-booksources-978-3-540-40722-5-bdi-978-3-540-40722-5-bdi-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-book-rft-btitle-hierarchical-neural-networks-for-image-interpretation-rft-series-lecture-notes-in-computer-science-rft-pub-springer-rft-date-2003-rft-id-info-3adoi-2f10-1007-2fb11963-rft-isbn-978-3-540-40722-5-rft-aulast-behnke-rft-aufirst-sven-rft-id-https-3a-2f-2fwww-ais-uni-bonn-de-2fbooks-2flncs2766-pdf-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-38-span-class-mw-cite-backlink-b-a-href-cite-ref-38-a-b-span-span-class-reference-text-cite-class-citation-journal-oh-ks-jung-k-2004-gpu-implementation-of-neural-networks-i-pattern-recognition-i-b-37-b-6-1311-1314-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1016-2fj-patcog-2004-01-013-10-1016-j-patcog-2004-01-013-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-pattern-recognition-rft-atitle-gpu-implementation-of-neural-networks-rft-volume-37-rft-issue-6-rft-pages-1311-1314-rft-date-2004-rft-id-info-3adoi-2f10-1016-2fj-patcog-2004-01-013-rft-aulast-oh-rft-aufirst-ks-rft-au-jung-2c-k-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-39-span-class-mw-cite-backlink-b-a-href-cite-ref-39-a-b-span-span-class-reference-text-cite-class-citation-book-dave-steinkraus-patrice-simard-ian-buck-2005-a-rel-nofollow-class-external-text-href-http-www-computer-org-csdl-proceedings-icdar-2005-2420-00-24201115-abs-html-using-gpus-for-machine-learning-algorithms-a-i-12th-international-conference-on-document-analysis-and-recognition-icdar-2005-i-pp-1115-1119-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-bookitem-rft-atitle-using-gpus-for-machine-learning-algorithms-rft-btitle-12th-international-conference-on-document-analysis-and-recognition-28icdar-2005-29-rft-pages-1115-1119-rft-date-2005-rft-au-dave-steinkraus-rft-au-patrice-simard-rft-au-ian-buck-rft-id-http-3a-2f-2fwww-computer-org-2fcsdl-2fproceedings-2ficdar-2f2005-2f2420-2f00-2f24201115-abs-html-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-40-span-class-mw-cite-backlink-b-a-href-cite-ref-40-a-b-span-span-class-reference-text-cite-class-citation-book-kumar-chellapilla-sid-puri-patrice-simard-2006-a-rel-nofollow-class-external-text-href-https-hal-inria-fr-inria-00112631-document-high-performance-convolutional-neural-networks-for-document-processing-a-in-lorette-guy-ed-i-tenth-international-workshop-on-frontiers-in-handwriting-recognition-i-suvisoft-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-bookitem-rft-atitle-high-performance-convolutional-neural-networks-for-document-processing-rft-btitle-tenth-international-workshop-on-frontiers-in-handwriting-recognition-rft-pub-suvisoft-rft-date-2006-rft-au-kumar-chellapilla-rft-au-sid-puri-rft-au-patrice-simard-rft-id-https-3a-2f-2fhal-inria-fr-2finria-00112631-2fdocument-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-41-span-class-mw-cite-backlink-b-a-href-cite-ref-41-a-b-span-span-class-reference-text-cite-class-citation-journal-hinton-ge-osindero-s-teh-yw-jul-2006-a-fast-learning-algorithm-for-deep-belief-nets-i-neural-computation-i-b-18-b-7-1527-54-a-href-wiki-citeseerx-title-citeseerx-citeseerx-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-citeseerx-ist-psu-edu-viewdoc-summary-doi-10-1-1-76-1541-10-1-1-76-1541-a-span-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1162-2fneco-2006-18-7-1527-10-1162-neco-2006-18-7-1527-a-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-16764513-16764513-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-neural-computation-rft-atitle-a-fast-learning-algorithm-for-deep-belief-nets-rft-volume-18-rft-issue-7-rft-pages-1527-54-rft-date-2006-07-rft-id-2f-2fciteseerx-ist-psu-edu-2fviewdoc-2fsummary-3fdoi-3d10-1-1-76-1541-rft-id-info-3apmid-2f16764513-rft-id-info-3adoi-2f10-1162-2fneco-2006-18-7-1527-rft-aulast-hinton-rft-aufirst-ge-rft-au-osindero-2c-s-rft-au-teh-2c-yw-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-42-span-class-mw-cite-backlink-b-a-href-cite-ref-42-a-b-span-span-class-reference-text-cite-class-citation-journal-bengio-yoshua-lamblin-pascal-popovici-dan-larochelle-hugo-2007-greedy-layer-wise-training-of-deep-networks-i-advances-in-neural-information-processing-systems-i-153-160-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-advances-in-neural-information-processing-systems-rft-atitle-greedy-layer-wise-training-of-deep-networks-rft-pages-153-160-rft-date-2007-rft-aulast-bengio-rft-aufirst-yoshua-rft-au-lamblin-2c-pascal-rft-au-popovici-2c-dan-rft-au-larochelle-2c-hugo-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-43-span-class-mw-cite-backlink-b-a-href-cite-ref-43-a-b-span-span-class-reference-text-cite-class-citation-journal-ranzato-marcaurelio-poultney-christopher-chopra-sumit-lecun-yann-2007-a-rel-nofollow-class-external-text-href-http-yann-lecun-com-exdb-publis-pdf-ranzato-06-pdf-efficient-learning-of-sparse-representations-with-an-energy-based-model-a-span-class-cs1-format-pdf-span-i-advances-in-neural-information-processing-systems-i-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-advances-in-neural-information-processing-systems-rft-atitle-efficient-learning-of-sparse-representations-with-an-energy-based-model-rft-date-2007-rft-aulast-ranzato-rft-aufirst-marcaurelio-rft-au-poultney-2c-christopher-rft-au-chopra-2c-sumit-rft-au-lecun-2c-yann-rft-id-http-3a-2f-2fyann-lecun-com-2fexdb-2fpublis-2fpdf-2franzato-06-pdf-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-44-span-class-mw-cite-backlink-b-a-href-cite-ref-44-a-b-span-span-class-reference-text-cite-class-citation-journal-raina-r-madhavan-a-ng-andrew-2009-large-scale-deep-unsupervised-learning-using-graphics-processors-i-icml-i-873-880-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-icml-rft-atitle-large-scale-deep-unsupervised-learning-using-graphics-processors-rft-pages-873-880-rft-date-2009-rft-aulast-raina-rft-aufirst-r-rft-au-madhavan-2c-a-rft-au-ng-2c-andrew-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-45-span-class-mw-cite-backlink-b-a-href-cite-ref-45-a-b-span-span-class-reference-text-cite-class-citation-journal-ciresan-dan-meier-ueli-gambardella-luca-schmidhuber-jurgen-2010-deep-big-simple-neural-nets-for-handwritten-digit-recognition-i-neural-computation-i-b-22-b-12-3207-3220-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1003-0358-1003-0358-a-span-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1162-2fneco-a-00052-10-1162-neco-a-00052-a-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-20858131-20858131-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-neural-computation-rft-atitle-deep-big-simple-neural-nets-for-handwritten-digit-recognition-rft-volume-22-rft-issue-12-rft-pages-3207-3220-rft-date-2010-rft-id-info-3aarxiv-2f1003-0358-rft-id-info-3apmid-2f20858131-rft-id-info-3adoi-2f10-1162-2fneco-a-00052-rft-aulast-ciresan-rft-aufirst-dan-rft-au-meier-2c-ueli-rft-au-gambardella-2c-luca-rft-au-schmidhuber-2c-j-c3-bcrgen-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-46-span-class-mw-cite-backlink-b-a-href-cite-ref-46-a-b-span-span-class-reference-text-cite-class-citation-web-a-rel-nofollow-class-external-text-href-http-benchmark-ini-rub-de-section-gtsrb-subsection-results-ijcnn-2011-competition-result-table-a-i-official-ijcnn2011-competition-i-2010-span-class-reference-accessdate-retrieved-span-class-nowrap-2019-01-14-span-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-unknown-rft-jtitle-official-ijcnn2011-competition-rft-atitle-ijcnn-2011-competition-result-table-rft-date-2010-rft-id-http-3a-2f-2fbenchmark-ini-rub-de-2f-3fsection-3dgtsrb-26subsection-3dresults-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-47-span-class-mw-cite-backlink-b-a-href-cite-ref-47-a-b-span-span-class-reference-text-cite-class-citation-web-schmidhuber-jurgen-17-march-2017-a-rel-nofollow-class-external-text-href-http-people-idsia-ch-juergen-computer-vision-contests-won-by-gpu-cnns-html-history-of-computer-vision-contests-won-by-deep-cnns-on-gpu-a-span-class-reference-accessdate-retrieved-span-class-nowrap-14-january-span-2019-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-unknown-rft-btitle-history-of-computer-vision-contests-won-by-deep-cnns-on-gpu-rft-date-2017-03-17-rft-aulast-schmidhuber-rft-aufirst-j-c3-bcrgen-rft-id-http-3a-2f-2fpeople-idsia-ch-2f-juergen-2fcomputer-vision-contests-won-by-gpu-cnns-html-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-02-48-span-class-mw-cite-backlink-a-href-cite-ref-02-48-0-sup-i-b-a-b-i-sup-a-a-href-cite-ref-02-48-1-sup-i-b-b-b-i-sup-a-span-span-class-reference-text-cite-class-citation-journal-krizhevsky-alex-sutskever-ilya-hinton-geoffrey-e-2017-05-24-a-rel-nofollow-class-external-text-href-https-papers-nips-cc-paper-4824-imagenet-classification-with-deep-convolutional-neural-networks-pdf-imagenet-classification-with-deep-convolutional-neural-networks-a-span-class-cs1-format-pdf-span-i-communications-of-the-acm-i-b-60-b-6-84-90-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1145-2f3065386-10-1145-3065386-a-a-href-wiki-international-standard-serial-number-title-international-standard-serial-number-issn-a-a-rel-nofollow-class-external-text-href-www-worldcat-org-issn-0001-0782-0001-0782-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-communications-of-the-acm-rft-atitle-imagenet-classification-with-deep-convolutional-neural-networks-rft-volume-60-rft-issue-6-rft-pages-84-90-rft-date-2017-05-24-rft-id-info-3adoi-2f10-1145-2f3065386-rft-issn-0001-0782-rft-aulast-krizhevsky-rft-aufirst-alex-rft-au-sutskever-2c-ilya-rft-au-hinton-2c-geoffrey-e-rft-id-https-3a-2f-2fpapers-nips-cc-2fpaper-2f4824-imagenet-classification-with-deep-convolutional-neural-networks-pdf-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-49-span-class-mw-cite-backlink-b-a-href-cite-ref-49-a-b-span-span-class-reference-text-cite-class-citation-journal-he-kaiming-zhang-xiangyu-ren-shaoqing-sun-jian-2016-deep-residual-learning-for-image-recognition-i-2016-ieee-conference-on-computer-vision-and-pattern-recognition-cvpr-i-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-2016-ieee-conference-on-computer-vision-and-pattern-recognition-28cvpr-29-rft-atitle-deep-residual-learning-for-image-recognition-rft-date-2016-rft-aulast-he-rft-aufirst-kaiming-rft-au-zhang-2c-xiangyu-rft-au-ren-2c-shaoqing-rft-au-sun-2c-jian-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-50-span-class-mw-cite-backlink-b-a-href-cite-ref-50-a-b-span-span-class-reference-text-cite-class-citation-web-viebke-andre-pllana-sabri-a-rel-nofollow-class-external-text-href-https-ieeexplore-ieee-org-document-7336249-the-potential-of-the-intel-r-xeon-phi-for-supervised-deep-learning-a-i-ieee-xplore-i-ieee-2015-span-class-reference-accessdate-retrieved-span-class-nowrap-19-june-span-2019-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-unknown-rft-jtitle-ieee-xplore-rft-atitle-the-potential-of-the-intel-28r-29-xeon-phi-for-supervised-deep-learning-rft-aulast-viebke-rft-aufirst-andre-rft-au-pllana-2c-sabri-rft-id-https-3a-2f-2fieeexplore-ieee-org-2fdocument-2f7336249-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-51-span-class-mw-cite-backlink-b-a-href-cite-ref-51-a-b-span-span-class-reference-text-cite-class-citation-journal-viebke-andre-memeti-suejb-pllana-sabri-abraham-ajith-2019-a-rel-nofollow-class-external-text-href-https-link-springer-com-article-10-1007-s11227-017-1994-x-chaos-a-parallelization-scheme-for-training-convolutional-neural-networks-on-intel-xeon-phi-a-i-the-journal-of-supercomputing-i-b-75-b-1-197-227-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1007-2fs11227-017-1994-x-10-1007-s11227-017-1994-x-a-span-class-reference-accessdate-retrieved-span-class-nowrap-19-june-span-2019-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-the-journal-of-supercomputing-rft-atitle-chaos-3a-a-parallelization-scheme-for-training-convolutional-neural-networks-on-intel-xeon-phi-rft-volume-75-rft-issue-1-rft-pages-197-227-rft-date-2019-rft-id-info-3adoi-2f10-1007-2fs11227-017-1994-x-rft-aulast-viebke-rft-aufirst-andre-rft-au-memeti-2c-suejb-rft-au-pllana-2c-sabri-rft-au-abraham-2c-ajith-rft-id-https-3a-2f-2flink-springer-com-2farticle-2f10-1007-2fs11227-017-1994-x-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-53-span-class-mw-cite-backlink-b-a-href-cite-ref-53-a-b-span-span-class-reference-text-cite-class-citation-web-a-rel-nofollow-class-external-text-href-https-cs231n-github-io-convolutional-networks-cs231n-convolutional-neural-networks-for-visual-recognition-a-i-cs231n-github-io-i-span-class-reference-accessdate-retrieved-span-class-nowrap-2017-04-25-span-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-unknown-rft-jtitle-cs231n-github-io-rft-atitle-cs231n-convolutional-neural-networks-for-visual-recognition-rft-id-https-3a-2f-2fcs231n-github-io-2fconvolutional-networks-2f-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-scherer-icann-2010-55-span-class-mw-cite-backlink-a-href-cite-ref-scherer-icann-2010-55-0-sup-i-b-a-b-i-sup-a-a-href-cite-ref-scherer-icann-2010-55-1-sup-i-b-b-b-i-sup-a-span-span-class-reference-text-cite-class-citation-conference-scherer-dominik-muller-andreas-c-behnke-sven-2010-a-rel-nofollow-class-external-text-href-http-ais-uni-bonn-de-papers-icann2010-maxpool-pdf-evaluation-of-pooling-operations-in-convolutional-architectures-for-object-recognition-a-span-class-cs1-format-pdf-span-i-artificial-neural-networks-icann-20th-international-conference-on-i-thessaloniki-greece-springer-pp-92-101-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-conference-rft-atitle-evaluation-of-pooling-operations-in-convolutional-architectures-for-object-recognition-rft-btitle-artificial-neural-networks-28icann-29-2c-20th-international-conference-on-rft-place-thessaloniki-2c-greece-rft-pages-92-101-rft-pub-springer-rft-date-2010-rft-aulast-scherer-rft-aufirst-dominik-rft-au-m-c3-bcller-2c-andreas-c-rft-au-behnke-2c-sven-rft-id-http-3a-2f-2fais-uni-bonn-de-2fpapers-2ficann2010-maxpool-pdf-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-56-span-class-mw-cite-backlink-b-a-href-cite-ref-56-a-b-span-span-class-reference-text-cite-class-citation-arxiv-graham-benjamin-2014-12-18-fractional-max-pooling-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1412-6071-1412-6071-a-span-a-rel-nofollow-class-external-text-href-arxiv-org-archive-cs-cv-cs-cv-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-preprint-rft-jtitle-arxiv-rft-atitle-fractional-max-pooling-rft-date-2014-12-18-rft-id-info-3aarxiv-2f1412-6071-rft-aulast-graham-rft-aufirst-benjamin-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-57-span-class-mw-cite-backlink-b-a-href-cite-ref-57-a-b-span-span-class-reference-text-cite-class-citation-arxiv-springenberg-jost-tobias-dosovitskiy-alexey-brox-thomas-riedmiller-martin-2014-12-21-striving-for-simplicity-the-all-convolutional-net-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1412-6806-1412-6806-a-span-a-rel-nofollow-class-external-text-href-arxiv-org-archive-cs-lg-cs-lg-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-preprint-rft-jtitle-arxiv-rft-atitle-striving-for-simplicity-3a-the-all-convolutional-net-rft-date-2014-12-21-rft-id-info-3aarxiv-2f1412-6806-rft-aulast-springenberg-rft-aufirst-jost-tobias-rft-au-dosovitskiy-2c-alexey-rft-au-brox-2c-thomas-rft-au-riedmiller-2c-martin-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-58-span-class-mw-cite-backlink-b-a-href-cite-ref-58-a-b-span-span-class-reference-text-cite-class-citation-web-grel-tomasz-2017-02-28-a-rel-nofollow-class-external-text-href-https-deepsense-io-region-of-interest-pooling-explained-region-of-interest-pooling-explained-a-i-deepsense-io-i-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-unknown-rft-jtitle-deepsense-io-rft-atitle-region-of-interest-pooling-explained-rft-date-2017-02-28-rft-aulast-grel-rft-aufirst-tomasz-rft-id-https-3a-2f-2fdeepsense-io-2fregion-of-interest-pooling-explained-2f-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-rcnn-59-span-class-mw-cite-backlink-b-a-href-cite-ref-rcnn-59-0-a-b-span-span-class-reference-text-cite-class-citation-arxiv-girshick-ross-2015-09-27-fast-r-cnn-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1504-08083-1504-08083-a-span-a-rel-nofollow-class-external-text-href-arxiv-org-archive-cs-cv-cs-cv-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-preprint-rft-jtitle-arxiv-rft-atitle-fast-r-cnn-rft-date-2015-09-27-rft-id-info-3aarxiv-2f1504-08083-rft-aulast-girshick-rft-aufirst-ross-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-romanuke4-60-span-class-mw-cite-backlink-b-a-href-cite-ref-romanuke4-60-0-a-b-span-span-class-reference-text-cite-class-citation-journal-romanuke-vadim-2017-a-rel-nofollow-class-external-text-href-http-bulletin-kpi-ua-article-view-88156-pdf-186-88156-200304-1-pb-pdf-appropriate-number-and-allocation-of-relus-in-convolutional-neural-networks-a-span-class-cs1-format-pdf-span-i-research-bulletin-of-ntuu-kyiv-polytechnic-institute-i-b-1-b-69-78-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-20535-2f1810-0546-2017-1-88156-10-20535-1810-0546-2017-1-88156-a-span-class-reference-accessdate-retrieved-span-class-nowrap-17-february-span-2019-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-research-bulletin-of-ntuu-22kyiv-polytechnic-institute-22-rft-atitle-appropriate-number-and-allocation-of-relus-in-convolutional-neural-networks-rft-volume-1-rft-pages-69-78-rft-date-2017-rft-id-info-3adoi-2f10-20535-2f1810-0546-2017-1-88156-rft-aulast-romanuke-rft-aufirst-vadim-rft-id-http-3a-2f-2fbulletin-kpi-ua-2farticle-2fview-2f88156-2fpdf-186-2f88156-200304-1-pb-pdf-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-61-span-class-mw-cite-backlink-b-a-href-cite-ref-61-a-b-span-span-class-reference-text-cite-class-citation-journal-krizhevsky-a-sutskever-i-hinton-g-e-2012-a-rel-nofollow-class-external-text-href-http-papers-nips-cc-paper-4824-imagenet-classification-with-deep-convolutional-neural-networks-pdf-imagenet-classification-with-deep-convolutional-neural-networks-a-span-class-cs1-format-pdf-span-i-advances-in-neural-information-processing-systems-i-b-1-b-1097-1105-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-advances-in-neural-information-processing-systems-rft-atitle-imagenet-classification-with-deep-convolutional-neural-networks-rft-volume-1-rft-pages-1097-1105-rft-date-2012-rft-aulast-krizhevsky-rft-aufirst-a-rft-au-sutskever-2c-i-rft-au-hinton-2c-g-e-rft-id-http-3a-2f-2fpapers-nips-cc-2fpaper-2f4824-imagenet-classification-with-deep-convolutional-neural-networks-pdf-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-63-span-class-mw-cite-backlink-b-a-href-cite-ref-63-a-b-span-span-class-reference-text-cite-class-citation-web-deshpande-adit-a-rel-nofollow-class-external-text-href-https-adeshpande3-github-io-adeshpande3-github-io-the-9-deep-learning-papers-you-need-to-know-about-html-the-9-deep-learning-papers-you-need-to-know-about-understanding-cnns-part-3-a-i-adeshpande3-github-io-i-span-class-reference-accessdate-retrieved-span-class-nowrap-2018-12-04-span-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-unknown-rft-jtitle-adeshpande3-github-io-rft-atitle-the-9-deep-learning-papers-you-need-to-know-about-28understanding-cnns-part-3-29-rft-aulast-deshpande-rft-aufirst-adit-rft-id-https-3a-2f-2fadeshpande3-github-io-2fadeshpande3-github-io-2fthe-9-deep-learning-papers-you-need-to-know-about-html-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-64-span-class-mw-cite-backlink-b-a-href-cite-ref-64-a-b-span-span-class-reference-text-cite-class-citation-journal-srivastava-nitish-c-geoffrey-hinton-alex-krizhevsky-ilya-sutskever-ruslan-salakhutdinov-2014-a-rel-nofollow-class-external-text-href-http-www-cs-toronto-edu-rsalakhu-papers-srivastava14a-pdf-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting-a-span-class-cs1-format-pdf-span-i-journal-of-machine-learning-research-i-b-15-b-1-1929-1958-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-journal-of-machine-learning-research-rft-atitle-dropout-3a-a-simple-way-to-prevent-neural-networks-from-overfitting-rft-volume-15-rft-issue-1-rft-pages-1929-1958-rft-date-2014-rft-aulast-srivastava-rft-aufirst-nitish-rft-au-c-geoffrey-hinton-rft-au-alex-krizhevsky-rft-au-ilya-sutskever-rft-au-ruslan-salakhutdinov-rft-id-http-3a-2f-2fwww-cs-toronto-edu-2f-rsalakhu-2fpapers-2fsrivastava14a-pdf-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-dlpatterns-65-span-class-mw-cite-backlink-b-a-href-cite-ref-dlpatterns-65-0-a-b-span-span-class-reference-text-cite-class-citation-web-carlos-e-perez-a-rel-nofollow-class-external-text-href-http-www-deeplearningpatterns-com-a-pattern-language-for-deep-learning-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-unknown-rft-btitle-a-pattern-language-for-deep-learning-rft-au-carlos-e-perez-rft-id-http-3a-2f-2fwww-deeplearningpatterns-com-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-66-span-class-mw-cite-backlink-b-a-href-cite-ref-66-a-b-span-span-class-reference-text-cite-class-citation-web-a-rel-nofollow-class-external-text-href-http-jmlr-org-proceedings-papers-v28-wan13-html-regularization-of-neural-networks-using-dropconnect-icml-2013-jmlr-w-cp-a-i-jmlr-org-i-2013-02-13-pp-1058-1066-span-class-reference-accessdate-retrieved-span-class-nowrap-2015-12-17-span-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-unknown-rft-jtitle-jmlr-org-rft-atitle-regularization-of-neural-networks-using-dropconnect-7c-icml-2013-7c-jmlr-w-26cp-rft-pages-1058-1066-rft-date-2013-02-13-rft-id-http-3a-2f-2fjmlr-org-2fproceedings-2fpapers-2fv28-2fwan13-html-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-67-span-class-mw-cite-backlink-b-a-href-cite-ref-67-a-b-span-span-class-reference-text-cite-class-citation-arxiv-zeiler-matthew-d-fergus-rob-2013-01-15-stochastic-pooling-for-regularization-of-deep-convolutional-neural-networks-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1301-3557-1301-3557-a-span-a-rel-nofollow-class-external-text-href-arxiv-org-archive-cs-lg-cs-lg-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-preprint-rft-jtitle-arxiv-rft-atitle-stochastic-pooling-for-regularization-of-deep-convolutional-neural-networks-rft-date-2013-01-15-rft-id-info-3aarxiv-2f1301-3557-rft-aulast-zeiler-rft-aufirst-matthew-d-rft-au-fergus-2c-rob-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-3-68-span-class-mw-cite-backlink-a-href-cite-ref-3-68-0-sup-i-b-a-b-i-sup-a-a-href-cite-ref-3-68-1-sup-i-b-b-b-i-sup-a-span-span-class-reference-text-cite-class-citation-journal-platt-john-steinkraus-dave-simard-patrice-y-august-2003-a-rel-nofollow-class-external-text-href-http-research-microsoft-com-apps-pubs-id-68920-best-practices-for-convolutional-neural-networks-applied-to-visual-document-analysis-microsoft-research-a-i-microsoft-research-i-span-class-reference-accessdate-retrieved-span-class-nowrap-2015-12-17-span-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-microsoft-research-rft-atitle-best-practices-for-convolutional-neural-networks-applied-to-visual-document-analysis-e2-80-93-microsoft-research-rft-date-2003-08-rft-aulast-platt-rft-aufirst-john-rft-au-steinkraus-2c-dave-rft-au-simard-2c-patrice-y-rft-id-http-3a-2f-2fresearch-microsoft-com-2fapps-2fpubs-2f-3fid-3d68920-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-69-span-class-mw-cite-backlink-b-a-href-cite-ref-69-a-b-span-span-class-reference-text-cite-class-citation-arxiv-hinton-geoffrey-e-srivastava-nitish-krizhevsky-alex-sutskever-ilya-salakhutdinov-ruslan-r-2012-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1207-0580-1207-0580-a-span-a-rel-nofollow-class-external-text-href-arxiv-org-archive-cs-ne-cs-ne-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-preprint-rft-jtitle-arxiv-rft-atitle-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors-rft-date-2012-rft-id-info-3aarxiv-2f1207-0580-rft-aulast-hinton-rft-aufirst-geoffrey-e-rft-au-srivastava-2c-nitish-rft-au-krizhevsky-2c-alex-rft-au-sutskever-2c-ilya-rft-au-salakhutdinov-2c-ruslan-r-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-70-span-class-mw-cite-backlink-b-a-href-cite-ref-70-a-b-span-span-class-reference-text-cite-class-citation-web-a-rel-nofollow-class-external-text-href-http-jmlr-org-papers-v15-srivastava14a-html-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting-a-i-jmlr-org-i-span-class-reference-accessdate-retrieved-span-class-nowrap-2015-12-17-span-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-unknown-rft-jtitle-jmlr-org-rft-atitle-dropout-3a-a-simple-way-to-prevent-neural-networks-from-overfitting-rft-id-http-3a-2f-2fjmlr-org-2fpapers-2fv15-2fsrivastava14a-html-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-71-span-class-mw-cite-backlink-b-a-href-cite-ref-71-a-b-span-span-class-reference-text-cite-class-citation-journal-hinton-geoffrey-1979-some-demonstrations-of-the-effects-of-structural-descriptions-in-mental-imagery-i-cognitive-science-i-b-3-b-3-231-250-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1016-2fs0364-0213-2879-2980008-7-10-1016-s0364-0213-79-80008-7-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-cognitive-science-rft-atitle-some-demonstrations-of-the-effects-of-structural-descriptions-in-mental-imagery-rft-volume-3-rft-issue-3-rft-pages-231-250-rft-date-1979-rft-id-info-3adoi-2f10-1016-2fs0364-0213-2879-2980008-7-rft-aulast-hinton-rft-aufirst-geoffrey-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-72-span-class-mw-cite-backlink-b-a-href-cite-ref-72-a-b-span-span-class-reference-text-rock-irvin-the-frame-of-reference-the-legacy-of-solomon-asch-essays-in-cognition-and-social-psychology-1990-243-268-span-li-li-id-cite-note-73-span-class-mw-cite-backlink-b-a-href-cite-ref-73-a-b-span-span-class-reference-text-j-hinton-coursera-lectures-on-neural-networks-2012-url-a-rel-nofollow-class-external-free-href-https-www-coursera-org-learn-neural-networks-https-www-coursera-org-learn-neural-networks-a-span-li-li-id-cite-note-quartz-74-span-class-mw-cite-backlink-b-a-href-cite-ref-quartz-74-0-a-b-span-span-class-reference-text-cite-class-citation-web-dave-gershgorn-18-june-2018-a-rel-nofollow-class-external-text-href-https-qz-com-1307091-the-inside-story-of-how-ai-got-good-enough-to-dominate-silicon-valley-the-inside-story-of-how-ai-got-good-enough-to-dominate-silicon-valley-a-i-a-href-wiki-quartz-website-class-mw-redirect-title-quartz-website-quartz-a-i-span-class-reference-accessdate-retrieved-span-class-nowrap-5-october-span-2018-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-unknown-rft-jtitle-quartz-rft-atitle-the-inside-story-of-how-ai-got-good-enough-to-dominate-silicon-valley-rft-date-2018-06-18-rft-au-dave-gershgorn-rft-id-https-3a-2f-2fqz-com-2f1307091-2fthe-inside-story-of-how-ai-got-good-enough-to-dominate-silicon-valley-2f-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-75-span-class-mw-cite-backlink-b-a-href-cite-ref-75-a-b-span-span-class-reference-text-cite-class-citation-journal-lawrence-steve-c-lee-giles-ah-chung-tsoi-andrew-d-back-1997-face-recognition-a-convolutional-neural-network-approach-i-ieee-transactions-on-neural-networks-i-b-8-b-1-98-113-a-href-wiki-citeseerx-title-citeseerx-citeseerx-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-citeseerx-ist-psu-edu-viewdoc-summary-doi-10-1-1-92-5813-10-1-1-92-5813-a-span-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1109-2f72-554195-10-1109-72-554195-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-ieee-transactions-on-neural-networks-rft-atitle-face-recognition-3a-a-convolutional-neural-network-approach-rft-volume-8-rft-issue-1-rft-pages-98-113-rft-date-1997-rft-id-2f-2fciteseerx-ist-psu-edu-2fviewdoc-2fsummary-3fdoi-3d10-1-1-92-5813-rft-id-info-3adoi-2f10-1109-2f72-554195-rft-aulast-lawrence-rft-aufirst-steve-rft-au-c-lee-giles-rft-au-ah-chung-tsoi-rft-au-andrew-d-back-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-ilsvrc2014-76-span-class-mw-cite-backlink-b-a-href-cite-ref-ilsvrc2014-76-0-a-b-span-span-class-reference-text-cite-class-citation-web-a-rel-nofollow-class-external-text-href-http-www-image-net-org-challenges-lsvrc-2014-results-imagenet-large-scale-visual-recognition-competition-2014-ilsvrc2014-a-span-class-reference-accessdate-retrieved-span-class-nowrap-30-january-span-2016-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-unknown-rft-btitle-imagenet-large-scale-visual-recognition-competition-2014-28ilsvrc2014-29-rft-id-http-3a-2f-2fwww-image-net-org-2fchallenges-2flsvrc-2f2014-2fresults-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-googlenet-77-span-class-mw-cite-backlink-b-a-href-cite-ref-googlenet-77-0-a-b-span-span-class-reference-text-cite-class-citation-journal-szegedy-christian-liu-wei-jia-yangqing-sermanet-pierre-reed-scott-anguelov-dragomir-erhan-dumitru-vanhoucke-vincent-rabinovich-andrew-2014-going-deeper-with-convolutions-i-computing-research-repository-i-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1409-4842-1409-4842-a-span-a-href-wiki-bibcode-title-bibcode-bibcode-a-a-rel-nofollow-class-external-text-href-http-adsabs-harvard-edu-abs-2014arxiv1409-4842s-2014arxiv1409-4842s-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-computing-research-repository-rft-atitle-going-deeper-with-convolutions-rft-date-2014-rft-id-info-3aarxiv-2f1409-4842-rft-id-info-3abibcode-2f2014arxiv1409-4842s-rft-aulast-szegedy-rft-aufirst-christian-rft-au-liu-2c-wei-rft-au-jia-2c-yangqing-rft-au-sermanet-2c-pierre-rft-au-reed-2c-scott-rft-au-anguelov-2c-dragomir-rft-au-erhan-2c-dumitru-rft-au-vanhoucke-2c-vincent-rft-au-rabinovich-2c-andrew-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-78-span-class-mw-cite-backlink-b-a-href-cite-ref-78-a-b-span-span-class-reference-text-cite-class-citation-arxiv-russakovsky-olga-deng-jia-su-hao-krause-jonathan-satheesh-sanjeev-ma-sean-huang-zhiheng-a-href-wiki-andrej-karpathy-title-andrej-karpathy-karpathy-andrej-a-khosla-aditya-bernstein-michael-berg-alexander-c-fei-fei-li-2014-image-i-net-i-large-scale-visual-recognition-challenge-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1409-0575-1409-0575-a-span-a-rel-nofollow-class-external-text-href-arxiv-org-archive-cs-cv-cs-cv-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-preprint-rft-jtitle-arxiv-rft-atitle-image-net-large-scale-visual-recognition-challenge-rft-date-2014-rft-id-info-3aarxiv-2f1409-0575-rft-aulast-russakovsky-rft-aufirst-olga-rft-au-deng-2c-jia-rft-au-su-2c-hao-rft-au-krause-2c-jonathan-rft-au-satheesh-2c-sanjeev-rft-au-ma-2c-sean-rft-au-huang-2c-zhiheng-rft-au-karpathy-2c-andrej-rft-au-khosla-2c-aditya-rft-au-bernstein-2c-michael-rft-au-berg-2c-alexander-c-rft-au-fei-fei-2c-li-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-79-span-class-mw-cite-backlink-b-a-href-cite-ref-79-a-b-span-span-class-reference-text-cite-class-citation-news-a-rel-nofollow-class-external-text-href-http-www-technologyreview-com-view-535201-the-face-detection-algorithm-set-to-revolutionize-image-search-the-face-detection-algorithm-set-to-revolutionize-image-search-a-i-technology-review-i-february-16-2015-span-class-reference-accessdate-retrieved-span-class-nowrap-27-october-span-2017-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-technology-review-rft-atitle-the-face-detection-algorithm-set-to-revolutionize-image-search-rft-date-2015-02-16-rft-id-http-3a-2f-2fwww-technologyreview-com-2fview-2f535201-2fthe-face-detection-algorithm-set-to-revolutionize-image-search-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-80-span-class-mw-cite-backlink-b-a-href-cite-ref-80-a-b-span-span-class-reference-text-cite-class-citation-book-baccouche-moez-mamalet-franck-wolf-christian-garcia-christophe-baskurt-atilla-2011-11-16-sequential-deep-learning-for-human-action-recognition-in-salah-albert-ali-lepri-bruno-eds-i-human-behavior-unterstanding-i-lecture-notes-in-computer-science-b-7065-b-springer-berlin-heidelberg-pp-29-39-a-href-wiki-citeseerx-title-citeseerx-citeseerx-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-citeseerx-ist-psu-edu-viewdoc-summary-doi-10-1-1-385-4740-10-1-1-385-4740-a-span-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1007-2f978-3-642-25446-8-4-10-1007-978-3-642-25446-8-4-a-a-href-wiki-international-standard-book-number-title-international-standard-book-number-isbn-a-a-href-wiki-special-booksources-978-3-642-25445-1-title-special-booksources-978-3-642-25445-1-bdi-978-3-642-25445-1-bdi-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-bookitem-rft-atitle-sequential-deep-learning-for-human-action-recognition-rft-btitle-human-behavior-unterstanding-rft-series-lecture-notes-in-computer-science-rft-pages-29-39-rft-pub-springer-berlin-heidelberg-rft-date-2011-11-16-rft-id-2f-2fciteseerx-ist-psu-edu-2fviewdoc-2fsummary-3fdoi-3d10-1-1-385-4740-rft-id-info-3adoi-2f10-1007-2f978-3-642-25446-8-4-rft-isbn-978-3-642-25445-1-rft-aulast-baccouche-rft-aufirst-moez-rft-au-mamalet-2c-franck-rft-au-wolf-2c-christian-rft-au-garcia-2c-christophe-rft-au-baskurt-2c-atilla-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-81-span-class-mw-cite-backlink-b-a-href-cite-ref-81-a-b-span-span-class-reference-text-cite-class-citation-journal-ji-shuiwang-xu-wei-yang-ming-yu-kai-2013-01-01-3d-convolutional-neural-networks-for-human-action-recognition-i-ieee-transactions-on-pattern-analysis-and-machine-intelligence-i-b-35-b-1-221-231-a-href-wiki-citeseerx-title-citeseerx-citeseerx-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-citeseerx-ist-psu-edu-viewdoc-summary-doi-10-1-1-169-4046-10-1-1-169-4046-a-span-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1109-2ftpami-2012-59-10-1109-tpami-2012-59-a-a-href-wiki-international-standard-serial-number-title-international-standard-serial-number-issn-a-a-rel-nofollow-class-external-text-href-www-worldcat-org-issn-0162-8828-0162-8828-a-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-22392705-22392705-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-ieee-transactions-on-pattern-analysis-and-machine-intelligence-rft-atitle-3d-convolutional-neural-networks-for-human-action-recognition-rft-volume-35-rft-issue-1-rft-pages-221-231-rft-date-2013-01-01-rft-id-2f-2fciteseerx-ist-psu-edu-2fviewdoc-2fsummary-3fdoi-3d10-1-1-169-4046-rft-issn-0162-8828-rft-id-info-3apmid-2f22392705-rft-id-info-3adoi-2f10-1109-2ftpami-2012-59-rft-aulast-ji-rft-aufirst-shuiwang-rft-au-xu-2c-wei-rft-au-yang-2c-ming-rft-au-yu-2c-kai-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-82-span-class-mw-cite-backlink-b-a-href-cite-ref-82-a-b-span-span-class-reference-text-cite-class-citation-arxiv-huang-jie-zhou-wengang-zhang-qilin-li-houqiang-li-weiping-2018-video-based-sign-language-recognition-without-temporal-segmentation-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1801-10111-1801-10111-a-span-a-rel-nofollow-class-external-text-href-arxiv-org-archive-cs-cv-cs-cv-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-preprint-rft-jtitle-arxiv-rft-atitle-video-based-sign-language-recognition-without-temporal-segmentation-rft-date-2018-rft-id-info-3aarxiv-2f1801-10111-rft-aulast-huang-rft-aufirst-jie-rft-au-zhou-2c-wengang-rft-au-zhang-2c-qilin-rft-au-li-2c-houqiang-rft-au-li-2c-weiping-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-83-span-class-mw-cite-backlink-b-a-href-cite-ref-83-a-b-span-span-class-reference-text-karpathy-andrej-et-al-large-scale-video-classification-with-convolutional-neural-networks-ieee-conference-on-computer-vision-and-pattern-recognition-cvpr-2014-span-li-li-id-cite-note-84-span-class-mw-cite-backlink-b-a-href-cite-ref-84-a-b-span-span-class-reference-text-cite-class-citation-arxiv-simonyan-karen-zisserman-andrew-2014-two-stream-convolutional-networks-for-action-recognition-in-videos-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1406-2199-1406-2199-a-span-a-rel-nofollow-class-external-text-href-arxiv-org-archive-cs-cv-cs-cv-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-preprint-rft-jtitle-arxiv-rft-atitle-two-stream-convolutional-networks-for-action-recognition-in-videos-rft-date-2014-rft-id-info-3aarxiv-2f1406-2199-rft-aulast-simonyan-rft-aufirst-karen-rft-au-zisserman-2c-andrew-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-2014-span-li-li-id-cite-note-wang-duan-zhang-niu-p-1657-85-span-class-mw-cite-backlink-b-a-href-cite-ref-wang-duan-zhang-niu-p-1657-85-0-a-b-span-span-class-reference-text-cite-class-citation-journal-wang-le-duan-xuhuan-zhang-qilin-niu-zhenxing-hua-gang-zheng-nanning-2018-05-22-a-rel-nofollow-class-external-text-href-https-qilin-zhang-github-io-pages-pdfs-segment-tube-spatio-temporal-action-localization-in-untrimmed-videos-with-per-frame-segmentation-pdf-segment-tube-spatio-temporal-action-localization-in-untrimmed-videos-with-per-frame-segmentation-a-span-class-cs1-format-pdf-span-i-sensors-i-b-18-b-5-1657-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-3390-2fs18051657-10-3390-s18051657-a-a-href-wiki-international-standard-serial-number-title-international-standard-serial-number-issn-a-a-rel-nofollow-class-external-text-href-www-worldcat-org-issn-1424-8220-1424-8220-a-a-href-wiki-pubmed-central-title-pubmed-central-pmc-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pmc-articles-pmc5982167-5982167-a-span-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-29789447-29789447-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-sensors-rft-atitle-segment-tube-3a-spatio-temporal-action-localization-in-untrimmed-videos-with-per-frame-segmentation-rft-volume-18-rft-issue-5-rft-pages-1657-rft-date-2018-05-22-rft-id-2f-2fwww-ncbi-nlm-nih-gov-2fpmc-2farticles-2fpmc5982167-rft-issn-1424-8220-rft-id-info-3apmid-2f29789447-rft-id-info-3adoi-2f10-3390-2fs18051657-rft-aulast-wang-rft-aufirst-le-rft-au-duan-2c-xuhuan-rft-au-zhang-2c-qilin-rft-au-niu-2c-zhenxing-rft-au-hua-2c-gang-rft-au-zheng-2c-nanning-rft-id-https-3a-2f-2fqilin-zhang-github-io-2f-pages-2fpdfs-2fsegment-tube-spatio-temporal-action-localization-in-untrimmed-videos-with-per-frame-segmentation-pdf-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-duan-wang-zhai-zheng-2018-p-86-span-class-mw-cite-backlink-b-a-href-cite-ref-duan-wang-zhai-zheng-2018-p-86-0-a-b-span-span-class-reference-text-cite-class-citation-conference-duan-xuhuan-wang-le-zhai-changbo-zheng-nanning-zhang-qilin-niu-zhenxing-hua-gang-2018-i-joint-spatio-temporal-action-localization-in-untrimmed-videos-with-per-frame-segmentation-i-25th-ieee-international-conference-on-image-processing-icip-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1109-2ficip-2018-8451692-10-1109-icip-2018-8451692-a-a-href-wiki-international-standard-book-number-title-international-standard-book-number-isbn-a-a-href-wiki-special-booksources-978-1-4799-7061-2-title-special-booksources-978-1-4799-7061-2-bdi-978-1-4799-7061-2-bdi-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-conference-rft-btitle-joint-spatio-temporal-action-localization-in-untrimmed-videos-with-per-frame-segmentation-rft-pub-25th-ieee-international-conference-on-image-processing-28icip-29-rft-date-2018-rft-id-info-3adoi-2f10-1109-2ficip-2018-8451692-rft-isbn-978-1-4799-7061-2-rft-aulast-duan-rft-aufirst-xuhuan-rft-au-wang-2c-le-rft-au-zhai-2c-changbo-rft-au-zheng-2c-nanning-rft-au-zhang-2c-qilin-rft-au-niu-2c-zhenxing-rft-au-hua-2c-gang-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-87-span-class-mw-cite-backlink-b-a-href-cite-ref-87-a-b-span-span-class-reference-text-cite-class-citation-book-taylor-graham-w-fergus-rob-lecun-yann-bregler-christoph-2010-01-01-a-rel-nofollow-class-external-text-href-http-dl-acm-org-citation-cfm-id-1888212-1888225-i-convolutional-learning-of-spatio-temporal-features-i-a-i-proceedings-of-the-11th-european-conference-on-computer-vision-part-vi-i-eccv-10-berlin-heidelberg-springer-verlag-pp-140-153-a-href-wiki-international-standard-book-number-title-international-standard-book-number-isbn-a-a-href-wiki-special-booksources-978-3-642-15566-6-title-special-booksources-978-3-642-15566-6-bdi-978-3-642-15566-6-bdi-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-book-rft-btitle-convolutional-learning-of-spatio-temporal-features-rft-place-berlin-2c-heidelberg-rft-series-eccv-2710-rft-pages-140-153-rft-pub-springer-verlag-rft-date-2010-01-01-rft-isbn-978-3-642-15566-6-rft-aulast-taylor-rft-aufirst-graham-w-rft-au-fergus-2c-rob-rft-au-lecun-2c-yann-rft-au-bregler-2c-christoph-rft-id-http-3a-2f-2fdl-acm-org-2fcitation-cfm-3fid-3d1888212-1888225-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-88-span-class-mw-cite-backlink-b-a-href-cite-ref-88-a-b-span-span-class-reference-text-cite-class-citation-book-le-q-v-zou-w-y-yeung-s-y-ng-a-y-2011-01-01-i-learning-hierarchical-invariant-spatio-temporal-features-for-action-recognition-with-independent-subspace-analysis-i-i-proceedings-of-the-2011-ieee-conference-on-computer-vision-and-pattern-recognition-i-cvpr-11-washington-dc-usa-ieee-computer-society-pp-3361-3368-a-href-wiki-citeseerx-title-citeseerx-citeseerx-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-citeseerx-ist-psu-edu-viewdoc-summary-doi-10-1-1-294-5948-10-1-1-294-5948-a-span-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1109-2fcvpr-2011-5995496-10-1109-cvpr-2011-5995496-a-a-href-wiki-international-standard-book-number-title-international-standard-book-number-isbn-a-a-href-wiki-special-booksources-978-1-4577-0394-2-title-special-booksources-978-1-4577-0394-2-bdi-978-1-4577-0394-2-bdi-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-book-rft-btitle-learning-hierarchical-invariant-spatio-temporal-features-for-action-recognition-with-independent-subspace-analysis-rft-place-washington-2c-dc-2c-usa-rft-series-cvpr-2711-rft-pages-3361-3368-rft-pub-ieee-computer-society-rft-date-2011-01-01-rft-id-2f-2fciteseerx-ist-psu-edu-2fviewdoc-2fsummary-3fdoi-3d10-1-1-294-5948-rft-id-info-3adoi-2f10-1109-2fcvpr-2011-5995496-rft-isbn-978-1-4577-0394-2-rft-aulast-le-rft-aufirst-q-v-rft-au-zou-2c-w-y-rft-au-yeung-2c-s-y-rft-au-ng-2c-a-y-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-89-span-class-mw-cite-backlink-b-a-href-cite-ref-89-a-b-span-span-class-reference-text-cite-class-citation-arxiv-grefenstette-edward-blunsom-phil-de-freitas-nando-hermann-karl-moritz-2014-04-29-a-deep-architecture-for-semantic-parsing-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1404-7296-1404-7296-a-span-a-rel-nofollow-class-external-text-href-arxiv-org-archive-cs-cl-cs-cl-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-preprint-rft-jtitle-arxiv-rft-atitle-a-deep-architecture-for-semantic-parsing-rft-date-2014-04-29-rft-id-info-3aarxiv-2f1404-7296-rft-aulast-grefenstette-rft-aufirst-edward-rft-au-blunsom-2c-phil-rft-au-de-freitas-2c-nando-rft-au-hermann-2c-karl-moritz-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-90-span-class-mw-cite-backlink-b-a-href-cite-ref-90-a-b-span-span-class-reference-text-cite-class-citation-journal-mesnil-gregoire-deng-li-gao-jianfeng-he-xiaodong-shen-yelong-april-2014-a-rel-nofollow-class-external-text-href-http-research-microsoft-com-apps-pubs-default-aspx-id-214617-learning-semantic-representations-using-convolutional-neural-networks-for-web-search-microsoft-research-a-i-microsoft-research-i-span-class-reference-accessdate-retrieved-span-class-nowrap-2015-12-17-span-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-microsoft-research-rft-atitle-learning-semantic-representations-using-convolutional-neural-networks-for-web-search-e2-80-93-microsoft-research-rft-date-2014-04-rft-aulast-mesnil-rft-aufirst-gregoire-rft-au-deng-2c-li-rft-au-gao-2c-jianfeng-rft-au-he-2c-xiaodong-rft-au-shen-2c-yelong-rft-id-http-3a-2f-2fresearch-microsoft-com-2fapps-2fpubs-2fdefault-aspx-3fid-3d214617-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-91-span-class-mw-cite-backlink-b-a-href-cite-ref-91-a-b-span-span-class-reference-text-cite-class-citation-arxiv-kalchbrenner-nal-grefenstette-edward-blunsom-phil-2014-04-08-a-convolutional-neural-network-for-modelling-sentences-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1404-2188-1404-2188-a-span-a-rel-nofollow-class-external-text-href-arxiv-org-archive-cs-cl-cs-cl-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-preprint-rft-jtitle-arxiv-rft-atitle-a-convolutional-neural-network-for-modelling-sentences-rft-date-2014-04-08-rft-id-info-3aarxiv-2f1404-2188-rft-aulast-kalchbrenner-rft-aufirst-nal-rft-au-grefenstette-2c-edward-rft-au-blunsom-2c-phil-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-92-span-class-mw-cite-backlink-b-a-href-cite-ref-92-a-b-span-span-class-reference-text-cite-class-citation-arxiv-kim-yoon-2014-08-25-convolutional-neural-networks-for-sentence-classification-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1408-5882-1408-5882-a-span-a-rel-nofollow-class-external-text-href-arxiv-org-archive-cs-cl-cs-cl-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-preprint-rft-jtitle-arxiv-rft-atitle-convolutional-neural-networks-for-sentence-classification-rft-date-2014-08-25-rft-id-info-3aarxiv-2f1408-5882-rft-aulast-kim-rft-aufirst-yoon-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-93-span-class-mw-cite-backlink-b-a-href-cite-ref-93-a-b-span-span-class-reference-text-collobert-ronan-and-jason-weston-a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning-proceedings-of-the-25th-international-conference-on-machine-learning-acm-2008-span-li-li-id-cite-note-94-span-class-mw-cite-backlink-b-a-href-cite-ref-94-a-b-span-span-class-reference-text-cite-class-citation-arxiv-collobert-ronan-weston-jason-bottou-leon-karlen-michael-kavukcuoglu-koray-kuksa-pavel-2011-03-02-natural-language-processing-almost-from-scratch-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1103-0398-1103-0398-a-span-a-rel-nofollow-class-external-text-href-arxiv-org-archive-cs-lg-cs-lg-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-preprint-rft-jtitle-arxiv-rft-atitle-natural-language-processing-28almost-29-from-scratch-rft-date-2011-03-02-rft-id-info-3aarxiv-2f1103-0398-rft-aulast-collobert-rft-aufirst-ronan-rft-au-weston-2c-jason-rft-au-bottou-2c-leon-rft-au-karlen-2c-michael-rft-au-kavukcuoglu-2c-koray-rft-au-kuksa-2c-pavel-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-95-span-class-mw-cite-backlink-b-a-href-cite-ref-95-a-b-span-span-class-reference-text-cite-class-citation-arxiv-wallach-izhar-dzamba-michael-heifets-abraham-2015-10-09-atomnet-a-deep-convolutional-neural-network-for-bioactivity-prediction-in-structure-based-drug-discovery-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1510-02855-1510-02855-a-span-a-rel-nofollow-class-external-text-href-arxiv-org-archive-cs-lg-cs-lg-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-preprint-rft-jtitle-arxiv-rft-atitle-atomnet-3a-a-deep-convolutional-neural-network-for-bioactivity-prediction-in-structure-based-drug-discovery-rft-date-2015-10-09-rft-id-info-3aarxiv-2f1510-02855-rft-aulast-wallach-rft-aufirst-izhar-rft-au-dzamba-2c-michael-rft-au-heifets-2c-abraham-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-96-span-class-mw-cite-backlink-b-a-href-cite-ref-96-a-b-span-span-class-reference-text-cite-class-citation-arxiv-yosinski-jason-clune-jeff-nguyen-anh-fuchs-thomas-lipson-hod-2015-06-22-understanding-neural-networks-through-deep-visualization-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1506-06579-1506-06579-a-span-a-rel-nofollow-class-external-text-href-arxiv-org-archive-cs-cv-cs-cv-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-preprint-rft-jtitle-arxiv-rft-atitle-understanding-neural-networks-through-deep-visualization-rft-date-2015-06-22-rft-id-info-3aarxiv-2f1506-06579-rft-aulast-yosinski-rft-aufirst-jason-rft-au-clune-2c-jeff-rft-au-nguyen-2c-anh-rft-au-fuchs-2c-thomas-rft-au-lipson-2c-hod-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-97-span-class-mw-cite-backlink-b-a-href-cite-ref-97-a-b-span-span-class-reference-text-cite-class-citation-news-a-rel-nofollow-class-external-text-href-https-www-theglobeandmail-com-report-on-business-small-business-starting-out-toronto-startup-has-a-faster-way-to-discover-effective-medicines-article25660419-toronto-startup-has-a-faster-way-to-discover-effective-medicines-a-i-the-globe-and-mail-i-span-class-reference-accessdate-retrieved-span-class-nowrap-2015-11-09-span-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-the-globe-and-mail-rft-atitle-toronto-startup-has-a-faster-way-to-discover-effective-medicines-rft-id-https-3a-2f-2fwww-theglobeandmail-com-2freport-on-business-2fsmall-business-2fstarting-out-2ftoronto-startup-has-a-faster-way-to-discover-effective-medicines-2farticle25660419-2f-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-98-span-class-mw-cite-backlink-b-a-href-cite-ref-98-a-b-span-span-class-reference-text-cite-class-citation-web-a-rel-nofollow-class-external-text-href-http-ww2-kqed-org-futureofyou-2015-05-27-startup-harnesses-supercomputers-to-seek-cures-startup-harnesses-supercomputers-to-seek-cures-a-i-kqed-future-of-you-i-2015-05-27-span-class-reference-accessdate-retrieved-span-class-nowrap-2015-11-09-span-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-unknown-rft-jtitle-kqed-future-of-you-rft-atitle-startup-harnesses-supercomputers-to-seek-cures-rft-date-2015-05-27-rft-id-http-3a-2f-2fww2-kqed-org-2ffutureofyou-2f2015-2f05-2f27-2fstartup-harnesses-supercomputers-to-seek-cures-2f-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-pmid-29581467-99-span-class-mw-cite-backlink-b-a-href-cite-ref-pmid-29581467-99-0-a-b-span-span-class-reference-text-cite-class-citation-journal-tim-pyrkov-konstantin-slipensky-mikhail-barg-alexey-kondrashin-boris-zhurov-alexander-zenin-mikhail-pyatnitskiy-leonid-menshikov-sergei-markov-and-peter-o-fedichev-2018-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pmc-articles-pmc5980076-extracting-biological-age-from-biomedical-data-via-deep-learning-too-much-of-a-good-thing-a-i-scientific-reports-i-b-8-b-1-5210-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1038-2fs41598-018-23534-9-10-1038-s41598-018-23534-9-a-a-href-wiki-pubmed-central-title-pubmed-central-pmc-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pmc-articles-pmc5980076-5980076-a-span-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-29581467-29581467-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-scientific-reports-rft-atitle-extracting-biological-age-from-biomedical-data-via-deep-learning-3a-too-much-of-a-good-thing-3f-rft-volume-8-rft-issue-1-rft-pages-5210-rft-date-2018-rft-id-2f-2fwww-ncbi-nlm-nih-gov-2fpmc-2farticles-2fpmc5980076-rft-id-info-3apmid-2f29581467-rft-id-info-3adoi-2f10-1038-2fs41598-018-23534-9-rft-au-tim-pyrkov-2c-konstantin-slipensky-2c-mikhail-barg-2c-alexey-kondrashin-2c-boris-zhurov-2c-alexander-zenin-2c-mikhail-pyatnitskiy-2c-leonid-menshikov-2c-sergei-markov-2c-and-peter-o-fedichev-rft-id-2f-2fwww-ncbi-nlm-nih-gov-2fpmc-2farticles-2fpmc5980076-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-span-class-cs1-maint-citation-comment-cs1-maint-multiple-names-authors-list-a-href-wiki-category-cs1-maint-multiple-names-authors-list-title-category-cs1-maint-multiple-names-authors-list-link-a-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-100-span-class-mw-cite-backlink-b-a-href-cite-ref-100-a-b-span-span-class-reference-text-cite-class-citation-journal-chellapilla-k-fogel-db-1999-evolving-neural-networks-to-play-checkers-without-relying-on-expert-knowledge-i-ieee-trans-neural-netw-i-b-10-b-6-1382-91-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1109-2f72-809083-10-1109-72-809083-a-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-18252639-18252639-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-ieee-trans-neural-netw-rft-atitle-evolving-neural-networks-to-play-checkers-without-relying-on-expert-knowledge-rft-volume-10-rft-issue-6-rft-pages-1382-91-rft-date-1999-rft-id-info-3adoi-2f10-1109-2f72-809083-rft-id-info-3apmid-2f18252639-rft-aulast-chellapilla-rft-aufirst-k-rft-au-fogel-2c-db-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-101-span-class-mw-cite-backlink-b-a-href-cite-ref-101-a-b-span-span-class-reference-text-cite-class-citation-journal-chellapilla-k-fogel-d-b-2001-evolving-an-expert-checkers-playing-program-without-using-human-expertise-i-ieee-transactions-on-evolutionary-computation-i-b-5-b-4-422-428-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1109-2f4235-942536-10-1109-4235-942536-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-ieee-transactions-on-evolutionary-computation-rft-atitle-evolving-an-expert-checkers-playing-program-without-using-human-expertise-rft-volume-5-rft-issue-4-rft-pages-422-428-rft-date-2001-rft-id-info-3adoi-2f10-1109-2f4235-942536-rft-aulast-chellapilla-rft-aufirst-k-rft-au-fogel-2c-d-b-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-102-span-class-mw-cite-backlink-b-a-href-cite-ref-102-a-b-span-span-class-reference-text-cite-class-citation-book-a-href-wiki-david-b-fogel-title-david-b-fogel-fogel-david-a-2001-i-blondie24-playing-at-the-edge-of-ai-i-san-francisco-ca-morgan-kaufmann-a-href-wiki-international-standard-book-number-title-international-standard-book-number-isbn-a-a-href-wiki-special-booksources-978-1558607835-title-special-booksources-978-1558607835-bdi-978-1558607835-bdi-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-book-rft-btitle-blondie24-3a-playing-at-the-edge-of-ai-rft-place-san-francisco-2c-ca-rft-pub-morgan-kaufmann-rft-date-2001-rft-isbn-978-1558607835-rft-aulast-fogel-rft-aufirst-david-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-103-span-class-mw-cite-backlink-b-a-href-cite-ref-103-a-b-span-span-class-reference-text-cite-class-citation-arxiv-clark-christopher-storkey-amos-2014-teaching-deep-convolutional-neural-networks-to-play-go-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1412-3409-1412-3409-a-span-a-rel-nofollow-class-external-text-href-arxiv-org-archive-cs-ai-cs-ai-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-preprint-rft-jtitle-arxiv-rft-atitle-teaching-deep-convolutional-neural-networks-to-play-go-rft-date-2014-rft-id-info-3aarxiv-2f1412-3409-rft-aulast-clark-rft-aufirst-christopher-rft-au-storkey-2c-amos-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-104-span-class-mw-cite-backlink-b-a-href-cite-ref-104-a-b-span-span-class-reference-text-cite-class-citation-arxiv-maddison-chris-j-huang-aja-sutskever-ilya-silver-david-2014-move-evaluation-in-go-using-deep-convolutional-neural-networks-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1412-6564-1412-6564-a-span-a-rel-nofollow-class-external-text-href-arxiv-org-archive-cs-lg-cs-lg-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-preprint-rft-jtitle-arxiv-rft-atitle-move-evaluation-in-go-using-deep-convolutional-neural-networks-rft-date-2014-rft-id-info-3aarxiv-2f1412-6564-rft-aulast-maddison-rft-aufirst-chris-j-rft-au-huang-2c-aja-rft-au-sutskever-2c-ilya-rft-au-silver-2c-david-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-105-span-class-mw-cite-backlink-b-a-href-cite-ref-105-a-b-span-span-class-reference-text-cite-class-citation-web-a-rel-nofollow-class-external-text-href-https-www-deepmind-com-alpha-go-html-alphago-google-deepmind-a-span-class-reference-accessdate-retrieved-span-class-nowrap-30-january-span-2016-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-unknown-rft-btitle-alphago-e2-80-93-google-deepmind-rft-id-https-3a-2f-2fwww-deepmind-com-2falpha-go-html-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-106-span-class-mw-cite-backlink-b-a-href-cite-ref-106-a-b-span-span-class-reference-text-durjoy-sen-maitra-ujjwal-bhattacharya-s-k-parui-a-rel-nofollow-class-external-text-href-http-ieeexplore-ieee-org-xpls-abs-all-jsp-arnumber-7333916-tag-1-cnn-based-common-approach-to-handwritten-character-recognition-of-multiple-scripts-a-in-document-analysis-and-recognition-icdar-2015-13th-international-conference-on-vol-no-pp-1021-1025-23-26-aug-2015-span-li-li-id-cite-note-interpretable-ml-symposium-2017-107-span-class-mw-cite-backlink-b-a-href-cite-ref-interpretable-ml-symposium-2017-107-0-a-b-span-span-class-reference-text-cite-class-citation-web-a-rel-nofollow-class-external-text-href-http-interpretable-ml-nips-2017-a-i-interpretable-ml-symposium-i-2017-10-20-span-class-reference-accessdate-retrieved-span-class-nowrap-2018-09-12-span-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-unknown-rft-jtitle-interpretable-ml-symposium-rft-atitle-nips-2017-rft-date-2017-10-20-rft-id-http-3a-2f-2finterpretable-ml-2f-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-zang-wang-liu-zhang-2018-pp-97-108-108-span-class-mw-cite-backlink-b-a-href-cite-ref-zang-wang-liu-zhang-2018-pp-97-108-108-0-a-b-span-span-class-reference-text-cite-class-citation-book-zang-jinliang-wang-le-liu-ziyi-zhang-qilin-hua-gang-zheng-nanning-2018-attention-based-temporal-weighted-convolutional-neural-network-for-action-recognition-i-ifip-advances-in-information-and-communication-technology-i-cham-springer-international-publishing-pp-97-108-a-href-wiki-arxiv-title-arxiv-arxiv-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-arxiv-org-abs-1803-07179-1803-07179-a-span-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1007-2f978-3-319-92007-8-9-10-1007-978-3-319-92007-8-9-a-a-href-wiki-international-standard-book-number-title-international-standard-book-number-isbn-a-a-href-wiki-special-booksources-978-3-319-92006-1-title-special-booksources-978-3-319-92006-1-bdi-978-3-319-92006-1-bdi-a-a-href-wiki-international-standard-serial-number-title-international-standard-serial-number-issn-a-a-rel-nofollow-class-external-text-href-www-worldcat-org-issn-1868-4238-1868-4238-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-bookitem-rft-atitle-attention-based-temporal-weighted-convolutional-neural-network-for-action-recognition-rft-btitle-ifip-advances-in-information-and-communication-technology-rft-place-cham-rft-pages-97-108-rft-pub-springer-international-publishing-rft-date-2018-rft-id-info-3aarxiv-2f1803-07179-rft-issn-1868-4238-rft-id-info-3adoi-2f10-1007-2f978-3-319-92007-8-9-rft-isbn-978-3-319-92006-1-rft-aulast-zang-rft-aufirst-jinliang-rft-au-wang-2c-le-rft-au-liu-2c-ziyi-rft-au-zhang-2c-qilin-rft-au-hua-2c-gang-rft-au-zheng-2c-nanning-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-wang-zang-zhang-niu-p-1979-109-span-class-mw-cite-backlink-b-a-href-cite-ref-wang-zang-zhang-niu-p-1979-109-0-a-b-span-span-class-reference-text-cite-class-citation-journal-wang-le-zang-jinliang-zhang-qilin-niu-zhenxing-hua-gang-zheng-nanning-2018-06-21-a-rel-nofollow-class-external-text-href-https-qilin-zhang-github-io-pages-pdfs-sensors-18-01979-action-recognition-by-an-attention-aware-temporal-weighted-convolutional-neural-network-pdf-action-recognition-by-an-attention-aware-temporal-weighted-convolutional-neural-network-a-span-class-cs1-format-pdf-span-i-sensors-i-b-18-b-7-1979-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-3390-2fs18071979-10-3390-s18071979-a-a-href-wiki-international-standard-serial-number-title-international-standard-serial-number-issn-a-a-rel-nofollow-class-external-text-href-www-worldcat-org-issn-1424-8220-1424-8220-a-a-href-wiki-pubmed-central-title-pubmed-central-pmc-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pmc-articles-pmc6069475-6069475-a-span-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-29933555-29933555-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-sensors-rft-atitle-action-recognition-by-an-attention-aware-temporal-weighted-convolutional-neural-network-rft-volume-18-rft-issue-7-rft-pages-1979-rft-date-2018-06-21-rft-id-2f-2fwww-ncbi-nlm-nih-gov-2fpmc-2farticles-2fpmc6069475-rft-issn-1424-8220-rft-id-info-3apmid-2f29933555-rft-id-info-3adoi-2f10-3390-2fs18071979-rft-aulast-wang-rft-aufirst-le-rft-au-zang-2c-jinliang-rft-au-zhang-2c-qilin-rft-au-niu-2c-zhenxing-rft-au-hua-2c-gang-rft-au-zheng-2c-nanning-rft-id-https-3a-2f-2fqilin-zhang-github-io-2f-pages-2fpdfs-2fsensors-18-01979-action-recognition-by-an-attention-aware-temporal-weighted-convolutional-neural-network-pdf-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-dqn-110-span-class-mw-cite-backlink-b-a-href-cite-ref-dqn-110-0-a-b-span-span-class-reference-text-cite-class-citation-journal-mnih-volodymyr-et-al-2015-human-level-control-through-deep-reinforcement-learning-i-nature-i-b-518-b-7540-529-533-a-href-wiki-bibcode-title-bibcode-bibcode-a-a-rel-nofollow-class-external-text-href-http-adsabs-harvard-edu-abs-2015natur-518-529m-2015natur-518-529m-a-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1038-2fnature14236-10-1038-nature14236-a-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-25719670-25719670-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-nature-rft-atitle-human-level-control-through-deep-reinforcement-learning-rft-volume-518-rft-issue-7540-rft-pages-529-533-rft-date-2015-rft-id-info-3apmid-2f25719670-rft-id-info-3adoi-2f10-1038-2fnature14236-rft-id-info-3abibcode-2f2015natur-518-529m-rft-aulast-mnih-rft-aufirst-volodymyr-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-111-span-class-mw-cite-backlink-b-a-href-cite-ref-111-a-b-span-span-class-reference-text-cite-class-citation-journal-sun-r-sessions-c-june-2000-self-segmentation-of-sequences-automatic-formation-of-hierarchies-of-sequential-behaviors-i-ieee-transactions-on-systems-man-and-cybernetics-part-b-cybernetics-i-b-30-b-3-403-418-a-href-wiki-citeseerx-title-citeseerx-citeseerx-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-citeseerx-ist-psu-edu-viewdoc-summary-doi-10-1-1-11-226-10-1-1-11-226-a-span-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1109-2f3477-846230-10-1109-3477-846230-a-a-href-wiki-international-standard-serial-number-title-international-standard-serial-number-issn-a-a-rel-nofollow-class-external-text-href-www-worldcat-org-issn-1083-4419-1083-4419-a-a-href-wiki-pubmed-identifier-class-mw-redirect-title-pubmed-identifier-pmid-a-a-rel-nofollow-class-external-text-href-www-ncbi-nlm-nih-gov-pubmed-18252373-18252373-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-ieee-transactions-on-systems-2c-man-2c-and-cybernetics-2c-part-b-28cybernetics-29-rft-atitle-self-segmentation-of-sequences-3a-automatic-formation-of-hierarchies-of-sequential-behaviors-rft-volume-30-rft-issue-3-rft-pages-403-418-rft-date-2000-06-rft-id-2f-2fciteseerx-ist-psu-edu-2fviewdoc-2fsummary-3fdoi-3d10-1-1-11-226-rft-issn-1083-4419-rft-id-info-3apmid-2f18252373-rft-id-info-3adoi-2f10-1109-2f3477-846230-rft-aulast-sun-rft-aufirst-r-rft-au-sessions-2c-c-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-cdbn-cifar-112-span-class-mw-cite-backlink-b-a-href-cite-ref-cdbn-cifar-112-0-a-b-span-span-class-reference-text-cite-class-citation-web-a-rel-nofollow-class-external-text-href-http-www-cs-toronto-edu-kriz-conv-cifar10-aug2010-pdf-convolutional-deep-belief-networks-on-cifar-10-a-span-class-cs1-format-pdf-span-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-unknown-rft-btitle-convolutional-deep-belief-networks-on-cifar-10-rft-id-http-3a-2f-2fwww-cs-toronto-edu-2f-kriz-2fconv-cifar10-aug2010-pdf-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-cdbn-113-span-class-mw-cite-backlink-b-a-href-cite-ref-cdbn-113-0-a-b-span-span-class-reference-text-cite-class-citation-book-lee-honglak-grosse-roger-ranganath-rajesh-ng-andrew-y-1-january-2009-i-convolutional-deep-belief-networks-for-scalable-unsupervised-learning-of-hierarchical-representations-i-i-proceedings-of-the-26th-annual-international-conference-on-machine-learning-icml-09-i-acm-pp-609-616-a-href-wiki-citeseerx-title-citeseerx-citeseerx-a-span-class-cs1-lock-free-title-freely-accessible-a-rel-nofollow-class-external-text-href-citeseerx-ist-psu-edu-viewdoc-summary-doi-10-1-1-149-6800-10-1-1-149-6800-a-span-a-href-wiki-digital-object-identifier-title-digital-object-identifier-doi-a-a-rel-nofollow-class-external-text-href-doi-org-10-1145-2f1553374-1553453-10-1145-1553374-1553453-a-a-href-wiki-international-standard-book-number-title-international-standard-book-number-isbn-a-a-href-wiki-special-booksources-9781605585161-title-special-booksources-9781605585161-bdi-9781605585161-bdi-a-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3abook-rft-genre-book-rft-btitle-convolutional-deep-belief-networks-for-scalable-unsupervised-learning-of-hierarchical-representations-rft-pages-609-616-rft-pub-acm-rft-date-2009-01-01-rft-id-2f-2fciteseerx-ist-psu-edu-2fviewdoc-2fsummary-3fdoi-3d10-1-1-149-6800-rft-id-info-3adoi-2f10-1145-2f1553374-1553453-rft-isbn-9781605585161-rft-aulast-lee-rft-aufirst-honglak-rft-au-grosse-2c-roger-rft-au-ranganath-2c-rajesh-rft-au-ng-2c-andrew-y-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-114-span-class-mw-cite-backlink-b-a-href-cite-ref-114-a-b-span-span-class-reference-text-cite-class-citation-news-cade-metz-may-18-2016-a-rel-nofollow-class-external-text-href-https-www-wired-com-2016-05-google-tpu-custom-chips-google-built-its-very-own-chips-to-power-its-ai-bots-a-i-wired-i-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-article-rft-jtitle-wired-rft-atitle-google-built-its-very-own-chips-to-power-its-ai-bots-rft-date-2016-05-18-rft-au-cade-metz-rft-id-https-3a-2f-2fwww-wired-com-2f2016-2f05-2fgoogle-tpu-custom-chips-2f-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-li-id-cite-note-115-span-class-mw-cite-backlink-b-a-href-cite-ref-115-a-b-span-span-class-reference-text-cite-class-citation-web-a-rel-nofollow-class-external-text-href-https-keras-io-you-have-just-found-keras-keras-documentation-a-i-keras-io-i-cite-span-title-ctx-ver-z39-88-2004-rft-val-fmt-info-3aofi-2ffmt-3akev-3amtx-3ajournal-rft-genre-unknown-rft-jtitle-keras-io-rft-atitle-keras-documentation-rft-id-https-3a-2f-2fkeras-io-2f-23you-have-just-found-keras-rfr-id-info-3asid-2fen-wikipedia-org-3aconvolutional-neural-network-class-z3988-span-link-rel-mw-deduplicated-inline-style-href-mw-data-templatestyles-r886058088-span-li-ol-div-h2-span-class-mw-headline-id-external-links-external-links-span-span-class-mw-editsection-span-class-mw-editsection-bracket-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-section-61-title-edit-section-external-links-edit-a-span-class-mw-editsection-bracket-span-span-h2-ul-li-a-rel-nofollow-class-external-text-href-https-cs231n-github-io-cs231n-convolutional-neural-networks-for-visual-recognition-a-a-href-wiki-andrej-karpathy-title-andrej-karpathy-andrej-karpathy-a-s-a-href-wiki-stanford-university-title-stanford-university-stanford-a-computer-science-course-on-cnns-in-computer-vision-li-li-a-rel-nofollow-class-external-text-href-https-ujjwalkarn-me-2016-08-11-intuitive-explanation-convnets-an-intuitive-explanation-of-convolutional-neural-networks-a-a-beginner-level-introduction-to-what-convolutional-neural-networks-are-and-how-they-work-li-li-a-rel-nofollow-class-external-text-href-https-www-completegate-com-2017022864-blog-deep-machine-learning-images-lenet-alexnet-cnn-all-pages-convolutional-neural-networks-for-image-classification-a-literature-survey-li-ul-newpp-limit-report-parsed-by-mw1339-cached-time-20190626115414-cache-expiry-2592000-dynamic-content-false-complications-vary-revision-cpu-time-usage-1-864-seconds-real-time-usage-2-201-seconds-preprocessor-visited-node-count-8237-1000000-preprocessor-generated-node-count-0-1500000-post-expand-include-size-308983-2097152-bytes-template-argument-size-9811-2097152-bytes-highest-expansion-depth-11-40-expensive-parser-function-count-17-500-unstrip-recursion-depth-1-20-unstrip-post-expand-size-332613-5000000-bytes-number-of-wikibase-entities-loaded-7-400-lua-time-usage-1-144-10-000-seconds-lua-memory-usage-23-47-mb-50-mb-transclusion-expansion-time-report-ms-calls-template-100-00-1811-773-1-total-61-47-1113-735-2-template-reflist-26-21-474-924-42-template-cite-journal-13-31-241-176-14-template-cite-book-9-58-173-530-1-template-lang-en-gb-8-67-157-098-15-template-fix-5-63-101-976-9-template-citation-needed-5-45-98-830-15-template-cite-arxiv-5-36-97-074-20-template-cite-web-5-02-90-906-16-template-delink-saved-in-parser-cache-with-key-enwiki-pcache-idhash-40409788-0-canonical-math-5-and-timestamp-20190626115412-and-revision-id-903560845-div-noscript-img-src-en-wikipedia-org-wiki-special-centralautologin-start-type-1x1-alt-title-width-1-height-1-style-border-none-position-absolute-noscript-div-div-class-printfooter-retrieved-from-a-dir-ltr-href-https-en-wikipedia-org-w-index-php-title-convolutional-neural-network-oldid-903560845-https-en-wikipedia-org-w-index-php-title-convolutional-neural-network-oldid-903560845-a-div-div-id-catlinks-class-catlinks-data-mw-interface-div-id-mw-normal-catlinks-class-mw-normal-catlinks-a-href-wiki-help-category-title-help-category-categories-a-ul-li-a-href-wiki-category-artificial-neural-networks-title-category-artificial-neural-networks-artificial-neural-networks-a-li-li-a-href-wiki-category-computer-vision-title-category-computer-vision-computer-vision-a-li-li-a-href-wiki-category-computational-neuroscience-title-category-computational-neuroscience-computational-neuroscience-a-li-li-a-href-wiki-category-machine-learning-title-category-machine-learning-machine-learning-a-li-ul-div-div-id-mw-hidden-catlinks-class-mw-hidden-catlinks-mw-hidden-cats-hidden-hidden-categories-ul-li-a-href-wiki-category-cs1-maint-multiple-names-authors-list-title-category-cs1-maint-multiple-names-authors-list-cs1-maint-multiple-names-authors-list-a-li-li-a-href-wiki-category-articles-needing-additional-references-from-june-2019-title-category-articles-needing-additional-references-from-june-2019-articles-needing-additional-references-from-june-2019-a-li-li-a-href-wiki-category-all-articles-needing-additional-references-title-category-all-articles-needing-additional-references-all-articles-needing-additional-references-a-li-li-a-href-wiki-category-all-articles-with-unsourced-statements-title-category-all-articles-with-unsourced-statements-all-articles-with-unsourced-statements-a-li-li-a-href-wiki-category-articles-with-unsourced-statements-from-june-2019-title-category-articles-with-unsourced-statements-from-june-2019-articles-with-unsourced-statements-from-june-2019-a-li-li-a-href-wiki-category-articles-with-unsourced-statements-from-october-2017-title-category-articles-with-unsourced-statements-from-october-2017-articles-with-unsourced-statements-from-october-2017-a-li-li-a-href-wiki-category-articles-containing-explicitly-cited-english-language-text-title-category-articles-containing-explicitly-cited-english-language-text-articles-containing-explicitly-cited-english-language-text-a-li-li-a-href-wiki-category-wikipedia-articles-needing-clarification-from-december-2018-title-category-wikipedia-articles-needing-clarification-from-december-2018-wikipedia-articles-needing-clarification-from-december-2018-a-li-li-a-href-wiki-category-all-articles-needing-examples-title-category-all-articles-needing-examples-all-articles-needing-examples-a-li-li-a-href-wiki-category-articles-needing-examples-from-october-2017-title-category-articles-needing-examples-from-october-2017-articles-needing-examples-from-october-2017-a-li-li-a-href-wiki-category-articles-with-unsourced-statements-from-march-2019-title-category-articles-with-unsourced-statements-from-march-2019-articles-with-unsourced-statements-from-march-2019-a-li-li-a-href-wiki-category-articles-needing-additional-references-from-june-2017-title-category-articles-needing-additional-references-from-june-2017-articles-needing-additional-references-from-june-2017-a-li-li-a-href-wiki-category-articles-with-unsourced-statements-from-december-2018-title-category-articles-with-unsourced-statements-from-december-2018-articles-with-unsourced-statements-from-december-2018-a-li-li-a-href-wiki-category-all-articles-with-specifically-marked-weasel-worded-phrases-title-category-all-articles-with-specifically-marked-weasel-worded-phrases-all-articles-with-specifically-marked-weasel-worded-phrases-a-li-li-a-href-wiki-category-articles-with-specifically-marked-weasel-worded-phrases-from-december-2018-title-category-articles-with-specifically-marked-weasel-worded-phrases-from-december-2018-articles-with-specifically-marked-weasel-worded-phrases-from-december-2018-a-li-ul-div-div-div-class-visualclear-div-div-div-div-id-mw-navigation-h2-navigation-menu-h2-div-id-mw-head-div-id-p-personal-role-navigation-aria-labelledby-p-personal-label-h3-id-p-personal-label-personal-tools-h3-ul-li-id-pt-anonuserpage-not-logged-in-li-li-id-pt-anontalk-a-href-wiki-special-mytalk-title-discussion-about-edits-from-this-ip-address-n-accesskey-n-talk-a-li-li-id-pt-anoncontribs-a-href-wiki-special-mycontributions-title-a-list-of-edits-made-from-this-ip-address-y-accesskey-y-contributions-a-li-li-id-pt-createaccount-a-href-w-index-php-title-special-createaccount-returnto-convolutional-neural-network-title-you-are-encouraged-to-create-an-account-and-log-in-however-it-is-not-mandatory-create-account-a-li-li-id-pt-login-a-href-w-index-php-title-special-userlogin-returnto-convolutional-neural-network-title-youre-encouraged-to-log-in-however-its-not-mandatory-o-accesskey-o-log-in-a-li-ul-div-div-id-left-navigation-div-id-p-namespaces-role-navigation-class-vectortabs-aria-labelledby-p-namespaces-label-h3-id-p-namespaces-label-namespaces-h3-ul-li-id-ca-nstab-main-class-selected-span-a-href-wiki-convolutional-neural-network-title-view-the-content-page-c-accesskey-c-article-a-span-li-li-id-ca-talk-span-a-href-wiki-talk-convolutional-neural-network-rel-discussion-title-discussion-about-the-content-page-t-accesskey-t-talk-a-span-li-ul-div-div-id-p-variants-role-navigation-class-vectormenu-emptyportlet-aria-labelledby-p-variants-label-input-type-checkbox-class-vectormenucheckbox-aria-labelledby-p-variants-label-h3-id-p-variants-label-span-variants-span-h3-ul-class-menu-ul-div-div-div-id-right-navigation-div-id-p-views-role-navigation-class-vectortabs-aria-labelledby-p-views-label-h3-id-p-views-label-views-h3-ul-li-id-ca-view-class-collapsible-selected-span-a-href-wiki-convolutional-neural-network-read-a-span-li-li-id-ca-edit-class-collapsible-span-a-href-w-index-php-title-convolutional-neural-network-action-edit-title-edit-this-page-e-accesskey-e-edit-a-span-li-li-id-ca-history-class-collapsible-span-a-href-w-index-php-title-convolutional-neural-network-action-history-title-past-revisions-of-this-page-h-accesskey-h-view-history-a-span-li-ul-div-div-id-p-cactions-role-navigation-class-vectormenu-emptyportlet-aria-labelledby-p-cactions-label-input-type-checkbox-class-vectormenucheckbox-aria-labelledby-p-cactions-label-h3-id-p-cactions-label-span-more-span-h3-ul-class-menu-ul-div-div-id-p-search-role-search-h3-label-for-searchinput-search-label-h3-form-action-w-index-php-id-searchform-div-id-simplesearch-input-type-search-name-search-placeholder-search-wikipedia-title-search-wikipedia-f-accesskey-f-id-searchinput-input-type-hidden-value-special-search-name-title-input-type-submit-name-fulltext-value-search-title-search-wikipedia-for-this-text-id-mw-searchbutton-class-searchbutton-mw-fallbacksearchbutton-input-type-submit-name-go-value-go-title-go-to-a-page-with-this-exact-name-if-it-exists-id-searchbutton-class-searchbutton-div-form-div-div-div-div-id-mw-panel-div-id-p-logo-role-banner-a-class-mw-wiki-logo-href-wiki-main-page-title-visit-the-main-page-a-div-div-class-portal-role-navigation-id-p-navigation-aria-labelledby-p-navigation-label-h3-id-p-navigation-label-navigation-h3-div-class-body-ul-li-id-n-mainpage-description-a-href-wiki-main-page-title-visit-the-main-page-z-accesskey-z-main-page-a-li-li-id-n-contents-a-href-wiki-portal-contents-title-guides-to-browsing-wikipedia-contents-a-li-li-id-n-featuredcontent-a-href-wiki-portal-featured-content-title-featured-content-the-best-of-wikipedia-featured-content-a-li-li-id-n-currentevents-a-href-wiki-portal-current-events-title-find-background-information-on-current-events-current-events-a-li-li-id-n-randompage-a-href-wiki-special-random-title-load-a-random-article-x-accesskey-x-random-article-a-li-li-id-n-sitesupport-a-href-https-donate-wikimedia-org-wiki-special-fundraiserredirector-utm-source-donate-utm-medium-sidebar-utm-campaign-c13-en-wikipedia-org-uselang-en-title-support-us-donate-to-wikipedia-a-li-li-id-n-shoplink-a-href-shop-wikimedia-org-title-visit-the-wikipedia-store-wikipedia-store-a-li-ul-div-div-div-class-portal-role-navigation-id-p-interaction-aria-labelledby-p-interaction-label-h3-id-p-interaction-label-interaction-h3-div-class-body-ul-li-id-n-help-a-href-wiki-help-contents-title-guidance-on-how-to-use-and-edit-wikipedia-help-a-li-li-id-n-aboutsite-a-href-wiki-wikipedia-about-title-find-out-about-wikipedia-about-wikipedia-a-li-li-id-n-portal-a-href-wiki-wikipedia-community-portal-title-about-the-project-what-you-can-do-where-to-find-things-community-portal-a-li-li-id-n-recentchanges-a-href-wiki-special-recentchanges-title-a-list-of-recent-changes-in-the-wiki-r-accesskey-r-recent-changes-a-li-li-id-n-contactpage-a-href-en-wikipedia-org-wiki-wikipedia-contact-us-title-how-to-contact-wikipedia-contact-page-a-li-ul-div-div-div-class-portal-role-navigation-id-p-tb-aria-labelledby-p-tb-label-h3-id-p-tb-label-tools-h3-div-class-body-ul-li-id-t-whatlinkshere-a-href-wiki-special-whatlinkshere-convolutional-neural-network-title-list-of-all-english-wikipedia-pages-containing-links-to-this-page-j-accesskey-j-what-links-here-a-li-li-id-t-recentchangeslinked-a-href-wiki-special-recentchangeslinked-convolutional-neural-network-rel-nofollow-title-recent-changes-in-pages-linked-from-this-page-k-accesskey-k-related-changes-a-li-li-id-t-upload-a-href-wiki-wikipedia-file-upload-wizard-title-upload-files-u-accesskey-u-upload-file-a-li-li-id-t-specialpages-a-href-wiki-special-specialpages-title-a-list-of-all-special-pages-q-accesskey-q-special-pages-a-li-li-id-t-permalink-a-href-w-index-php-title-convolutional-neural-network-oldid-903560845-title-permanent-link-to-this-revision-of-the-page-permanent-link-a-li-li-id-t-info-a-href-w-index-php-title-convolutional-neural-network-action-info-title-more-information-about-this-page-page-information-a-li-li-id-t-wikibase-a-href-https-www-wikidata-org-wiki-special-entitypage-q17084460-title-link-to-connected-data-repository-item-g-accesskey-g-wikidata-item-a-li-li-id-t-cite-a-href-w-index-php-title-special-citethispage-page-convolutional-neural-network-id-903560845-title-information-on-how-to-cite-this-page-cite-this-page-a-li-ul-div-div-div-class-portal-role-navigation-id-p-coll-print-export-aria-labelledby-p-coll-print-export-label-h3-id-p-coll-print-export-label-print-export-h3-div-class-body-ul-li-id-coll-create-a-book-a-href-w-index-php-title-special-book-bookcmd-book-creator-referer-convolutional-neural-network-create-a-book-a-li-li-id-coll-download-as-rl-a-href-w-index-php-title-special-electronpdf-page-convolutional-neural-network-action-show-download-screen-download-as-pdf-a-li-li-id-t-print-a-href-w-index-php-title-convolutional-neural-network-printable-yes-title-printable-version-of-this-page-p-accesskey-p-printable-version-a-li-ul-div-div-div-class-portal-role-navigation-id-p-lang-aria-labelledby-p-lang-label-h3-id-p-lang-label-languages-h3-div-class-body-ul-li-class-interlanguage-link-interwiki-ar-a-href-https-ar-wikipedia-org-wiki-d8-b4-d8-a8-d9-83-d8-a9-d8-b9-d8-b5-d8-a8-d9-88-d9-86-d9-8a-d8-a9-d8-a7-d9-84-d8-aa-d9-81-d8-a7-d9-81-d9-8a-d8-a9-title-shbk-sbwny-ltffy-arabic-lang-ar-hreflang-ar-class-interlanguage-link-target-l-rby-a-li-li-class-interlanguage-link-interwiki-ca-a-href-https-ca-wikipedia-org-wiki-xarxa-neuronal-convolucional-title-xarxa-neuronal-convolucional-catalan-lang-ca-hreflang-ca-class-interlanguage-link-target-catala-a-li-li-class-interlanguage-link-interwiki-de-a-href-https-de-wikipedia-org-wiki-convolutional-neural-network-title-convolutional-neural-network-german-lang-de-hreflang-de-class-interlanguage-link-target-deutsch-a-li-li-class-interlanguage-link-interwiki-es-a-href-https-es-wikipedia-org-wiki-redes-neuronales-convolucionales-title-redes-neuronales-convolucionales-spanish-lang-es-hreflang-es-class-interlanguage-link-target-espanol-a-li-li-class-interlanguage-link-interwiki-fa-a-href-https-fa-wikipedia-org-wiki-d8-b4-d8-a8-da-a9-d9-87-d8-b9-d8-b5-d8-a8-db-8c-d9-be-db-8c-da-86-d8-b4-db-8c-title-shbkhh-sby-pychshy-persian-lang-fa-hreflang-fa-class-interlanguage-link-target-frsy-a-li-li-class-interlanguage-link-interwiki-fr-a-href-https-fr-wikipedia-org-wiki-r-c3-a9seau-neuronal-convolutif-title-reseau-neuronal-convolutif-french-lang-fr-hreflang-fr-class-interlanguage-link-target-francais-a-li-li-class-interlanguage-link-interwiki-ko-a-href-https-ko-wikipedia-org-wiki-ed-95-a9-ec-84-b1-ea-b3-b1-ec-8b-a0-ea-b2-bd-eb-a7-9d-title-habseonggob-singyeongmang-korean-lang-ko-hreflang-ko-class-interlanguage-link-target-hangugeo-a-li-li-class-interlanguage-link-interwiki-it-a-href-https-it-wikipedia-org-wiki-rete-neurale-convoluzionale-title-rete-neurale-convoluzionale-italian-lang-it-hreflang-it-class-interlanguage-link-target-italiano-a-li-li-class-interlanguage-link-interwiki-ja-a-href-https-ja-wikipedia-org-wiki-e7-95-b3-e3-81-bf-e8-be-bc-e3-81-bf-e3-83-8b-e3-83-a5-e3-83-bc-e3-83-a9-e3-83-ab-e3-83-8d-e3-83-83-e3-83-88-e3-83-af-e3-83-bc-e3-82-af-title-die-miip-miniyurarunetutowaku-japanese-lang-ja-hreflang-ja-class-interlanguage-link-target-ri-ben-yu-a-li-li-class-interlanguage-link-interwiki-pt-a-href-https-pt-wikipedia-org-wiki-rede-neural-convolucional-title-rede-neural-convolucional-portuguese-lang-pt-hreflang-pt-class-interlanguage-link-target-portugues-a-li-li-class-interlanguage-link-interwiki-ru-a-href-https-ru-wikipedia-org-wiki-d0-a1-d0-b2-d1-91-d1-80-d1-82-d0-be-d1-87-d0-bd-d0-b0-d1-8f-d0-bd-d0-b5-d0-b9-d1-80-d0-be-d0-bd-d0-bd-d0-b0-d1-8f-d1-81-d0-b5-d1-82-d1-8c-title-sviortochnaia-neironnaia-set-russian-lang-ru-hreflang-ru-class-interlanguage-link-target-russkii-a-li-li-class-interlanguage-link-interwiki-uk-a-href-https-uk-wikipedia-org-wiki-d0-97-d0-b3-d0-be-d1-80-d1-82-d0-ba-d0-be-d0-b2-d0-b0-d0-bd-d0-b5-d0-b9-d1-80-d0-be-d0-bd-d0-bd-d0-b0-d0-bc-d0-b5-d1-80-d0-b5-d0-b6-d0-b0-title-zgortkova-neironna-merezha-ukrainian-lang-uk-hreflang-uk-class-interlanguage-link-target-ukrayinska-a-li-li-class-interlanguage-link-interwiki-zh-a-href-https-zh-wikipedia-org-wiki-e5-8d-b7-e7-a7-af-e7-a5-9e-e7-bb-8f-e7-bd-91-e7-bb-9c-title-juan-ji-shen-jing-wang-luo-chinese-lang-zh-hreflang-zh-class-interlanguage-link-target-zhong-wen-a-li-ul-div-class-after-portlet-after-portlet-lang-span-class-wb-langlinks-edit-wb-langlinks-link-a-href-https-www-wikidata-org-wiki-special-entitypage-q17084460-sitelinks-wikipedia-title-edit-interlanguage-links-class-wbc-editpage-edit-links-a-span-div-div-div-div-div-div-id-footer-role-contentinfo-ul-id-footer-info-li-id-footer-info-lastmod-this-page-was-last-edited-on-26-june-2019-at-11-54-span-class-anonymous-show-utc-span-li-li-id-footer-info-copyright-text-is-available-under-the-a-rel-license-href-en-wikipedia-org-wiki-wikipedia-text-of-creative-commons-attribution-sharealike-3-0-unported-license-creative-commons-attribution-sharealike-license-a-a-rel-license-href-creativecommons-org-licenses-by-sa-3-0-style-display-none-a-additional-terms-may-apply-by-using-this-site-you-agree-to-the-a-href-foundation-wikimedia-org-wiki-terms-of-use-terms-of-use-a-and-a-href-foundation-wikimedia-org-wiki-privacy-policy-privacy-policy-a-wikipedia-r-is-a-registered-trademark-of-the-a-href-www-wikimediafoundation-org-wikimedia-foundation-inc-a-a-non-profit-organization-li-ul-ul-id-footer-places-li-id-footer-places-privacy-a-href-https-foundation-wikimedia-org-wiki-privacy-policy-class-extiw-title-wmf-privacy-policy-privacy-policy-a-li-li-id-footer-places-about-a-href-wiki-wikipedia-about-title-wikipedia-about-about-wikipedia-a-li-li-id-footer-places-disclaimer-a-href-wiki-wikipedia-general-disclaimer-title-wikipedia-general-disclaimer-disclaimers-a-li-li-id-footer-places-contact-a-href-en-wikipedia-org-wiki-wikipedia-contact-us-contact-wikipedia-a-li-li-id-footer-places-developers-a-href-https-www-mediawiki-org-wiki-special-mylanguage-how-to-contribute-developers-a-li-li-id-footer-places-cookiestatement-a-href-https-foundation-wikimedia-org-wiki-cookie-statement-cookie-statement-a-li-li-id-footer-places-mobileview-a-href-en-m-wikipedia-org-w-index-php-title-convolutional-neural-network-mobileaction-toggle-view-mobile-class-noprint-stopmobileredirecttoggle-mobile-view-a-li-ul-ul-id-footer-icons-class-noprint-li-id-footer-copyrightico-a-href-https-wikimediafoundation-org-img-src-static-images-wikimedia-button-png-srcset-static-images-wikimedia-button-1-5x-png-1-5x-static-images-wikimedia-button-2x-png-2x-width-88-height-31-alt-wikimedia-foundation-a-li-li-id-footer-poweredbyico-a-href-https-www-mediawiki-org-img-src-static-images-poweredby-mediawiki-88x31-png-alt-powered-by-mediawiki-srcset-static-images-poweredby-mediawiki-132x47-png-1-5x-static-images-poweredby-mediawiki-176x62-png-2x-width-88-height-31-a-li-ul-div-style-clear-both-div-div-script-rlq-window-rlq-push-function-mw-config-set-wgpageparsereport-limitreport-cputime-1-864-walltime-2-201-ppvisitednodes-value-8237-limit-1000000-ppgeneratednodes-value-0-limit-1500000-postexpandincludesize-value-308983-limit-2097152-templateargumentsize-value-9811-limit-2097152-expansiondepth-value-11-limit-40-expensivefunctioncount-value-17-limit-500-unstrip-depth-value-1-limit-20-unstrip-size-value-332613-limit-5000000-entityaccesscount-value-7-limit-400-timingprofile-100-00-1811-773-1-total-61-47-1113-735-2-template-reflist-26-21-474-924-42-template-cite-journal-13-31-241-176-14-template-cite-book-9-58-173-530-1-template-lang-en-gb-8-67-157-098-15-template-fix-5-63-101-976-9-template-citation-needed-5-45-98-830-15-template-cite-arxiv-5-36-97-074-20-template-cite-web-5-02-90-906-16-template-delink-scribunto-limitreport-timeusage-value-1-144-limit-10-000-limitreport-memusage-value-24611696-limit-52428800-limitreport-logs-table-1-n-size-tiny-n-n-cachereport-origin-mw1339-timestamp-20190626115414-ttl-2592000-transientcontent-false-script-script-type-application-ld-json-context-https-schema-org-type-article-name-convolutional-neural-network-url-https-en-wikipedia-org-wiki-convolutional-neural-network-sameas-http-www-wikidata-org-entity-q17084460-mainentity-http-www-wikidata-org-entity-q17084460-author-type-organization-name-contributors-to-wikimedia-projects-publisher-type-organization-name-wikimedia-foundation-inc-logo-type-imageobject-url-https-www-wikimedia-org-static-images-wmf-hor-googpub-png-datepublished-2013-08-31t15-20-31z-datemodified-2019-06-26t11-54-05z-image-https-upload-wikimedia-org-wikipedia-commons-f-fe-kernel-machine-svg-headline-artificial-neural-network-script-script-rlq-window-rlq-push-function-mw-config-set-wgbackendresponsetime-131-wghostname-mw1262-script-body-html