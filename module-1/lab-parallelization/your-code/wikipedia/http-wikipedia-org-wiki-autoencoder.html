b'<!DOCTYPE html>\n<html class="client-nojs" lang="en" dir="ltr">\n<head>\n<meta charset="UTF-8"/>\n<title>Autoencoder - Wikipedia</title>\n<script>document.documentElement.className=document.documentElement.className.replace(/(^|\\s)client-nojs(\\s|$)/,"$1client-js$2");RLCONF={"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":!1,"wgNamespaceNumber":0,"wgPageName":"Autoencoder","wgTitle":"Autoencoder","wgCurRevisionId":901360758,"wgRevisionId":901360758,"wgArticleId":6836612,"wgIsArticle":!0,"wgIsRedirect":!1,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["All articles with unsourced statements","Articles with unsourced statements from May 2019","Articles with unsourced statements from December 2013","Artificial neural networks","Unsupervised learning"],"wgBreakFrames":!1,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb",\n"Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Autoencoder","wgRelevantArticleId":6836612,"wgRequestId":"XQj4UApAAEIAACWbyjMAAABV","wgCSPNonce":!1,"wgIsProbablyEditable":!0,"wgRelevantPageIsProbablyEditable":!0,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgMediaViewerOnClick":!0,"wgMediaViewerEnabledByDefault":!0,"wgPopupsReferencePreviews":!1,"wgPopupsConflictsWithNavPopupGadget":!1,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en"},"wgMFDisplayWikibaseDescriptions":{"search":!0,"nearby":!0,"watchlist":!0,"tagline":!1},"wgWMESchemaEditAttemptStepOversample":!1,"wgPoweredByHHVM":!0,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgWikibaseItemId":"Q786435","wgCentralAuthMobileDomain":!1,"wgEditSubmitButtonLabelPublish":!0};RLSTATE={"ext.gadget.charinsert-styles":"ready",\n"ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.toc.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","ext.3d.styles":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready"};RLPAGEMODULES=["ext.cite.ux-enhancements","ext.math.scripts","ext.scribunto.logs","site","mediawiki.page.startup","mediawiki.page.ready","mediawiki.toc","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar",\n"ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging","ext.wikimediaEvents","ext.navigationTiming","ext.uls.compactlinks","ext.uls.interface","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"];</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.loader.implement("user.tokens@0tffind",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"editToken":"+\\\\","patrolToken":"+\\\\","watchToken":"+\\\\","csrfToken":"+\\\\"});\n});});</script>\n<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.3d.styles%7Cext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.skinning.interface%7Cmediawiki.toc.styles%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>\n<script async="" src="/w/load.php?lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector"></script>\n<meta name="ResourceLoaderDynamicStyles" content=""/>\n<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=ext.gadget.charinsert-styles&amp;only=styles&amp;skin=vector"/>\n<link rel="stylesheet" href="/w/load.php?lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>\n<meta name="generator" content="MediaWiki 1.34.0-wmf.8"/>\n<meta name="referrer" content="origin"/>\n<meta name="referrer" content="origin-when-crossorigin"/>\n<meta name="referrer" content="origin-when-cross-origin"/>\n<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png"/>\n<link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Autoencoder"/>\n<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Autoencoder&amp;action=edit"/>\n<link rel="edit" title="Edit this page" href="/w/index.php?title=Autoencoder&amp;action=edit"/>\n<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>\n<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>\n<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>\n<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>\n<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>\n<link rel="canonical" href="https://en.wikipedia.org/wiki/Autoencoder"/>\n<link rel="dns-prefetch" href="//login.wikimedia.org"/>\n<link rel="dns-prefetch" href="//meta.wikimedia.org" />\n<!--[if lt IE 9]><script src="/w/load.php?lang=qqx&amp;modules=html5shiv&amp;only=scripts&amp;skin=fallback&amp;sync=1"></script><![endif]-->\n</head>\n<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject mw-editable page-Autoencoder rootpage-Autoencoder skin-vector action-view">\n<div id="mw-page-base" class="noprint"></div>\n<div id="mw-head-base" class="noprint"></div>\n<div id="content" class="mw-body" role="main">\n\t<a id="top"></a>\n\t<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div>\n\t<div class="mw-indicators mw-body-content">\n</div>\n\n\t<h1 id="firstHeading" class="firstHeading" lang="en">Autoencoder</h1>\n\t\n\t<div id="bodyContent" class="mw-body-content">\n\t\t<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>\n\t\t<div id="contentSub"></div>\n\t\t\n\t\t\n\t\t\n\t\t<div id="jump-to-nav"></div>\n\t\t<a class="mw-jump-link" href="#mw-head">Jump to navigation</a>\n\t\t<a class="mw-jump-link" href="#p-search">Jump to search</a>\n\t\t<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%"><tbody><tr><th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a> and<br /><a href="/wiki/Data_mining" title="Data mining">data mining</a></th></tr><tr><td style="padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;"><a href="/wiki/File:Kernel_Machine.svg" class="image"><img alt="Kernel Machine.svg" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/220px-Kernel_Machine.svg.png" decoding="async" width="220" height="100" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x" data-file-width="512" data-file-height="233" /></a></td></tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>\n<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>\n<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>\n<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>\n<li><a href="/wiki/Automated_machine_learning" title="Automated machine learning">AutoML</a></li>\n<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>\n<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>\n<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>\n<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>\n<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>\n<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>\n<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>\n<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>\n<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>\n<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><div style="padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br /><style data-mw-deduplicate="TemplateStyles:r886047488">.mw-parser-output .nobold{font-weight:normal}</style><span class="nobold"><span style="font-size:85%;">(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&#160;&#8226;&#32;<b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</span></span> </div></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>\n<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a>\n<ul><li><a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a></li>\n<li><a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a></li>\n<li><a href="/wiki/Random_forest" title="Random forest">Random forest</a></li></ul></li>\n<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>\n<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>\n<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>\n<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Artificial neural networks</a></li>\n<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>\n<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>\n<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>\n<li><a href="/wiki/Support-vector_machine" title="Support-vector machine">Support vector machine (SVM)</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>\n<li><a href="/wiki/CURE_data_clustering_algorithm" class="mw-redirect" title="CURE data clustering algorithm">CURE</a></li>\n<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>\n<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>\n<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation\xe2\x80\x93maximization algorithm">Expectation\xe2\x80\x93maximization (EM)</a></li>\n<li><br /><a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>\n<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>\n<li><a href="/wiki/Mean-shift" class="mw-redirect" title="Mean-shift">Mean-shift</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>\n<li><a href="/wiki/Canonical_correlation_analysis" class="mw-redirect" title="Canonical correlation analysis">CCA</a></li>\n<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>\n<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>\n<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>\n<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>\n<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a>\n<ul><li><a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a></li>\n<li><a href="/wiki/Conditional_random_field" title="Conditional random field">Conditional random field</a></li>\n<li><a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">Hidden Markov</a></li></ul></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/K-nearest_neighbors_classification" class="mw-redirect" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>\n<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Artificial_neural_networks" class="mw-redirect" title="Artificial neural networks">Artificial neural networks</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a class="mw-selflink selflink">Autoencoder</a></li>\n<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>\n<li><a href="/wiki/DeepDream" title="DeepDream">DeepDream</a></li>\n<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>\n<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a>\n<ul><li><a href="/wiki/Long_short-term_memory" title="Long short-term memory">LSTM</a></li>\n<li><a href="/wiki/Gated_recurrent_unit" title="Gated recurrent unit">GRU</a></li></ul></li>\n<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>\n<li><a href="/wiki/Generative_adversarial_network" title="Generative adversarial network">GAN</a></li>\n<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>\n<li><a href="/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a>\n<ul><li><a href="/wiki/U-Net" title="U-Net">U-Net</a></li></ul></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>\n<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State\xe2\x80\x93action\xe2\x80\x93reward\xe2\x80\x93state\xe2\x80\x93action">SARSA</a></li>\n<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Bias%E2%80%93variance_dilemma" class="mw-redirect" title="Bias\xe2\x80\x93variance dilemma">Bias\xe2\x80\x93variance dilemma</a></li>\n<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>\n<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>\n<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>\n<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>\n<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>\n<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik\xe2\x80\x93Chervonenkis theory">VC theory</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine-learning venues</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NIPS</a></li>\n<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>\n<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>\n<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>\n<li><a rel="nofollow" class="external text" href="https://arxiv.org/list/cs.LG/recent">ArXiv:cs.LG</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/Glossary_of_artificial_intelligence" title="Glossary of artificial intelligence">Glossary of artificial intelligence</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style="padding:0 0.1em 0.4em">\n<div class="NavFrame collapsed" style="border:none;padding:0"><div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Related articles</div><div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center"><div class="hlist">\n<ul><li><a href="/wiki/List_of_datasets_for_machine-learning_research" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>\n<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li></ul>\n</div></div></div></td>\n</tr><tr><td class="plainlist" style="padding:0.3em 0.4em 0.3em;font-weight:bold;border-top: 1px solid #aaa; border-bottom: 1px solid #aaa;border-top:1px solid #aaa;border-bottom:1px solid #aaa;">\n<ul><li><a href="/wiki/File:Portal-puzzle.svg" class="image"><img alt="Portal-puzzle.svg" src="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/16px-Portal-puzzle.svg.png" decoding="async" width="16" height="14" class="noviewer" srcset="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/24px-Portal-puzzle.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/32px-Portal-puzzle.svg.png 2x" data-file-width="32" data-file-height="28" /></a> <a href="/wiki/Portal:Machine_learning" title="Portal:Machine learning">Machine learning&#32;portal</a></li></ul></td></tr><tr><td style="text-align:right;font-size:115%;padding-top: 0.6em;"><div class="plainlinks hlist navbar mini"><ul><li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li><li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li><li class="nv-edit"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li></ul></div></td></tr></tbody></table>\n<div class="thumb tright"><div class="thumbinner" style="width:352px;"><a href="/wiki/File:Autoencoder_structure.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/2/28/Autoencoder_structure.png/350px-Autoencoder_structure.png" decoding="async" width="350" height="262" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/2/28/Autoencoder_structure.png/525px-Autoencoder_structure.png 1.5x, //upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png 2x" data-file-width="677" data-file-height="506" /></a>  <div class="thumbcaption"><div class="magnify"><a href="/wiki/File:Autoencoder_structure.png" class="internal" title="Enlarge"></a></div>Schematic structure of an autoencoder with 3 fully connected hidden layers.</div></div></div>\n<p>An <b>autoencoder</b> is a type of <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural network</a> used to learn <a href="/wiki/Feature_learning" title="Feature learning">efficient data codings</a> in an <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised</a> manner.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">&#91;1&#93;</a></sup><sup id="cite_ref-2" class="reference"><a href="#cite_note-2">&#91;2&#93;</a></sup> The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for <a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">dimensionality reduction</a>, by training the network to ignore signal \xe2\x80\x9cnoise\xe2\x80\x9d. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. Recently, the autoencoder concept has become more widely used for learning <a href="/wiki/Generative_model" title="Generative model">generative models</a> of data.<sup id="cite_ref-VAE_3-0" class="reference"><a href="#cite_note-VAE-3">&#91;3&#93;</a></sup><sup id="cite_ref-gan_faces_4-0" class="reference"><a href="#cite_note-gan_faces-4">&#91;4&#93;</a></sup> Some of the most powerful AIs in the 2010s involved sparse autoencoders stacked inside of <a href="/wiki/Deep_learning" title="Deep learning">deep</a> neural networks.<sup id="cite_ref-domingos_5-0" class="reference"><a href="#cite_note-domingos-5">&#91;5&#93;</a></sup>\n</p>\n<div id="toc" class="toc"><input type="checkbox" role="button" id="toctogglecheckbox" class="toctogglecheckbox" style="display:none" /><div class="toctitle" lang="en" dir="ltr"><h2>Contents</h2><span class="toctogglespan"><label class="toctogglelabel" for="toctogglecheckbox"></label></span></div>\n<ul>\n<li class="toclevel-1 tocsection-1"><a href="#Purpose"><span class="tocnumber">1</span> <span class="toctext">Purpose</span></a></li>\n<li class="toclevel-1 tocsection-2"><a href="#Structure"><span class="tocnumber">2</span> <span class="toctext">Structure</span></a>\n<ul>\n<li class="toclevel-2 tocsection-3"><a href="#Variations"><span class="tocnumber">2.1</span> <span class="toctext">Variations</span></a>\n<ul>\n<li class="toclevel-3 tocsection-4"><a href="#Denoising_autoencoder"><span class="tocnumber">2.1.1</span> <span class="toctext">Denoising autoencoder</span></a></li>\n<li class="toclevel-3 tocsection-5"><a href="#Sparse_autoencoder"><span class="tocnumber">2.1.2</span> <span class="toctext">Sparse autoencoder</span></a></li>\n<li class="toclevel-3 tocsection-6"><a href="#Variational_autoencoder_(VAE)"><span class="tocnumber">2.1.3</span> <span class="toctext">Variational autoencoder (VAE)</span></a></li>\n<li class="toclevel-3 tocsection-7"><a href="#Contractive_autoencoder_(CAE)"><span class="tocnumber">2.1.4</span> <span class="toctext">Contractive autoencoder (CAE)</span></a></li>\n</ul>\n</li>\n<li class="toclevel-2 tocsection-8"><a href="#Relationship_with_principal_component_analysis_(PCA)"><span class="tocnumber">2.2</span> <span class="toctext">Relationship with principal component analysis (PCA)</span></a></li>\n</ul>\n</li>\n<li class="toclevel-1 tocsection-9"><a href="#Training"><span class="tocnumber">3</span> <span class="toctext">Training</span></a></li>\n<li class="toclevel-1 tocsection-10"><a href="#See_also"><span class="tocnumber">4</span> <span class="toctext">See also</span></a></li>\n<li class="toclevel-1 tocsection-11"><a href="#References"><span class="tocnumber">5</span> <span class="toctext">References</span></a></li>\n</ul>\n</div>\n\n<h2><span class="mw-headline" id="Purpose">Purpose</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=1" title="Edit section: Purpose">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>An autoencoder learns to compress data from the input layer into a short code, and then uncompress that code into something that closely matches the original data. This forces the autoencoder to engage in dimensionality reduction, for example by learning how to ignore noise. Some architectures use stacked sparse autoencoder layers for image recognition.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (May 2019)">citation needed</span></a></i>&#93;</sup> The first encoding layer might learn to encode basic features such as corners, the second to analyze the first layer\'s output and then encode less local features like the tip of a nose, the third might encode a whole nose, etc., until the final encoding layer encodes the whole image into a code that matches (for example) the concept of "cat".<sup id="cite_ref-domingos_5-1" class="reference"><a href="#cite_note-domingos-5">&#91;5&#93;</a></sup> The decoding layers learn to decode the representation back into its original form as closely as possible. An alternative use is as a generative model: for example, if a system is manually fed the codes it has learned for "cat" and "flying", it may attempt to generate an image of a flying cat, even if it has never seen a flying cat before.<sup id="cite_ref-VAE_3-1" class="reference"><a href="#cite_note-VAE-3">&#91;3&#93;</a></sup><sup id="cite_ref-6" class="reference"><a href="#cite_note-6">&#91;6&#93;</a></sup>\n</p>\n<h2><span class="mw-headline" id="Structure">Structure</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=2" title="Edit section: Structure">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>The simplest form of an autoencoder is a <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feedforward</a>, non-<a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">recurrent neural network</a> similar to single layer perceptrons that participate in <a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">multilayer perceptrons</a> (MLP) \xe2\x80\x93 having an input layer, an output layer and one or more hidden layers connecting them \xe2\x80\x93 where the output layer has the same number of nodes (neurons) as the input layer, and with the purpose of reconstructing its inputs (minimizing the difference between the input and the output) instead of predicting the target value <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle Y}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>Y</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle Y}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/961d67d6b454b4df2301ac571808a3538b3a6d3f" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.171ex; width:1.773ex; height:2.009ex;" alt="Y"/></span> given inputs <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle X}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>X</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle X}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/68baa052181f707c662844a465bfeeb135e82bab" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.98ex; height:2.176ex;" alt="X"/></span>. Therefore, autoencoders are <a href="/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a> models (do not require labeled inputs to enable learning).\n</p><p>An autoencoder consists of two parts, the encoder and the decoder, which can be defined as transitions <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\phi }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>&#x03D5;<!-- \xcf\x95 --></mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\phi }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/72b1f30316670aee6270a28334bdf4f5072cdde4" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:1.385ex; height:2.509ex;" alt="\\phi "/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\psi ,}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>&#x03C8;<!-- \xcf\x88 --></mi>\n        <mo>,</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\psi ,}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/bfc4043b55bade492740e58cba74198873db1464" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.16ex; height:2.509ex;" alt="{\\displaystyle \\psi ,}"/></span> such that:\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\phi :{\\mathcal {X}}\\rightarrow {\\mathcal {F}}}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>&#x03D5;<!-- \xcf\x95 --></mi>\n        <mo>:</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">X</mi>\n          </mrow>\n        </mrow>\n        <mo stretchy="false">&#x2192;<!-- \xe2\x86\x92 --></mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">F</mi>\n          </mrow>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\phi :{\\mathcal {X}}\\rightarrow {\\mathcal {F}}}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/af15e24dd1ebaa221aa2be25d304f58de027135d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:10.739ex; height:2.509ex;" alt="{\\displaystyle \\phi :{\\mathcal {X}}\\rightarrow {\\mathcal {F}}}"/></span></dd>\n<dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\psi :{\\mathcal {F}}\\rightarrow {\\mathcal {X}}}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>&#x03C8;<!-- \xcf\x88 --></mi>\n        <mo>:</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">F</mi>\n          </mrow>\n        </mrow>\n        <mo stretchy="false">&#x2192;<!-- \xe2\x86\x92 --></mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">X</mi>\n          </mrow>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\psi :{\\mathcal {F}}\\rightarrow {\\mathcal {X}}}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9b3812977cdf1ef5a7c522cae067fd186ac9cddd" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:10.866ex; height:2.509ex;" alt="{\\displaystyle \\psi :{\\mathcal {F}}\\rightarrow {\\mathcal {X}}}"/></span></dd>\n<dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\phi ,\\psi ={\\underset {\\phi ,\\psi }{\\operatorname {arg\\,min} }}\\,\\|X-(\\psi \\circ \\phi )X\\|^{2}}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>&#x03D5;<!-- \xcf\x95 --></mi>\n        <mo>,</mo>\n        <mi>&#x03C8;<!-- \xcf\x88 --></mi>\n        <mo>=</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <munder>\n            <mrow class="MJX-TeXAtom-OP MJX-fixedlimits">\n              <mi mathvariant="normal">a</mi>\n              <mi mathvariant="normal">r</mi>\n              <mi mathvariant="normal">g</mi>\n              <mspace width="thinmathspace" />\n              <mi mathvariant="normal">m</mi>\n              <mi mathvariant="normal">i</mi>\n              <mi mathvariant="normal">n</mi>\n            </mrow>\n            <mrow>\n              <mi>&#x03D5;<!-- \xcf\x95 --></mi>\n              <mo>,</mo>\n              <mi>&#x03C8;<!-- \xcf\x88 --></mi>\n            </mrow>\n          </munder>\n        </mrow>\n        <mspace width="thinmathspace" />\n        <mo fence="false" stretchy="false">&#x2016;<!-- \xe2\x80\x96 --></mo>\n        <mi>X</mi>\n        <mo>&#x2212;<!-- \xe2\x88\x92 --></mo>\n        <mo stretchy="false">(</mo>\n        <mi>&#x03C8;<!-- \xcf\x88 --></mi>\n        <mo>&#x2218;<!-- \xe2\x88\x98 --></mo>\n        <mi>&#x03D5;<!-- \xcf\x95 --></mi>\n        <mo stretchy="false">)</mo>\n        <mi>X</mi>\n        <msup>\n          <mo fence="false" stretchy="false">&#x2016;<!-- \xe2\x80\x96 --></mo>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mn>2</mn>\n          </mrow>\n        </msup>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\phi ,\\psi ={\\underset {\\phi ,\\psi }{\\operatorname {arg\\,min} }}\\,\\|X-(\\psi \\circ \\phi )X\\|^{2}}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4fa7f6a4ca888943ff470cbf76fb18e64815f831" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.838ex; width:31.999ex; height:5.176ex;" alt="{\\displaystyle \\phi ,\\psi ={\\underset {\\phi ,\\psi }{\\operatorname {arg\\,min} }}\\,\\|X-(\\psi \\circ \\phi )X\\|^{2}}"/></span></dd></dl>\n<p>In the simplest case, given one hidden layer, the encoder stage of an autoencoder takes the input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {x} \\in \\mathbb {R} ^{d}={\\mathcal {X}}}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mo>&#x2208;<!-- \xe2\x88\x88 --></mo>\n        <msup>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi mathvariant="double-struck">R</mi>\n          </mrow>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>d</mi>\n          </mrow>\n        </msup>\n        <mo>=</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">X</mi>\n          </mrow>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {x} \\in \\mathbb {R} ^{d}={\\mathcal {X}}}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/70a238822b434f8555744fcb5ce2b4e028e47003" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:11.996ex; height:2.676ex;" alt="{\\displaystyle \\mathbf {x} \\in \\mathbb {R} ^{d}={\\mathcal {X}}}"/></span> and maps it to <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {z} \\in \\mathbb {R} ^{p}={\\mathcal {F}}}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mo>&#x2208;<!-- \xe2\x88\x88 --></mo>\n        <msup>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi mathvariant="double-struck">R</mi>\n          </mrow>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>p</mi>\n          </mrow>\n        </msup>\n        <mo>=</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">F</mi>\n          </mrow>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {z} \\in \\mathbb {R} ^{p}={\\mathcal {F}}}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e07154adee17220c273351dd92028e261fd675d9" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:11.791ex; height:2.343ex;" alt="{\\displaystyle \\mathbf {z} \\in \\mathbb {R} ^{p}={\\mathcal {F}}}"/></span>:\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {z} =\\sigma (\\mathbf {Wx} +\\mathbf {b} )}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mo>=</mo>\n        <mi>&#x03C3;<!-- \xcf\x83 --></mi>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">W</mi>\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mo>+</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">b</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {z} =\\sigma (\\mathbf {Wx} +\\mathbf {b} )}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ab0ddd08aac4251ffc4aeeb1f8b644f7b76273ae" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:15.925ex; height:2.843ex;" alt="{\\displaystyle \\mathbf {z} =\\sigma (\\mathbf {Wx} +\\mathbf {b} )}"/></span></dd></dl>\n<p>This image <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {z} }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {z} }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/82eca5d0928078d5a61b9e7e98cc73db31070909" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.188ex; height:1.676ex;" alt="\\mathbf {z} "/></span> is usually referred to as <i>code</i>, <i>latent variables</i>, or <i>latent representation</i>. Here, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\sigma }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>&#x03C3;<!-- \xcf\x83 --></mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\sigma }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/59f59b7c3e6fdb1d0365a494b81fb9a696138c36" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;" alt="\\sigma "/></span> is an element-wise <a href="/wiki/Activation_function" title="Activation function">activation function</a> such as a <a href="/wiki/Sigmoid_function" title="Sigmoid function">sigmoid function</a> or a <a href="/wiki/Rectified_linear_unit" class="mw-redirect" title="Rectified linear unit">rectified linear unit</a>.  <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {W} }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">W</mi>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {W} }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/04749f1e87cca59c094da23c79cc64b085b0df12" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.763ex; height:2.176ex;" alt="\\mathbf {W} "/></span> is a weight matrix and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {b} }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">b</mi>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {b} }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/13ebf4628a1adf07133a6009e4a78bdd990c6eb9" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.485ex; height:2.176ex;" alt="\\mathbf {b} "/></span> is a bias vector. After that, the decoder stage of the autoencoder maps <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {z} }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {z} }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/82eca5d0928078d5a61b9e7e98cc73db31070909" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.188ex; height:1.676ex;" alt="\\mathbf {z} "/></span> to the reconstruction <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {x\'} }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <msup>\n            <mi mathvariant="bold">x</mi>\n            <mo>&#x2032;</mo>\n          </msup>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {x\'} }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7d14ab6186e99346cb608a30858c3e1580f760e6" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.096ex; height:2.509ex;" alt="\\mathbf {x&#039;} "/></span> of the same shape as <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {x} }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {x} }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/32adf004df5eb0a8c7fd8c0b6b7405183c5a5ef2" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.411ex; height:1.676ex;" alt="\\mathbf {x} "/></span>:\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {x\'} =\\sigma \'(\\mathbf {W\'z} +\\mathbf {b\'} )}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <msup>\n            <mi mathvariant="bold">x</mi>\n            <mo>&#x2032;</mo>\n          </msup>\n        </mrow>\n        <mo>=</mo>\n        <msup>\n          <mi>&#x03C3;<!-- \xcf\x83 --></mi>\n          <mo>&#x2032;</mo>\n        </msup>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <msup>\n            <mi mathvariant="bold">W</mi>\n            <mo>&#x2032;</mo>\n          </msup>\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mo>+</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <msup>\n            <mi mathvariant="bold">b</mi>\n            <mo>&#x2032;</mo>\n          </msup>\n        </mrow>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {x\'} =\\sigma \'(\\mathbf {W\'z} +\\mathbf {b\'} )}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a858da67f83488256a4ee2a32f1bddda7399cd8f" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:18.665ex; height:3.009ex;" alt="{\\displaystyle \\mathbf {x&#039;} =\\sigma &#039;(\\mathbf {W&#039;z} +\\mathbf {b&#039;} )}"/></span></dd></dl>\n<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {\\sigma \'} ,\\mathbf {W\'} ,{\\text{ and }}\\mathbf {b\'} }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <msup>\n            <mi>&#x03C3;<!-- \xcf\x83 --></mi>\n            <mo>&#x2032;</mo>\n          </msup>\n        </mrow>\n        <mo>,</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <msup>\n            <mi mathvariant="bold">W</mi>\n            <mo>&#x2032;</mo>\n          </msup>\n        </mrow>\n        <mo>,</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mtext>&#xA0;and&#xA0;</mtext>\n        </mrow>\n        <mrow class="MJX-TeXAtom-ORD">\n          <msup>\n            <mi mathvariant="bold">b</mi>\n            <mo>&#x2032;</mo>\n          </msup>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {\\sigma \'} ,\\mathbf {W\'} ,{\\text{ and }}\\mathbf {b\'} }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/062a0023b06f960a4c9d005bbe283818d5946d33" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:14.609ex; height:2.843ex;" alt="{\\displaystyle \\mathbf {\\sigma &#039;} ,\\mathbf {W&#039;} ,{\\text{ and }}\\mathbf {b&#039;} }"/></span> for the decoder may be unrelated to the corresponding <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {\\sigma } ,\\mathbf {W} ,{\\text{ and }}\\mathbf {b} }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi>&#x03C3;<!-- \xcf\x83 --></mi>\n        </mrow>\n        <mo>,</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">W</mi>\n        </mrow>\n        <mo>,</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mtext>&#xA0;and&#xA0;</mtext>\n        </mrow>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">b</mi>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {\\sigma } ,\\mathbf {W} ,{\\text{ and }}\\mathbf {b} }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2c5d41529f76819f274a8466c0fbfdb937649f31" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:12.554ex; height:2.509ex;" alt="{\\displaystyle \\mathbf {\\sigma } ,\\mathbf {W} ,{\\text{ and }}\\mathbf {b} }"/></span> for the encoder.\n</p><p>Autoencoders are trained to minimise reconstruction errors (such as <a href="/wiki/Mean_squared_error" title="Mean squared error">squared errors</a>), often referred to as the "<a href="/wiki/Loss_function" title="Loss function">loss</a>":\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle {\\mathcal {L}}(\\mathbf {x} ,\\mathbf {x\'} )=\\|\\mathbf {x} -\\mathbf {x\'} \\|^{2}=\\|\\mathbf {x} -\\sigma \'(\\mathbf {W\'} (\\sigma (\\mathbf {Wx} +\\mathbf {b} ))+\\mathbf {b\'} )\\|^{2}}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">L</mi>\n          </mrow>\n        </mrow>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mo>,</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <msup>\n            <mi mathvariant="bold">x</mi>\n            <mo>&#x2032;</mo>\n          </msup>\n        </mrow>\n        <mo stretchy="false">)</mo>\n        <mo>=</mo>\n        <mo fence="false" stretchy="false">&#x2016;<!-- \xe2\x80\x96 --></mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mo>&#x2212;<!-- \xe2\x88\x92 --></mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <msup>\n            <mi mathvariant="bold">x</mi>\n            <mo>&#x2032;</mo>\n          </msup>\n        </mrow>\n        <msup>\n          <mo fence="false" stretchy="false">&#x2016;<!-- \xe2\x80\x96 --></mo>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mn>2</mn>\n          </mrow>\n        </msup>\n        <mo>=</mo>\n        <mo fence="false" stretchy="false">&#x2016;<!-- \xe2\x80\x96 --></mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mo>&#x2212;<!-- \xe2\x88\x92 --></mo>\n        <msup>\n          <mi>&#x03C3;<!-- \xcf\x83 --></mi>\n          <mo>&#x2032;</mo>\n        </msup>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <msup>\n            <mi mathvariant="bold">W</mi>\n            <mo>&#x2032;</mo>\n          </msup>\n        </mrow>\n        <mo stretchy="false">(</mo>\n        <mi>&#x03C3;<!-- \xcf\x83 --></mi>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">W</mi>\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mo>+</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">b</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n        <mo stretchy="false">)</mo>\n        <mo>+</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <msup>\n            <mi mathvariant="bold">b</mi>\n            <mo>&#x2032;</mo>\n          </msup>\n        </mrow>\n        <mo stretchy="false">)</mo>\n        <msup>\n          <mo fence="false" stretchy="false">&#x2016;<!-- \xe2\x80\x96 --></mo>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mn>2</mn>\n          </mrow>\n        </msup>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle {\\mathcal {L}}(\\mathbf {x} ,\\mathbf {x\'} )=\\|\\mathbf {x} -\\mathbf {x\'} \\|^{2}=\\|\\mathbf {x} -\\sigma \'(\\mathbf {W\'} (\\sigma (\\mathbf {Wx} +\\mathbf {b} ))+\\mathbf {b\'} )\\|^{2}}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2c9d074db8e3d0e19f9f5128edcc26a2e7baad36" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:57.237ex; height:3.176ex;" alt="{\\displaystyle {\\mathcal {L}}(\\mathbf {x} ,\\mathbf {x&#039;} )=\\|\\mathbf {x} -\\mathbf {x&#039;} \\|^{2}=\\|\\mathbf {x} -\\sigma &#039;(\\mathbf {W&#039;} (\\sigma (\\mathbf {Wx} +\\mathbf {b} ))+\\mathbf {b&#039;} )\\|^{2}}"/></span></dd></dl>\n<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {x} }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {x} }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/32adf004df5eb0a8c7fd8c0b6b7405183c5a5ef2" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.411ex; height:1.676ex;" alt="\\mathbf {x} "/></span> is usually averaged over some input training set.\n</p><p>Should the <a href="/wiki/Feature_(machine_learning)" title="Feature (machine learning)">feature space</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle {\\mathcal {F}}}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">F</mi>\n          </mrow>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle {\\mathcal {F}}}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/205d4b91000d9dcf1a5bbabdfa6a8395fa60b676" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.927ex; height:2.176ex;" alt="{\\mathcal {F}}"/></span> have lower dimensionality than the input space <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle {\\mathcal {X}}}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">X</mi>\n          </mrow>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle {\\mathcal {X}}}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8c7e5461c5286852df4ef652fca7e4b0b63030e9" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.875ex; height:2.176ex;" alt="{\\mathcal {X}}"/></span>, the feature vector <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\phi (x)}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>&#x03D5;<!-- \xcf\x95 --></mi>\n        <mo stretchy="false">(</mo>\n        <mi>x</mi>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\phi (x)}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/546b660b2f3cfb5f34be7b3ed8371d54f5c74227" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:4.524ex; height:2.843ex;" alt="\\phi (x)"/></span> can be regarded as a <a href="/wiki/Data_compression" title="Data compression">compressed</a> representation of the input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle x}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>x</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle x}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/87f9e315fd7e2ba406057a97300593c4802b53e4" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.33ex; height:1.676ex;" alt="x"/></span>. If the hidden layers are larger than the input layer, an autoencoder can potentially learn the <a href="/wiki/Identity_function" title="Identity function">identity function</a> and become useless. However, experimental results have shown that autoencoders might still <a href="/wiki/Feature_learning" title="Feature learning">learn useful features</a> in these cases.<sup id="cite_ref-bengio_7-0" class="reference"><a href="#cite_note-bengio-7">&#91;7&#93;</a></sup><sup class="reference" style="white-space:nowrap;">:<span>19</span></sup>\n</p>\n<h3><span class="mw-headline" id="Variations">Variations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=3" title="Edit section: Variations">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>Various techniques exist to prevent autoencoders from learning the identity function and to improve their ability to capture important information and learn richer representations:\n</p>\n<h4><span class="mw-headline" id="Denoising_autoencoder">Denoising autoencoder</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=4" title="Edit section: Denoising autoencoder">edit</a><span class="mw-editsection-bracket">]</span></span></h4>\n<p>Denoising autoencoders take a partially corrupted input whilst training to recover the original undistorted input. This technique was introduced to achieve <i>good</i> representation.<sup id="cite_ref-ref9_8-0" class="reference"><a href="#cite_note-ref9-8">&#91;8&#93;</a></sup> A <i>good</i> representation is one that can be obtained <a href="/wiki/Robustness_(computer_science)" title="Robustness (computer science)">robustly</a> from a corrupted input and that will be useful for recovering the corresponding clean input. This definition contains the following assumptions:\n</p>\n<ul><li>Higher level representations are relatively stable and robust to the corruption of the input;</li>\n<li>It is necessary to extract features that are useful for representation of the input distribution.</li></ul>\n<p>To train an autoencoder to denoise data, it is necessary to perform preliminary stochastic mapping <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {x} \\rightarrow \\mathbf {\\tilde {x}} }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mo stretchy="false">&#x2192;<!-- \xe2\x86\x92 --></mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mover>\n              <mi mathvariant="bold">x</mi>\n              <mo mathvariant="bold" stretchy="false">&#x007E;<!-- ~ --></mo>\n            </mover>\n          </mrow>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {x} \\rightarrow \\mathbf {\\tilde {x}} }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7f657aeff940a37a876e92381648a1e1cd08c865" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:6.436ex; height:2.176ex;" alt="{\\displaystyle \\mathbf {x} \\rightarrow \\mathbf {\\tilde {x}} }"/></span> in order to corrupt the data and use <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {\\tilde {x}} }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mover>\n              <mi mathvariant="bold">x</mi>\n              <mo mathvariant="bold" stretchy="false">&#x007E;<!-- ~ --></mo>\n            </mover>\n          </mrow>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {\\tilde {x}} }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7ef203353d67a8d8db4b7cf01bf0a30beb24df43" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.411ex; height:2.176ex;" alt="{\\displaystyle \\mathbf {\\tilde {x}} }"/></span> as input, with only the exception that the loss is computed for the initial input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle {\\mathcal {L}}(\\mathbf {x} ,\\mathbf {{\\tilde {x}}\'} )}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">L</mi>\n          </mrow>\n        </mrow>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mo>,</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <msup>\n            <mrow class="MJX-TeXAtom-ORD">\n              <mrow class="MJX-TeXAtom-ORD">\n                <mover>\n                  <mi mathvariant="bold">x</mi>\n                  <mo mathvariant="bold" stretchy="false">&#x007E;<!-- ~ --></mo>\n                </mover>\n              </mrow>\n            </mrow>\n            <mo>&#x2032;</mo>\n          </msup>\n        </mrow>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle {\\mathcal {L}}(\\mathbf {x} ,\\mathbf {{\\tilde {x}}\'} )}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cdb521b4e87e860db8255da0672e3fb1b2bdba96" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:7.954ex; height:3.009ex;" alt="{\\displaystyle {\\mathcal {L}}(\\mathbf {x} ,\\mathbf {{\\tilde {x}}&#039;} )}"/></span> instead of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle {\\mathcal {L}}(\\mathbf {\\tilde {x}} ,\\mathbf {{\\tilde {x}}\'} )}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">L</mi>\n          </mrow>\n        </mrow>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mover>\n              <mi mathvariant="bold">x</mi>\n              <mo mathvariant="bold" stretchy="false">&#x007E;<!-- ~ --></mo>\n            </mover>\n          </mrow>\n        </mrow>\n        <mo>,</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <msup>\n            <mrow class="MJX-TeXAtom-ORD">\n              <mrow class="MJX-TeXAtom-ORD">\n                <mover>\n                  <mi mathvariant="bold">x</mi>\n                  <mo mathvariant="bold" stretchy="false">&#x007E;<!-- ~ --></mo>\n                </mover>\n              </mrow>\n            </mrow>\n            <mo>&#x2032;</mo>\n          </msup>\n        </mrow>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle {\\mathcal {L}}(\\mathbf {\\tilde {x}} ,\\mathbf {{\\tilde {x}}\'} )}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7972550fbca370355103dd5321a23c27105a2b0f" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:7.954ex; height:3.009ex;" alt="{\\displaystyle {\\mathcal {L}}(\\mathbf {\\tilde {x}} ,\\mathbf {{\\tilde {x}}&#039;} )}"/></span>.\n</p>\n<h4><span class="mw-headline" id="Sparse_autoencoder">Sparse autoencoder</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=5" title="Edit section: Sparse autoencoder">edit</a><span class="mw-editsection-bracket">]</span></span></h4>\n<p>Autoencoders were invented in the 1980s; however, the initial versions were difficult to train, as the encodings had to compete to set the same small set of bits. This was solved by "sparse autoencoding". Sparse autoencoder include more (rather than fewer) hidden units than inputs, but only a small number of the hidden units are allowed to be active at once.<sup id="cite_ref-domingos_5-2" class="reference"><a href="#cite_note-domingos-5">&#91;5&#93;</a></sup>\n</p><p>Sparsity may be achieved by additional terms in the <a href="/wiki/Loss_function" title="Loss function">loss function</a> during training (by <a href="/wiki/Kullback%E2%80%93Leibler_divergence" title="Kullback\xe2\x80\x93Leibler divergence">comparing the probability distribution</a> of the hidden unit activations with some low desired value),<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">&#91;9&#93;</a></sup> or by manually zeroing all but the strongest hidden unit activations (<i>k-sparse autoencoder</i>).<sup id="cite_ref-10" class="reference"><a href="#cite_note-10">&#91;10&#93;</a></sup>\n</p>\n<h4><span id="Variational_autoencoder_.28VAE.29"></span><span class="mw-headline" id="Variational_autoencoder_(VAE)">Variational autoencoder (VAE)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=6" title="Edit section: Variational autoencoder (VAE)">edit</a><span class="mw-editsection-bracket">]</span></span></h4>\n<p>Variational autoencoder models make strong assumptions concerning the distribution of <i>latent variables</i>. They use a <a href="/wiki/Variational_Bayesian_methods" title="Variational Bayesian methods">variational approach</a> for latent representation learning, which results in an additional loss component and a specific estimator for the training algorithm called the Stochastic Gradient Variational Bayes (SGVB) estimator.<sup id="cite_ref-VAE_3-2" class="reference"><a href="#cite_note-VAE-3">&#91;3&#93;</a></sup> It assumes that the data is generated by a directed <a href="/wiki/Graphical_model" title="Graphical model">graphical model</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle p_{\\theta }(\\mathbf {x} |\\mathbf {z} )}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <msub>\n          <mi>p</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>&#x03B8;<!-- \xce\xb8 --></mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mo stretchy="false">|</mo>\n        </mrow>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle p_{\\theta }(\\mathbf {x} |\\mathbf {z} )}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5f7175baedaa1415342021639cd8b1a70ca09ecf" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:7.317ex; height:2.843ex;" alt="{\\displaystyle p_{\\theta }(\\mathbf {x} |\\mathbf {z} )}"/></span> and that the encoder is learning an approximation <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle q_{\\phi }(\\mathbf {z} |\\mathbf {x} )}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <msub>\n          <mi>q</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>&#x03D5;<!-- \xcf\x95 --></mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mo stretchy="false">|</mo>\n        </mrow>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle q_{\\phi }(\\mathbf {z} |\\mathbf {x} )}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/16232d03fe3d45ae6876e5141bcdde6f9c70d90d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:7.304ex; height:3.009ex;" alt="{\\displaystyle q_{\\phi }(\\mathbf {z} |\\mathbf {x} )}"/></span> to the <a href="/wiki/Posterior_probability" title="Posterior probability">posterior distribution</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle p_{\\theta }(\\mathbf {z} |\\mathbf {x} )}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <msub>\n          <mi>p</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>&#x03B8;<!-- \xce\xb8 --></mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mo stretchy="false">|</mo>\n        </mrow>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle p_{\\theta }(\\mathbf {z} |\\mathbf {x} )}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/072e6cf173d6af987805ac112a7c830f47707cb2" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:7.317ex; height:2.843ex;" alt="{\\displaystyle p_{\\theta }(\\mathbf {z} |\\mathbf {x} )}"/></span> where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {\\phi } }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi>&#x03D5;<!-- \xcf\x95 --></mi>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {\\phi } }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/03b16c8b97a15009afffb576b1424e184f48f6b3" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:1.385ex; height:2.509ex;" alt="{\\mathbf  {\\phi }}"/></span> and <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {\\theta } }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi>&#x03B8;<!-- \xce\xb8 --></mi>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {\\theta } }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9813b67e5416572ea1d10056c50d380742924a78" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.09ex; height:2.176ex;" alt="\\mathbf {\\theta } "/></span> denote the parameters of the encoder (recognition model) and decoder (generative model) respectively. The probability distribution of the latent vector of a VAE typically matches that of the training data much closer than a standard autoencoder. The objective of VAE has the following form:\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle {\\mathcal {L}}(\\mathbf {\\phi } ,\\mathbf {\\theta } ,\\mathbf {x} )=D_{\\mathrm {KL} }(q_{\\phi }(\\mathbf {z} |\\mathbf {x} )\\Vert p_{\\theta }(\\mathbf {z} ))-\\mathbb {E} _{q_{\\phi }(\\mathbf {z} |\\mathbf {x} )}{\\big (}\\log p_{\\theta }(\\mathbf {x} |\\mathbf {z} ){\\big )}}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">L</mi>\n          </mrow>\n        </mrow>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi>&#x03D5;<!-- \xcf\x95 --></mi>\n        </mrow>\n        <mo>,</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi>&#x03B8;<!-- \xce\xb8 --></mi>\n        </mrow>\n        <mo>,</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n        <mo>=</mo>\n        <msub>\n          <mi>D</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mrow class="MJX-TeXAtom-ORD">\n              <mi mathvariant="normal">K</mi>\n              <mi mathvariant="normal">L</mi>\n            </mrow>\n          </mrow>\n        </msub>\n        <mo stretchy="false">(</mo>\n        <msub>\n          <mi>q</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>&#x03D5;<!-- \xcf\x95 --></mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mo stretchy="false">|</mo>\n        </mrow>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n        <mo fence="false" stretchy="false">&#x2016;<!-- \xe2\x80\x96 --></mo>\n        <msub>\n          <mi>p</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>&#x03B8;<!-- \xce\xb8 --></mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n        <mo stretchy="false">)</mo>\n        <mo>&#x2212;<!-- \xe2\x88\x92 --></mo>\n        <msub>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi mathvariant="double-struck">E</mi>\n          </mrow>\n          <mrow class="MJX-TeXAtom-ORD">\n            <msub>\n              <mi>q</mi>\n              <mrow class="MJX-TeXAtom-ORD">\n                <mi>&#x03D5;<!-- \xcf\x95 --></mi>\n              </mrow>\n            </msub>\n            <mo stretchy="false">(</mo>\n            <mrow class="MJX-TeXAtom-ORD">\n              <mi mathvariant="bold">z</mi>\n            </mrow>\n            <mrow class="MJX-TeXAtom-ORD">\n              <mo stretchy="false">|</mo>\n            </mrow>\n            <mrow class="MJX-TeXAtom-ORD">\n              <mi mathvariant="bold">x</mi>\n            </mrow>\n            <mo stretchy="false">)</mo>\n          </mrow>\n        </msub>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mo maxsize="1.2em" minsize="1.2em">(</mo>\n          </mrow>\n        </mrow>\n        <mi>log</mi>\n        <mo>&#x2061;<!-- \xe2\x81\xa1 --></mo>\n        <msub>\n          <mi>p</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>&#x03B8;<!-- \xce\xb8 --></mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mo stretchy="false">|</mo>\n        </mrow>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mo maxsize="1.2em" minsize="1.2em">)</mo>\n          </mrow>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle {\\mathcal {L}}(\\mathbf {\\phi } ,\\mathbf {\\theta } ,\\mathbf {x} )=D_{\\mathrm {KL} }(q_{\\phi }(\\mathbf {z} |\\mathbf {x} )\\Vert p_{\\theta }(\\mathbf {z} ))-\\mathbb {E} _{q_{\\phi }(\\mathbf {z} |\\mathbf {x} )}{\\big (}\\log p_{\\theta }(\\mathbf {x} |\\mathbf {z} ){\\big )}}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fbdab7aee7c3fef4d142e829b83503ab943d1ddc" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.338ex; width:55.368ex; height:3.509ex;" alt="{\\displaystyle {\\mathcal {L}}(\\mathbf {\\phi } ,\\mathbf {\\theta } ,\\mathbf {x} )=D_{\\mathrm {KL} }(q_{\\phi }(\\mathbf {z} |\\mathbf {x} )\\Vert p_{\\theta }(\\mathbf {z} ))-\\mathbb {E} _{q_{\\phi }(\\mathbf {z} |\\mathbf {x} )}{\\big (}\\log p_{\\theta }(\\mathbf {x} |\\mathbf {z} ){\\big )}}"/></span></dd></dl>\n<p>Here, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle D_{\\mathrm {KL} }}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <msub>\n          <mi>D</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mrow class="MJX-TeXAtom-ORD">\n              <mi mathvariant="normal">K</mi>\n              <mi mathvariant="normal">L</mi>\n            </mrow>\n          </mrow>\n        </msub>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle D_{\\mathrm {KL} }}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2ab26e8799fe9a2b74e4c49da80bb18eaff04da0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:4.462ex; height:2.509ex;" alt="D_{{{\\mathrm  {KL}}}}"/></span> stands for the <a href="/wiki/Kullback%E2%80%93Leibler_divergence" title="Kullback\xe2\x80\x93Leibler divergence">Kullback\xe2\x80\x93Leibler divergence</a>. The prior over the latent variables is usually set to be the centred isotropic multivariate <a href="/wiki/Gaussian_function" title="Gaussian function">Gaussian</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle p_{\\theta }(\\mathbf {z} )={\\mathcal {N}}(\\mathbf {0,I} )}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <msub>\n          <mi>p</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>&#x03B8;<!-- \xce\xb8 --></mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n        <mo>=</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>\n          </mrow>\n        </mrow>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mn mathvariant="bold">0</mn>\n          <mo mathvariant="bold">,</mo>\n          <mi mathvariant="bold">I</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle p_{\\theta }(\\mathbf {z} )={\\mathcal {N}}(\\mathbf {0,I} )}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/11c420df48dea7eefc909e417a3d8309f3d25aeb" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:15.922ex; height:3.009ex;" alt="{\\displaystyle p_{\\theta }(\\mathbf {z} )={\\mathcal {N}}(\\mathbf {0,I} )}"/></span>; however, alternative configurations have been considered.<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">&#91;11&#93;</a></sup>\n</p><p>Commonly, the shape of the variational and the likelihood distributions are chosen such that they are factorized Gaussians:\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle {\\begin{aligned}q_{\\phi }(\\mathbf {z} |\\mathbf {x} )&amp;={\\mathcal {N}}({\\boldsymbol {\\rho }}(\\mathbf {x} ),{\\boldsymbol {\\omega }}^{2}(\\mathbf {x} )\\mathbf {I} ),\\\\p_{\\theta }(\\mathbf {x} |\\mathbf {z} )&amp;={\\mathcal {N}}({\\boldsymbol {\\mu }}(\\mathbf {z} ),{\\boldsymbol {\\sigma }}^{2}(\\mathbf {z} )\\mathbf {I} )),\\end{aligned}}}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true">\n            <mtr>\n              <mtd>\n                <msub>\n                  <mi>q</mi>\n                  <mrow class="MJX-TeXAtom-ORD">\n                    <mi>&#x03D5;<!-- \xcf\x95 --></mi>\n                  </mrow>\n                </msub>\n                <mo stretchy="false">(</mo>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi mathvariant="bold">z</mi>\n                </mrow>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mo stretchy="false">|</mo>\n                </mrow>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi mathvariant="bold">x</mi>\n                </mrow>\n                <mo stretchy="false">)</mo>\n              </mtd>\n              <mtd>\n                <mi></mi>\n                <mo>=</mo>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mrow class="MJX-TeXAtom-ORD">\n                    <mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>\n                  </mrow>\n                </mrow>\n                <mo stretchy="false">(</mo>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi mathvariant="bold-italic">&#x03C1;<!-- \xcf\x81 --></mi>\n                </mrow>\n                <mo stretchy="false">(</mo>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi mathvariant="bold">x</mi>\n                </mrow>\n                <mo stretchy="false">)</mo>\n                <mo>,</mo>\n                <msup>\n                  <mrow class="MJX-TeXAtom-ORD">\n                    <mi mathvariant="bold-italic">&#x03C9;<!-- \xcf\x89 --></mi>\n                  </mrow>\n                  <mrow class="MJX-TeXAtom-ORD">\n                    <mn>2</mn>\n                  </mrow>\n                </msup>\n                <mo stretchy="false">(</mo>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi mathvariant="bold">x</mi>\n                </mrow>\n                <mo stretchy="false">)</mo>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi mathvariant="bold">I</mi>\n                </mrow>\n                <mo stretchy="false">)</mo>\n                <mo>,</mo>\n              </mtd>\n            </mtr>\n            <mtr>\n              <mtd>\n                <msub>\n                  <mi>p</mi>\n                  <mrow class="MJX-TeXAtom-ORD">\n                    <mi>&#x03B8;<!-- \xce\xb8 --></mi>\n                  </mrow>\n                </msub>\n                <mo stretchy="false">(</mo>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi mathvariant="bold">x</mi>\n                </mrow>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mo stretchy="false">|</mo>\n                </mrow>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi mathvariant="bold">z</mi>\n                </mrow>\n                <mo stretchy="false">)</mo>\n              </mtd>\n              <mtd>\n                <mi></mi>\n                <mo>=</mo>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mrow class="MJX-TeXAtom-ORD">\n                    <mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>\n                  </mrow>\n                </mrow>\n                <mo stretchy="false">(</mo>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi mathvariant="bold-italic">&#x03BC;<!-- \xce\xbc --></mi>\n                </mrow>\n                <mo stretchy="false">(</mo>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi mathvariant="bold">z</mi>\n                </mrow>\n                <mo stretchy="false">)</mo>\n                <mo>,</mo>\n                <msup>\n                  <mrow class="MJX-TeXAtom-ORD">\n                    <mi mathvariant="bold-italic">&#x03C3;<!-- \xcf\x83 --></mi>\n                  </mrow>\n                  <mrow class="MJX-TeXAtom-ORD">\n                    <mn>2</mn>\n                  </mrow>\n                </msup>\n                <mo stretchy="false">(</mo>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi mathvariant="bold">z</mi>\n                </mrow>\n                <mo stretchy="false">)</mo>\n                <mrow class="MJX-TeXAtom-ORD">\n                  <mi mathvariant="bold">I</mi>\n                </mrow>\n                <mo stretchy="false">)</mo>\n                <mo stretchy="false">)</mo>\n                <mo>,</mo>\n              </mtd>\n            </mtr>\n          </mtable>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle {\\begin{aligned}q_{\\phi }(\\mathbf {z} |\\mathbf {x} )&amp;={\\mathcal {N}}({\\boldsymbol {\\rho }}(\\mathbf {x} ),{\\boldsymbol {\\omega }}^{2}(\\mathbf {x} )\\mathbf {I} ),\\\\p_{\\theta }(\\mathbf {x} |\\mathbf {z} )&amp;={\\mathcal {N}}({\\boldsymbol {\\mu }}(\\mathbf {z} ),{\\boldsymbol {\\sigma }}^{2}(\\mathbf {z} )\\mathbf {I} )),\\end{aligned}}}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/04b6030acea844d7113f70823e8c59f432d8c899" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.671ex; width:29.126ex; height:6.509ex;" alt="{\\displaystyle {\\begin{aligned}q_{\\phi }(\\mathbf {z} |\\mathbf {x} )&amp;={\\mathcal {N}}({\\boldsymbol {\\rho }}(\\mathbf {x} ),{\\boldsymbol {\\omega }}^{2}(\\mathbf {x} )\\mathbf {I} ),\\\\p_{\\theta }(\\mathbf {x} |\\mathbf {z} )&amp;={\\mathcal {N}}({\\boldsymbol {\\mu }}(\\mathbf {z} ),{\\boldsymbol {\\sigma }}^{2}(\\mathbf {z} )\\mathbf {I} )),\\end{aligned}}}"/></span></dd></dl>\n<p>where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle {\\boldsymbol {\\rho }}(\\mathbf {x} )}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold-italic">&#x03C1;<!-- \xcf\x81 --></mi>\n        </mrow>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle {\\boldsymbol {\\rho }}(\\mathbf {x} )}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/429e0e5f67d2fe1ca9455c15952d680ae686d597" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:4.643ex; height:2.843ex;" alt="{\\displaystyle {\\boldsymbol {\\rho }}(\\mathbf {x} )}"/></span> and  <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle {\\boldsymbol {\\omega }}^{2}(\\mathbf {x} )}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <msup>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi mathvariant="bold-italic">&#x03C9;<!-- \xcf\x89 --></mi>\n          </mrow>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mn>2</mn>\n          </mrow>\n        </msup>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle {\\boldsymbol {\\omega }}^{2}(\\mathbf {x} )}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8765f05e7f021c6f2805faa360e534aabc6128da" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:5.943ex; height:3.176ex;" alt="{\\displaystyle {\\boldsymbol {\\omega }}^{2}(\\mathbf {x} )}"/></span> are the encoder outputs, while <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle {\\boldsymbol {\\mu }}(\\mathbf {z} )}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold-italic">&#x03BC;<!-- \xce\xbc --></mi>\n        </mrow>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle {\\boldsymbol {\\mu }}(\\mathbf {z} )}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a72505ecfa73b2ff5dbd1310674b5d0c87fd12f1" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:4.643ex; height:2.843ex;" alt="{\\displaystyle {\\boldsymbol {\\mu }}(\\mathbf {z} )}"/></span> and  <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle {\\boldsymbol {\\sigma }}^{2}(\\mathbf {z} )}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <msup>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi mathvariant="bold-italic">&#x03C3;<!-- \xcf\x83 --></mi>\n          </mrow>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mn>2</mn>\n          </mrow>\n        </msup>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle {\\boldsymbol {\\sigma }}^{2}(\\mathbf {z} )}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a1456217108bb94f654f572571bc7a7a79d6d8bd" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:5.646ex; height:3.176ex;" alt="{\\displaystyle {\\boldsymbol {\\sigma }}^{2}(\\mathbf {z} )}"/></span> are the decoder outputs.\nThis choice is justified by the simplifications<sup id="cite_ref-VAE_3-3" class="reference"><a href="#cite_note-VAE-3">&#91;3&#93;</a></sup> that it produces when evaluating both the KL divergence and the likelihood term in variational objective defined above.\n</p><p>VAE have been criticized because they generate blurry images.<sup id="cite_ref-12" class="reference"><a href="#cite_note-12">&#91;12&#93;</a></sup> However, researchers employing this model were showing only the mean of the distributions, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle {\\boldsymbol {\\mu }}(\\mathbf {z} )}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold-italic">&#x03BC;<!-- \xce\xbc --></mi>\n        </mrow>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle {\\boldsymbol {\\mu }}(\\mathbf {z} )}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a72505ecfa73b2ff5dbd1310674b5d0c87fd12f1" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:4.643ex; height:2.843ex;" alt="{\\displaystyle {\\boldsymbol {\\mu }}(\\mathbf {z} )}"/></span>, rather than a sample of the learned Gaussian distribution\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {x} \\sim {\\mathcal {N}}({\\boldsymbol {\\mu }}(\\mathbf {z} ),{\\boldsymbol {\\sigma }}^{2}(\\mathbf {z} )\\mathbf {I} ))}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mo>&#x223C;<!-- \xe2\x88\xbc --></mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>\n          </mrow>\n        </mrow>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold-italic">&#x03BC;<!-- \xce\xbc --></mi>\n        </mrow>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n        <mo>,</mo>\n        <msup>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi mathvariant="bold-italic">&#x03C3;<!-- \xcf\x83 --></mi>\n          </mrow>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mn>2</mn>\n          </mrow>\n        </msup>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">I</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {x} \\sim {\\mathcal {N}}({\\boldsymbol {\\mu }}(\\mathbf {z} ),{\\boldsymbol {\\sigma }}^{2}(\\mathbf {z} )\\mathbf {I} ))}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/61b90824220c25f549b5125f9f518d0290cb86c7" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:21.835ex; height:3.176ex;" alt="{\\displaystyle \\mathbf {x} \\sim {\\mathcal {N}}({\\boldsymbol {\\mu }}(\\mathbf {z} ),{\\boldsymbol {\\sigma }}^{2}(\\mathbf {z} )\\mathbf {I} ))}"/></span>.</dd></dl>\n<p>These samples were shown to be overly noisy due to the choice of a factorized Gaussian distribution.<sup id="cite_ref-SigmaVAE1_13-0" class="reference"><a href="#cite_note-SigmaVAE1-13">&#91;13&#93;</a></sup><sup id="cite_ref-SigmaVAE2_14-0" class="reference"><a href="#cite_note-SigmaVAE2-14">&#91;14&#93;</a></sup> Employing a Gaussian distribution with a full covariance matrix,\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle p_{\\theta }(\\mathbf {x} |\\mathbf {z} )={\\mathcal {N}}({\\boldsymbol {\\mu }}(\\mathbf {z} ),{\\boldsymbol {\\Sigma }}(\\mathbf {z} ))),}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <msub>\n          <mi>p</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>&#x03B8;<!-- \xce\xb8 --></mi>\n          </mrow>\n        </msub>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mo stretchy="false">|</mo>\n        </mrow>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n        <mo>=</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">N</mi>\n          </mrow>\n        </mrow>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold-italic">&#x03BC;<!-- \xce\xbc --></mi>\n        </mrow>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n        <mo>,</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">&#x03A3;<!-- \xce\xa3 --></mi>\n        </mrow>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n        <mo stretchy="false">)</mo>\n        <mo stretchy="false">)</mo>\n        <mo>,</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle p_{\\theta }(\\mathbf {x} |\\mathbf {z} )={\\mathcal {N}}({\\boldsymbol {\\mu }}(\\mathbf {z} ),{\\boldsymbol {\\Sigma }}(\\mathbf {z} ))),}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cfb261c593775c4bb3da5246ef2ebb4a742106ed" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; margin-left: -0.089ex; width:26.657ex; height:3.009ex;" alt="{\\displaystyle p_{\\theta }(\\mathbf {x} |\\mathbf {z} )={\\mathcal {N}}({\\boldsymbol {\\mu }}(\\mathbf {z} ),{\\boldsymbol {\\Sigma }}(\\mathbf {z} ))),}"/></span></dd></dl>\n<p>could solve this issue, but is computationally intractable and numerically unstable, as it requires estimating a covariance matrix from a single data sample. However, later research<sup id="cite_ref-SigmaVAE1_13-1" class="reference"><a href="#cite_note-SigmaVAE1-13">&#91;13&#93;</a></sup><sup id="cite_ref-SigmaVAE2_14-1" class="reference"><a href="#cite_note-SigmaVAE2-14">&#91;14&#93;</a></sup> showed that a restricted approach where the inverse matrix <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle {\\boldsymbol {\\Sigma }}^{-1}(\\mathbf {z} )}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <msup>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi mathvariant="bold">&#x03A3;<!-- \xce\xa3 --></mi>\n          </mrow>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mo>&#x2212;<!-- \xe2\x88\x92 --></mo>\n            <mn>1</mn>\n          </mrow>\n        </msup>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">z</mi>\n        </mrow>\n        <mo stretchy="false">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle {\\boldsymbol {\\Sigma }}^{-1}(\\mathbf {z} )}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/913cc2cda5de0269aac27b58e543df80ca8aef60" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:7.261ex; height:3.176ex;" alt="{\\displaystyle {\\boldsymbol {\\Sigma }}^{-1}(\\mathbf {z} )}"/></span> is sparse, could be tractably employed to generate images with high-frequency details.\n</p>\n<h4><span id="Contractive_autoencoder_.28CAE.29"></span><span class="mw-headline" id="Contractive_autoencoder_(CAE)">Contractive autoencoder (CAE)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=7" title="Edit section: Contractive autoencoder (CAE)">edit</a><span class="mw-editsection-bracket">]</span></span></h4>\n<p>Contractive autoencoder adds an explicit regularizer in their objective function that forces the model to learn a function that is robust to slight variations of input values. This regularizer corresponds to the <a href="/wiki/Frobenius_norm" class="mw-redirect" title="Frobenius norm">Frobenius norm</a> of the <a href="/wiki/Jacobian_matrix_and_determinant" title="Jacobian matrix and determinant">Jacobian matrix</a> of the encoder activations with respect to the input. The final objective function has the following form:\n</p>\n<dl><dd><span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle {\\mathcal {L}}(\\mathbf {x} ,\\mathbf {x\'} )+\\lambda \\sum _{i}||\\nabla _{x}h_{i}||^{2}}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi class="MJX-tex-caligraphic" mathvariant="script">L</mi>\n          </mrow>\n        </mrow>\n        <mo stretchy="false">(</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n        <mo>,</mo>\n        <mrow class="MJX-TeXAtom-ORD">\n          <msup>\n            <mi mathvariant="bold">x</mi>\n            <mo>&#x2032;</mo>\n          </msup>\n        </mrow>\n        <mo stretchy="false">)</mo>\n        <mo>+</mo>\n        <mi>&#x03BB;<!-- \xce\xbb --></mi>\n        <munder>\n          <mo>&#x2211;<!-- \xe2\x88\x91 --></mo>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>i</mi>\n          </mrow>\n        </munder>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mo stretchy="false">|</mo>\n        </mrow>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mo stretchy="false">|</mo>\n        </mrow>\n        <msub>\n          <mi mathvariant="normal">&#x2207;<!-- \xe2\x88\x87 --></mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>x</mi>\n          </mrow>\n        </msub>\n        <msub>\n          <mi>h</mi>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mi>i</mi>\n          </mrow>\n        </msub>\n        <mrow class="MJX-TeXAtom-ORD">\n          <mo stretchy="false">|</mo>\n        </mrow>\n        <msup>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mo stretchy="false">|</mo>\n          </mrow>\n          <mrow class="MJX-TeXAtom-ORD">\n            <mn>2</mn>\n          </mrow>\n        </msup>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle {\\mathcal {L}}(\\mathbf {x} ,\\mathbf {x\'} )+\\lambda \\sum _{i}||\\nabla _{x}h_{i}||^{2}}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8c969944dbfe7ccfb757e5430f98f1492d101282" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:25.167ex; height:5.509ex;" alt="{\\displaystyle {\\mathcal {L}}(\\mathbf {x} ,\\mathbf {x&#039;} )+\\lambda \\sum _{i}||\\nabla _{x}h_{i}||^{2}}"/></span></dd></dl>\n<h3><span id="Relationship_with_principal_component_analysis_.28PCA.29"></span><span class="mw-headline" id="Relationship_with_principal_component_analysis_(PCA)">Relationship with principal component analysis (PCA)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=8" title="Edit section: Relationship with principal component analysis (PCA)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>\n<p>If linear activations are used, or only a single sigmoid hidden layer, then the optimal solution to an autoencoder is strongly related to <a href="/wiki/Principal_component_analysis" title="Principal component analysis">principal component analysis</a> (PCA).<sup id="cite_ref-15" class="reference"><a href="#cite_note-15">&#91;15&#93;</a></sup><sup id="cite_ref-16" class="reference"><a href="#cite_note-16">&#91;16&#93;</a></sup> The weights of an autoencoder with a single hidden layer of size <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle p}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>p</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle p}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.089ex; width:1.259ex; height:2.009ex;" alt="p"/></span> (where <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle p}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>p</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle p}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.089ex; width:1.259ex; height:2.009ex;" alt="p"/></span> is less than the size of the input) span the same vector subspace as the one spanned by the first <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle p}">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mi>p</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle p}</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.089ex; width:1.259ex; height:2.009ex;" alt="p"/></span> principal components, and the output of the autoencoder is an orthogonal projection onto this subspace. The autoencoder weights are not equal to the principal components, and are generally not orthogonal, yet the principal components may be recovered from them using the <a href="/wiki/Singular_value_decomposition" title="Singular value decomposition">singular value decomposition</a>.<sup id="cite_ref-17" class="reference"><a href="#cite_note-17">&#91;17&#93;</a></sup>\n</p>\n<h2><span class="mw-headline" id="Training">Training</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=9" title="Edit section: Training">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<p>The training algorithm for an autoencoder can be summarized as\n</p>\n<dl><dd>For each input <span class="texhtml mvar" style="font-style:italic;">x</span>,\n<dl><dd>Do a feed-forward pass to compute activations at all hidden layers, then at the output layer to obtain an output <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {x\'} }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <msup>\n            <mi mathvariant="bold">x</mi>\n            <mo>&#x2032;</mo>\n          </msup>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {x\'} }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7d14ab6186e99346cb608a30858c3e1580f760e6" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.096ex; height:2.509ex;" alt="\\mathbf {x&#039;} "/></span></dd>\n<dd>Measure the deviation of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {x\'} }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <msup>\n            <mi mathvariant="bold">x</mi>\n            <mo>&#x2032;</mo>\n          </msup>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {x\'} }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7d14ab6186e99346cb608a30858c3e1580f760e6" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.096ex; height:2.509ex;" alt="\\mathbf {x&#039;} "/></span> from the input <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML"  alttext="{\\displaystyle \\mathbf {x} }">\n  <semantics>\n    <mrow class="MJX-TeXAtom-ORD">\n      <mstyle displaystyle="true" scriptlevel="0">\n        <mrow class="MJX-TeXAtom-ORD">\n          <mi mathvariant="bold">x</mi>\n        </mrow>\n      </mstyle>\n    </mrow>\n    <annotation encoding="application/x-tex">{\\displaystyle \\mathbf {x} }</annotation>\n  </semantics>\n</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/32adf004df5eb0a8c7fd8c0b6b7405183c5a5ef2" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.411ex; height:1.676ex;" alt="\\mathbf {x} "/></span> (typically using <a href="/wiki/Mean_squared_error" title="Mean squared error">squared error</a>),</dd>\n<dd><a href="/wiki/Backpropagation" title="Backpropagation">Backpropagate</a> the error through the net and perform weight updates.</dd></dl></dd></dl>\n<p>An autoencoder is often trained using one of the many variants of <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a> (such as <a href="/wiki/Conjugate_gradient_method" title="Conjugate gradient method">conjugate gradient method</a>, <a href="/wiki/Steepest_descent" class="mw-redirect" title="Steepest descent">steepest descent</a>, etc.). Though these are often reasonably effective, there are fundamental problems with the use of backpropagation to train networks with many hidden layers. Once errors are backpropagated to the first few layers, they become minuscule and insignificant. This means that the network will almost always learn to reconstruct the average of all the training data.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">&#91;<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (December 2013)">citation needed</span></a></i>&#93;</sup> Though more advanced backpropagation methods (such as the conjugate gradient method) can solve this problem to a certain extent, they still result in a very slow learning process and poor solutions. This problem can be remedied by using initial weights that approximate the final solution. The process of finding these initial weights is often referred to as <i>pretraining</i>.\n</p><p>Stephen Luttrell, while based at <a href="/wiki/RSRE" class="mw-redirect" title="RSRE">RSRE</a>, developed a technique for unsupervised training of hierarchical self-organizing neural nets with "many hidden layers",<sup id="cite_ref-18" class="reference"><a href="#cite_note-18">&#91;18&#93;</a></sup> which are equivalent to deep autoencoders. <a href="/wiki/Geoffrey_Hinton" title="Geoffrey Hinton">Geoffrey Hinton</a> developed an alternative pretraining technique for training many-layered deep autoencoders. This method involves treating each neighbouring set of two layers as a <a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">restricted Boltzmann machine</a> so that the pretraining approximates a good solution, then using a backpropagation technique to fine-tune the results.<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">&#91;19&#93;</a></sup> This model takes the name of <a href="/wiki/Deep_belief_network" title="Deep belief network">deep belief network</a>.\n</p>\n<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=10" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<ul><li><a href="/wiki/Representation_learning" class="mw-redirect" title="Representation learning">Representation learning</a></li>\n<li><a href="/wiki/Sparse_dictionary_learning" title="Sparse dictionary learning">Sparse dictionary learning</a></li></ul>\n<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Autoencoder&amp;action=edit&amp;section=11" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>\n<div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;">\n<ol class="references">\n<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><cite class="citation journal">Liou, Cheng-Yuan; Huang, Jau-Chi; Yang, Wen-Chie (2008). "Modeling word perception using the Elman network". <i>Neurocomputing</i>. <b>71</b> (16\xe2\x80\x9318): 3150. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1016%2Fj.neucom.2008.04.030">10.1016/j.neucom.2008.04.030</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neurocomputing&amp;rft.atitle=Modeling+word+perception+using+the+Elman+network&amp;rft.volume=71&amp;rft.issue=16%E2%80%9318&amp;rft.pages=3150&amp;rft.date=2008&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neucom.2008.04.030&amp;rft.aulast=Liou&amp;rft.aufirst=Cheng-Yuan&amp;rft.au=Huang%2C+Jau-Chi&amp;rft.au=Yang%2C+Wen-Chie&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span><style data-mw-deduplicate="TemplateStyles:r886058088">.mw-parser-output cite.citation{font-style:inherit}.mw-parser-output .citation q{quotes:"\\"""\\"""\'""\'"}.mw-parser-output .citation .cs1-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-limited a,.mw-parser-output .citation .cs1-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/Lock-gray-alt-2.svg/9px-Lock-gray-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .citation .cs1-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/Lock-red-alt-2.svg/9px-Lock-red-alt-2.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration{color:#555}.mw-parser-output .cs1-subscription span,.mw-parser-output .cs1-registration span{border-bottom:1px dotted;cursor:help}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Wikisource-logo.svg/12px-Wikisource-logo.svg.png")no-repeat;background-position:right .1em center}.mw-parser-output code.cs1-code{color:inherit;background:inherit;border:inherit;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;font-size:100%}.mw-parser-output .cs1-visible-error{font-size:100%}.mw-parser-output .cs1-maint{display:none;color:#33aa33;margin-left:0.3em}.mw-parser-output .cs1-subscription,.mw-parser-output .cs1-registration,.mw-parser-output .cs1-format{font-size:95%}.mw-parser-output .cs1-kern-left,.mw-parser-output .cs1-kern-wl-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right,.mw-parser-output .cs1-kern-wl-right{padding-right:0.2em}</style></span>\n</li>\n<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite class="citation journal">Liou, Cheng-Yuan; Cheng, Wei-Chen; Liou, Jiun-Wei; Liou, Daw-Ran (2014). "Autoencoder for words". <i>Neurocomputing</i>. <b>139</b>: 84\xe2\x80\x9396. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1016%2Fj.neucom.2013.09.055">10.1016/j.neucom.2013.09.055</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neurocomputing&amp;rft.atitle=Autoencoder+for+words&amp;rft.volume=139&amp;rft.pages=84-96&amp;rft.date=2014&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neucom.2013.09.055&amp;rft.aulast=Liou&amp;rft.aufirst=Cheng-Yuan&amp;rft.au=Cheng%2C+Wei-Chen&amp;rft.au=Liou%2C+Jiun-Wei&amp;rft.au=Liou%2C+Daw-Ran&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>\n</li>\n<li id="cite_note-VAE-3"><span class="mw-cite-backlink">^ <a href="#cite_ref-VAE_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-VAE_3-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-VAE_3-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-VAE_3-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><cite class="citation arxiv">Diederik P Kingma; Welling, Max (2013). "Auto-Encoding Variational Bayes". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1312.6114">1312.6114</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/stat.ML">stat.ML</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Auto-Encoding+Variational+Bayes&amp;rft.date=2013&amp;rft_id=info%3Aarxiv%2F1312.6114&amp;rft.au=Diederik+P+Kingma&amp;rft.au=Welling%2C+Max&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>\n</li>\n<li id="cite_note-gan_faces-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-gan_faces_4-0">^</a></b></span> <span class="reference-text">Generating Faces with Torch, Boesen A., Larsen L. and Sonderby S.K., 2015 <span class="url"><a rel="nofollow" class="external text" href="http://torch.ch/blog/2015/11/13/gan.html">torch<wbr />.ch<wbr />/blog<wbr />/2015<wbr />/11<wbr />/13<wbr />/gan<wbr />.html</a></span></span>\n</li>\n<li id="cite_note-domingos-5"><span class="mw-cite-backlink">^ <a href="#cite_ref-domingos_5-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-domingos_5-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-domingos_5-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation book"><a href="/wiki/Pedro_Domingos" title="Pedro Domingos">Domingos, Pedro</a> (2015). "4". <a href="/wiki/The_Master_Algorithm" title="The Master Algorithm"><i>The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World</i></a>. Basic Books. "Deeper into the Brain" subsection. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-046506192-1" title="Special:BookSources/978-046506192-1"><bdi>978-046506192-1</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=4&amp;rft.btitle=The+Master+Algorithm%3A+How+the+Quest+for+the+Ultimate+Learning+Machine+Will+Remake+Our+World&amp;rft.pages=%22Deeper+into+the+Brain%22+subsection&amp;rft.pub=Basic+Books&amp;rft.date=2015&amp;rft.isbn=978-046506192-1&amp;rft.aulast=Domingos&amp;rft.aufirst=Pedro&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>\n</li>\n<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><cite class="citation journal">Hutson, Matthew (23 February 2018). "New algorithm can create movies from just a few snippets of text". <i>Science</i>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1126%2Fscience.aat4126">10.1126/science.aat4126</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=New+algorithm+can+create+movies+from+just+a+few+snippets+of+text&amp;rft.date=2018-02-23&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.aat4126&amp;rft.aulast=Hutson&amp;rft.aufirst=Matthew&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>\n</li>\n<li id="cite_note-bengio-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-bengio_7-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Bengio, Y. (2009). <a rel="nofollow" class="external text" href="http://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf">"Learning Deep Architectures for AI"</a> <span class="cs1-format">(PDF)</span>. <i>Foundations and Trends in Machine Learning</i>. <b>2</b>: 1\xe2\x80\x93127. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.701.9550">10.1.1.701.9550</a></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1561%2F2200000006">10.1561/2200000006</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Foundations+and+Trends+in+Machine+Learning&amp;rft.atitle=Learning+Deep+Architectures+for+AI&amp;rft.volume=2&amp;rft.pages=1-127&amp;rft.date=2009&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.701.9550&amp;rft_id=info%3Adoi%2F10.1561%2F2200000006&amp;rft.aulast=Bengio&amp;rft.aufirst=Y.&amp;rft_id=http%3A%2F%2Fwww.iro.umontreal.ca%2F~lisa%2Fpointeurs%2FTR1312.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>\n</li>\n<li id="cite_note-ref9-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-ref9_8-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Vincent, Pascal; Larochelle, Hugo; Lajoie, Isabelle; Bengio, Yoshua; Manzagol, Pierre-Antoine (2010). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=1953039">"Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion"</a>. <i>The Journal of Machine Learning Research</i>. <b>11</b>: 3371\xe2\x80\x933408.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Journal+of+Machine+Learning+Research&amp;rft.atitle=Stacked+Denoising+Autoencoders%3A+Learning+Useful+Representations+in+a+Deep+Network+with+a+Local+Denoising+Criterion&amp;rft.volume=11&amp;rft.pages=3371-3408&amp;rft.date=2010&amp;rft.aulast=Vincent&amp;rft.aufirst=Pascal&amp;rft.au=Larochelle%2C+Hugo&amp;rft.au=Lajoie%2C+Isabelle&amp;rft.au=Bengio%2C+Yoshua&amp;rft.au=Manzagol%2C+Pierre-Antoine&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D1953039&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>\n</li>\n<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text"><cite class="citation"><a rel="nofollow" class="external text" href="https://web.stanford.edu/class/cs294a/sparseAutoencoder.pdf"><i>sparse autoencoders</i></a> <span class="cs1-format">(PDF)</span></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=sparse+autoencoders&amp;rft_id=https%3A%2F%2Fweb.stanford.edu%2Fclass%2Fcs294a%2FsparseAutoencoder.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>\n</li>\n<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Makhzani, Alireza; Frey, Brendan (2013). "k-sparse autoencoder". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1312.5663">1312.5663</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.LG">cs.LG</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=k-sparse+autoencoder&amp;rft.date=2013&amp;rft_id=info%3Aarxiv%2F1312.5663&amp;rft.aulast=Makhzani&amp;rft.aufirst=Alireza&amp;rft.au=Frey%2C+Brendan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>\n</li>\n<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text"><cite class="citation journal">Partaourides, Harris; Chatzis, Sotirios P (2017). "Asymmetric deep generative models". <i>Neurocomputing</i>. <b>241</b>: 90\xe2\x80\x9396. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1016%2Fj.neucom.2017.02.028">10.1016/j.neucom.2017.02.028</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neurocomputing&amp;rft.atitle=Asymmetric+deep+generative+models&amp;rft.volume=241&amp;rft.pages=90-96&amp;rft.date=2017&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neucom.2017.02.028&amp;rft.aulast=Partaourides&amp;rft.aufirst=Harris&amp;rft.au=Chatzis%2C+Sotirios+P&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>\n</li>\n<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><cite class="citation journal">Larsen, A. B. L.; S\xc3\xb8nderby, S. K.; Larochelle, H.; Winther, O. (2016). <a rel="nofollow" class="external text" href="http://proceedings.mlr.press/v48/larsen16.html">"Autoencoding beyond pixels using a learned similarity metric"</a>. <i>Proceedings of the 33rd International Conference on Machine Learning</i>. <b>28</b>: 1558\xe2\x80\x931566. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1512.09300">1512.09300</a></span>. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/2015arXiv151209300B">2015arXiv151209300B</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+33rd+International+Conference+on+Machine+Learning&amp;rft.atitle=Autoencoding+beyond+pixels+using+a+learned+similarity+metric&amp;rft.volume=28&amp;rft.pages=1558-1566&amp;rft.date=2016&amp;rft_id=info%3Aarxiv%2F1512.09300&amp;rft_id=info%3Abibcode%2F2015arXiv151209300B&amp;rft.aulast=Larsen&amp;rft.aufirst=A.+B.+L.&amp;rft.au=S%C3%B8nderby%2C+S.+K.&amp;rft.au=Larochelle%2C+H.&amp;rft.au=Winther%2C+O.&amp;rft_id=http%3A%2F%2Fproceedings.mlr.press%2Fv48%2Flarsen16.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>\n</li>\n<li id="cite_note-SigmaVAE1-13"><span class="mw-cite-backlink">^ <a href="#cite_ref-SigmaVAE1_13-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-SigmaVAE1_13-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Dorta, G.; Vicente, S.; Agapito, L.; Campbell, N. D. F.; Simpson, I. (2018). <a rel="nofollow" class="external text" href="http://openaccess.thecvf.com/content_cvpr_2018/html/Dorta_Structured_Uncertainty_Prediction_CVPR_2018_paper.html">"Structured Uncertainty Prediction Networks"</a>. <i>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</i>: 5477\xe2\x80\x935485. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1802.07079">1802.07079</a></span>. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/2018arXiv180207079D">2018arXiv180207079D</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+IEEE+Conference+on+Computer+Vision+and+Pattern+Recognition+%28CVPR%29&amp;rft.atitle=Structured+Uncertainty+Prediction+Networks&amp;rft.pages=5477-5485&amp;rft.date=2018&amp;rft_id=info%3Aarxiv%2F1802.07079&amp;rft_id=info%3Abibcode%2F2018arXiv180207079D&amp;rft.aulast=Dorta&amp;rft.aufirst=G.&amp;rft.au=Vicente%2C+S.&amp;rft.au=Agapito%2C+L.&amp;rft.au=Campbell%2C+N.+D.+F.&amp;rft.au=Simpson%2C+I.&amp;rft_id=http%3A%2F%2Fopenaccess.thecvf.com%2Fcontent_cvpr_2018%2Fhtml%2FDorta_Structured_Uncertainty_Prediction_CVPR_2018_paper.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>\n</li>\n<li id="cite_note-SigmaVAE2-14"><span class="mw-cite-backlink">^ <a href="#cite_ref-SigmaVAE2_14-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-SigmaVAE2_14-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation arxiv">Dorta, G.; Vicente, S.; Agapito, L.; Campbell, N. D. F.; Simpson, I. (2018). "Training VAEs Under Structured Residuals". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1804.01050">1804.01050</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/stat.ML">stat.ML</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=Training+VAEs+Under+Structured+Residuals&amp;rft.date=2018&amp;rft_id=info%3Aarxiv%2F1804.01050&amp;rft.aulast=Dorta&amp;rft.aufirst=G.&amp;rft.au=Vicente%2C+S.&amp;rft.au=Agapito%2C+L.&amp;rft.au=Campbell%2C+N.+D.+F.&amp;rft.au=Simpson%2C+I.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>\n</li>\n<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><cite class="citation journal">Bourlard, H.; Kamp, Y. (1988). "Auto-association by multilayer perceptrons and singular value decomposition". <i>Biological Cybernetics</i>. <b>59</b> (4\xe2\x80\x935): 291\xe2\x80\x93294. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1007%2FBF00332918">10.1007/BF00332918</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/3196773">3196773</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Biological+Cybernetics&amp;rft.atitle=Auto-association+by+multilayer+perceptrons+and+singular+value+decomposition&amp;rft.volume=59&amp;rft.issue=4%E2%80%935&amp;rft.pages=291-294&amp;rft.date=1988&amp;rft_id=info%3Adoi%2F10.1007%2FBF00332918&amp;rft_id=info%3Apmid%2F3196773&amp;rft.aulast=Bourlard&amp;rft.aufirst=H.&amp;rft.au=Kamp%2C+Y.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>\n</li>\n<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><cite class="citation book">Chicco, Davide; Sadowski, Peter; Baldi, Pierre (2014). "Deep autoencoder neural networks for gene ontology annotation predictions". <i>Proceedings of the 5th ACM Conference on Bioinformatics, Computational Biology, and Health Informatics - BCB \'14</i>. p.&#160;533. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1145%2F2649387.2649442">10.1145/2649387.2649442</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781450328944" title="Special:BookSources/9781450328944"><bdi>9781450328944</bdi></a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Deep+autoencoder+neural+networks+for+gene+ontology+annotation+predictions&amp;rft.btitle=Proceedings+of+the+5th+ACM+Conference+on+Bioinformatics%2C+Computational+Biology%2C+and+Health+Informatics+-+BCB+%2714&amp;rft.pages=533&amp;rft.date=2014&amp;rft_id=info%3Adoi%2F10.1145%2F2649387.2649442&amp;rft.isbn=9781450328944&amp;rft.aulast=Chicco&amp;rft.aufirst=Davide&amp;rft.au=Sadowski%2C+Peter&amp;rft.au=Baldi%2C+Pierre&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>\n</li>\n<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Plaut, E (2018). "From Principal Subspaces to Principal Components with Linear Autoencoders". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="cs1-lock-free" title="Freely accessible"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1804.10253">1804.10253</a></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/stat.ML">stat.ML</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft.atitle=From+Principal+Subspaces+to+Principal+Components+with+Linear+Autoencoders&amp;rft.date=2018&amp;rft_id=info%3Aarxiv%2F1804.10253&amp;rft.aulast=Plaut&amp;rft.aufirst=E&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>\n</li>\n<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text"><cite class="citation book">Luttrell, S. P. (October 1989). <a rel="nofollow" class="external text" href="http://ieeexplore.ieee.org/abstract/document/51919">"Hierarchical self-organising networks"</a>. <i>1989 First IEE International Conference on Artificial Neural Networks</i>. London: Institution of Engineering and Technology. pp.&#160;2\xe2\x80\x936.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Hierarchical+self-organising+networks&amp;rft.btitle=1989+First+IEE+International+Conference+on+Artificial+Neural+Networks&amp;rft.place=London&amp;rft.pages=2-6&amp;rft.pub=Institution+of+Engineering+and+Technology&amp;rft.date=1989-10&amp;rft.aulast=Luttrell&amp;rft.aufirst=S.+P.&amp;rft_id=http%3A%2F%2Fieeexplore.ieee.org%2Fabstract%2Fdocument%2F51919&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>\n</li>\n<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><cite class="citation journal">Hinton, G. E.; Salakhutdinov, R.R. (28 July 2006). "Reducing the Dimensionality of Data with Neural Networks". <i>Science</i>. <b>313</b> (5786): 504\xe2\x80\x93507. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/2006Sci...313..504H">2006Sci...313..504H</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1126%2Fscience.1127647">10.1126/science.1127647</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/16873662">16873662</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Reducing+the+Dimensionality+of+Data+with+Neural+Networks&amp;rft.volume=313&amp;rft.issue=5786&amp;rft.pages=504-507&amp;rft.date=2006-07-28&amp;rft_id=info%3Apmid%2F16873662&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.1127647&amp;rft_id=info%3Abibcode%2F2006Sci...313..504H&amp;rft.aulast=Hinton&amp;rft.aufirst=G.+E.&amp;rft.au=Salakhutdinov%2C+R.R.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutoencoder" class="Z3988"></span><link rel="mw-deduplicated-inline-style" href="mw-data:TemplateStyles:r886058088"/></span>\n</li>\n</ol></div>\n<!-- \nNewPP limit report\nParsed by mw1326\nCached time: 20190618144145\nCache expiry: 2592000\nDynamic content: false\nComplications: [vary\xe2\x80\x90revision]\nCPU time usage: 0.500 seconds\nReal time usage: 0.733 seconds\nPreprocessor visited node count: 1778/1000000\nPreprocessor generated node count: 0/1500000\nPost\xe2\x80\x90expand include size: 65886/2097152 bytes\nTemplate argument size: 1908/2097152 bytes\nHighest expansion depth: 11/40\nExpensive parser function count: 7/500\nUnstrip recursion depth: 1/20\nUnstrip post\xe2\x80\x90expand size: 59516/5000000 bytes\nNumber of Wikibase entities loaded: 4/400\nLua time usage: 0.248/10.000 seconds\nLua memory usage: 5.86 MB/50 MB\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  532.360      1 -total\n 67.72%  360.527      1 Template:Reflist\n 39.64%  211.004     10 Template:Cite_journal\n 16.19%   86.193      1 Template:Machine_learning_bar\n 15.37%   81.816      1 Template:Sidebar_with_collapsible_lists\n 14.40%   76.644      4 Template:Cite_arxiv\n  9.85%   52.415      1 Template:Cn\n  9.48%   50.484      2 Template:Fix\n  6.21%   33.068      4 Template:Category_handler\n  3.83%   20.368      1 Template:Longitem\n-->\n\n<!-- Saved in parser cache with key enwiki:pcache:idhash:6836612-0!canonical!math=5 and timestamp 20190618144144 and revision id 901360758\n -->\n</div><noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>\n\t\t\n\t\t<div class="printfooter">Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Autoencoder&amp;oldid=901360758">https://en.wikipedia.org/w/index.php?title=Autoencoder&amp;oldid=901360758</a>"</div>\n\t\t\n\t\t<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li><li><a href="/wiki/Category:Unsupervised_learning" title="Category:Unsupervised learning">Unsupervised learning</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_May_2019" title="Category:Articles with unsourced statements from May 2019">Articles with unsourced statements from May 2019</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_December_2013" title="Category:Articles with unsourced statements from December 2013">Articles with unsourced statements from December 2013</a></li></ul></div></div>\n\t\t\n\t\t<div class="visualClear"></div>\n\t\t\n\t</div>\n</div>\n\n\t\t<div id="mw-navigation">\n\t\t\t<h2>Navigation menu</h2>\n\t\t\t<div id="mw-head">\n\t\t\t\t\t\t\t\t\t<div id="p-personal" role="navigation" aria-labelledby="p-personal-label">\n\t\t\t\t\t\t<h3 id="p-personal-label">Personal tools</h3>\n\t\t\t\t\t\t<ul>\n\t\t\t\t\t\t\t<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Autoencoder" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Autoencoder" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>\t\t\t\t\t\t</ul>\n\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t<div id="left-navigation">\n\t\t\t\t\t\t\t\t\t\t<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">\n\t\t\t\t\t\t<h3 id="p-namespaces-label">Namespaces</h3>\n\t\t\t\t\t\t<ul>\n\t\t\t\t\t\t\t<li id="ca-nstab-main" class="selected"><span><a href="/wiki/Autoencoder" title="View the content page [c]" accesskey="c">Article</a></span></li><li id="ca-talk"><span><a href="/wiki/Talk:Autoencoder" rel="discussion" title="Discussion about the content page [t]" accesskey="t">Talk</a></span></li>\t\t\t\t\t\t</ul>\n\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">\n\t\t\t\t\t\t\t\t\t\t\t\t<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-variants-label" />\n\t\t\t\t\t\t<h3 id="p-variants-label">\n\t\t\t\t\t\t\t<span>Variants</span>\n\t\t\t\t\t\t</h3>\n\t\t\t\t\t\t<ul class="menu">\n\t\t\t\t\t\t\t\t\t\t\t\t\t</ul>\n\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t\t<div id="right-navigation">\n\t\t\t\t\t\t\t\t\t\t<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">\n\t\t\t\t\t\t<h3 id="p-views-label">Views</h3>\n\t\t\t\t\t\t<ul>\n\t\t\t\t\t\t\t<li id="ca-view" class="collapsible selected"><span><a href="/wiki/Autoencoder">Read</a></span></li><li id="ca-edit" class="collapsible"><span><a href="/w/index.php?title=Autoencoder&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></span></li><li id="ca-history" class="collapsible"><span><a href="/w/index.php?title=Autoencoder&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></span></li>\t\t\t\t\t\t</ul>\n\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">\n\t\t\t\t\t\t<input type="checkbox" class="vectorMenuCheckbox" aria-labelledby="p-cactions-label" />\n\t\t\t\t\t\t<h3 id="p-cactions-label"><span>More</span></h3>\n\t\t\t\t\t\t<ul class="menu">\n\t\t\t\t\t\t\t\t\t\t\t\t\t</ul>\n\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t\t<div id="p-search" role="search">\n\t\t\t\t\t\t<h3>\n\t\t\t\t\t\t\t<label for="searchInput">Search</label>\n\t\t\t\t\t\t</h3>\n\t\t\t\t\t\t<form action="/w/index.php" id="searchform">\n\t\t\t\t\t\t\t<div id="simpleSearch">\n\t\t\t\t\t\t\t\t<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/><input type="hidden" value="Special:Search" name="title"/><input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/><input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>\t\t\t\t\t\t\t</div>\n\t\t\t\t\t\t</form>\n\t\t\t\t\t</div>\n\t\t\t\t\t\t\t\t\t</div>\n\t\t\t</div>\n\t\t\t<div id="mw-panel">\n\t\t\t\t<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="/wiki/Main_Page" title="Visit the main page"></a></div>\n\t\t\t\t\t\t<div class="portal" role="navigation" id="p-navigation" aria-labelledby="p-navigation-label">\n\t\t\t<h3 id="p-navigation-label">Navigation</h3>\n\t\t\t<div class="body">\n\t\t\t\t\t\t\t\t<ul>\n\t\t\t\t\t<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content \xe2\x80\x93 the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li>\t\t\t\t</ul>\n\t\t\t\t\t\t\t</div>\n\t\t</div>\n\t\t\t<div class="portal" role="navigation" id="p-interaction" aria-labelledby="p-interaction-label">\n\t\t\t<h3 id="p-interaction-label">Interaction</h3>\n\t\t\t<div class="body">\n\t\t\t\t\t\t\t\t<ul>\n\t\t\t\t\t<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>\t\t\t\t</ul>\n\t\t\t\t\t\t\t</div>\n\t\t</div>\n\t\t\t<div class="portal" role="navigation" id="p-tb" aria-labelledby="p-tb-label">\n\t\t\t<h3 id="p-tb-label">Tools</h3>\n\t\t\t<div class="body">\n\t\t\t\t\t\t\t\t<ul>\n\t\t\t\t\t<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Autoencoder" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Autoencoder" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Autoencoder&amp;oldid=901360758" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Autoencoder&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q786435" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Autoencoder&amp;id=901360758" title="Information on how to cite this page">Cite this page</a></li>\t\t\t\t</ul>\n\t\t\t\t\t\t\t</div>\n\t\t</div>\n\t\t\t<div class="portal" role="navigation" id="p-coll-print_export" aria-labelledby="p-coll-print_export-label">\n\t\t\t<h3 id="p-coll-print_export-label">Print/export</h3>\n\t\t\t<div class="body">\n\t\t\t\t\t\t\t\t<ul>\n\t\t\t\t\t<li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Autoencoder">Create a book</a></li><li id="coll-download-as-rl"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Autoencoder&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Autoencoder&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>\t\t\t\t</ul>\n\t\t\t\t\t\t\t</div>\n\t\t</div>\n\t\t\t<div class="portal" role="navigation" id="p-lang" aria-labelledby="p-lang-label">\n\t\t\t<h3 id="p-lang-label">Languages</h3>\n\t\t\t<div class="body">\n\t\t\t\t\t\t\t\t<ul>\n\t\t\t\t\t<li class="interlanguage-link interwiki-de"><a href="https://de.wikipedia.org/wiki/Autoencoder" title="Autoencoder \xe2\x80\x93 German" lang="de" hreflang="de" class="interlanguage-link-target">Deutsch</a></li><li class="interlanguage-link interwiki-fa"><a href="https://fa.wikipedia.org/wiki/%D8%AE%D9%88%D8%AF%D8%B1%D9%85%D8%B2%DA%AF%D8%B0%D8%A7%D8%B1" title="\xd8\xae\xd9\x88\xd8\xaf\xd8\xb1\xd9\x85\xd8\xb2\xda\xaf\xd8\xb0\xd8\xa7\xd8\xb1 \xe2\x80\x93 Persian" lang="fa" hreflang="fa" class="interlanguage-link-target">\xd9\x81\xd8\xa7\xd8\xb1\xd8\xb3\xdb\x8c</a></li><li class="interlanguage-link interwiki-fr"><a href="https://fr.wikipedia.org/wiki/Auto-encodeur" title="Auto-encodeur \xe2\x80\x93 French" lang="fr" hreflang="fr" class="interlanguage-link-target">Fran\xc3\xa7ais</a></li><li class="interlanguage-link interwiki-ja"><a href="https://ja.wikipedia.org/wiki/%E3%82%AA%E3%83%BC%E3%83%88%E3%82%A8%E3%83%B3%E3%82%B3%E3%83%BC%E3%83%80" title="\xe3\x82\xaa\xe3\x83\xbc\xe3\x83\x88\xe3\x82\xa8\xe3\x83\xb3\xe3\x82\xb3\xe3\x83\xbc\xe3\x83\x80 \xe2\x80\x93 Japanese" lang="ja" hreflang="ja" class="interlanguage-link-target">\xe6\x97\xa5\xe6\x9c\xac\xe8\xaa\x9e</a></li><li class="interlanguage-link interwiki-ru"><a href="https://ru.wikipedia.org/wiki/%D0%90%D0%B2%D1%82%D0%BE%D0%BA%D0%BE%D0%B4%D0%B8%D1%80%D0%BE%D0%B2%D1%89%D0%B8%D0%BA" title="\xd0\x90\xd0\xb2\xd1\x82\xd0\xbe\xd0\xba\xd0\xbe\xd0\xb4\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd1\x89\xd0\xb8\xd0\xba \xe2\x80\x93 Russian" lang="ru" hreflang="ru" class="interlanguage-link-target">\xd0\xa0\xd1\x83\xd1\x81\xd1\x81\xd0\xba\xd0\xb8\xd0\xb9</a></li><li class="interlanguage-link interwiki-uk"><a href="https://uk.wikipedia.org/wiki/%D0%90%D0%B2%D1%82%D0%BE%D0%BA%D0%BE%D0%B4%D1%83%D0%B2%D0%B0%D0%BB%D1%8C%D0%BD%D0%B8%D0%BA" title="\xd0\x90\xd0\xb2\xd1\x82\xd0\xbe\xd0\xba\xd0\xbe\xd0\xb4\xd1\x83\xd0\xb2\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xb8\xd0\xba \xe2\x80\x93 Ukrainian" lang="uk" hreflang="uk" class="interlanguage-link-target">\xd0\xa3\xd0\xba\xd1\x80\xd0\xb0\xd1\x97\xd0\xbd\xd1\x81\xd1\x8c\xd0\xba\xd0\xb0</a></li><li class="interlanguage-link interwiki-zh"><a href="https://zh.wikipedia.org/wiki/%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8" title="\xe8\x87\xaa\xe7\xbc\x96\xe7\xa0\x81\xe5\x99\xa8 \xe2\x80\x93 Chinese" lang="zh" hreflang="zh" class="interlanguage-link-target">\xe4\xb8\xad\xe6\x96\x87</a></li>\t\t\t\t</ul>\n\t\t\t\t<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q786435#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>\t\t\t</div>\n\t\t</div>\n\t\t\t\t</div>\n\t\t</div>\n\t\t\t\t<div id="footer" role="contentinfo">\n\t\t\t\t\t\t<ul id="footer-info">\n\t\t\t\t\t\t\t\t<li id="footer-info-lastmod"> This page was last edited on 11 June 2019, at 10:59<span class="anonymous-show">&#160;(UTC)</span>.</li>\n\t\t\t\t\t\t\t\t<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;\nadditional terms may apply.  By using this site, you agree to the <a href="//foundation.wikimedia.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//foundation.wikimedia.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia\xc2\xae is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>\n\t\t\t\t\t\t\t</ul>\n\t\t\t\t\t\t<ul id="footer-places">\n\t\t\t\t\t\t\t\t<li id="footer-places-privacy"><a href="https://foundation.wikimedia.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>\n\t\t\t\t\t\t\t\t<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>\n\t\t\t\t\t\t\t\t<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>\n\t\t\t\t\t\t\t\t<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>\n\t\t\t\t\t\t\t\t<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>\n\t\t\t\t\t\t\t\t<li id="footer-places-cookiestatement"><a href="https://foundation.wikimedia.org/wiki/Cookie_statement">Cookie statement</a></li>\n\t\t\t\t\t\t\t\t<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Autoencoder&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>\n\t\t\t\t\t\t\t</ul>\n\t\t\t\t\t\t\t\t\t\t<ul id="footer-icons" class="noprint">\n\t\t\t\t\t\t\t\t\t\t<li id="footer-copyrightico">\n\t\t\t\t\t\t<a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a>\t\t\t\t\t</li>\n\t\t\t\t\t\t\t\t\t\t<li id="footer-poweredbyico">\n\t\t\t\t\t\t<a href="https://www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a>\t\t\t\t\t</li>\n\t\t\t\t\t\t\t\t\t</ul>\n\t\t\t\t\t\t<div style="clear: both;"></div>\n\t\t</div>\n\t\t\n\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.500","walltime":"0.733","ppvisitednodes":{"value":1778,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":65886,"limit":2097152},"templateargumentsize":{"value":1908,"limit":2097152},"expansiondepth":{"value":11,"limit":40},"expensivefunctioncount":{"value":7,"limit":500},"unstrip-depth":{"value":1,"limit":20},"unstrip-size":{"value":59516,"limit":5000000},"entityaccesscount":{"value":4,"limit":400},"timingprofile":["100.00%  532.360      1 -total"," 67.72%  360.527      1 Template:Reflist"," 39.64%  211.004     10 Template:Cite_journal"," 16.19%   86.193      1 Template:Machine_learning_bar"," 15.37%   81.816      1 Template:Sidebar_with_collapsible_lists"," 14.40%   76.644      4 Template:Cite_arxiv","  9.85%   52.415      1 Template:Cn","  9.48%   50.484      2 Template:Fix","  6.21%   33.068      4 Template:Category_handler","  3.83%   20.368      1 Template:Longitem"]},"scribunto":{"limitreport-timeusage":{"value":"0.248","limit":"10.000"},"limitreport-memusage":{"value":6144745,"limit":52428800},"limitreport-logs":"table#1 {\\n  [\\"size\\"] = \\"tiny\\",\\n}\\n"},"cachereport":{"origin":"mw1326","timestamp":"20190618144145","ttl":2592000,"transientcontent":false}}});});</script>\n<script type="application/ld+json">{"@context":"https:\\/\\/schema.org","@type":"Article","name":"Autoencoder","url":"https:\\/\\/en.wikipedia.org\\/wiki\\/Autoencoder","sameAs":"http:\\/\\/www.wikidata.org\\/entity\\/Q786435","mainEntity":"http:\\/\\/www.wikidata.org\\/entity\\/Q786435","author":{"@type":"Organization","name":"Contributors to Wikimedia projects"},"publisher":{"@type":"Organization","name":"Wikimedia Foundation, Inc.","logo":{"@type":"ImageObject","url":"https:\\/\\/www.wikimedia.org\\/static\\/images\\/wmf-hor-googpub.png"}},"datePublished":"2006-09-04T09:11:50Z","dateModified":"2019-06-11T10:59:57Z","image":"https:\\/\\/upload.wikimedia.org\\/wikipedia\\/commons\\/2\\/28\\/Autoencoder_structure.png"}</script>\n<script>(RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":86,"wgHostname":"mw1271"});});</script>\n</body>\n</html>\n'