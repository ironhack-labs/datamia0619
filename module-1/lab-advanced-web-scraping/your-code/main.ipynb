{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Web Scraping Lab\n",
    "\n",
    "In this lab you will first learn the following code snippet which is a simple web spider class that allows you to scrape paginated webpages. Read the code, run it, and make sure you understand how it work. In the challenges of this lab, we will guide you in building up this class so that eventually you will have a more robus web spider that you can further work on in the Web Scraping Project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"A Light in the Attic\"', '\"Tipping the Velvet\"', '\"Soumission\"', '\"Sharp Objects\"', '\"Sapiens: A Brief History of Humankind\"', '\"The Requiem Red\"', '\"The Dirty Little Secrets of Getting Your Dream Job\"', '\"The Black Maria\"', '\"Shakespeare\\'s Sonnets\"', '\"Set Me Free\"', '\"Rip it Up and Start Again\"', '\"Olio\"', '\"Libertarianism for Beginners\"', '\"It\\'s Only the Himalayas\"']\n",
      "request was successful\n",
      "['\"In Her Wake\"', '\"How Music Works\"', '\"Black Dust\"', '\"Birdsong: A Story in Pictures\"', '\"Aladdin and His Wonderful Lamp\"', '\"Wall and Piece\"', '\"The Elephant Tree\"', '\"The Bear and the Piano\"', '\"Sophie\\'s World\"', '\"Penny Maybe\"', '\"Maude (1883-1993):She Grew Up with the country\"', '\"In a Dark, Dark Wood\"', '\"Behind Closed Doors\"', '\"You can\\'t bury them all: Poems\"']\n",
      "request was successful\n",
      "['\"Slow States of Collapse: Poems\"', '\"Reasons to Stay Alive\"', '\"When We Collided\"', '\"We Love You, Charlie Freeman\"', '\"Unicorn Tracks\"', '\"This One Summer\"', '\"Thirst\"', '\"The Torch Is Passed: A Harding Family Story\"', '\"The Secret of Dreadwillow Carse\"', '\"The Past Never Ends\"']\n",
      "request was successful\n",
      "['\"The Death of Humanity: and the Case for Life\"', '\"The Art Forger\"', '\"Soul Reader\"', '\"Security\"']\n",
      "request was successful\n",
      "['\"Pop Gun War, Volume 1: Gift\"', '\"Patience\"', '\"On a Midnight Clear\"', '\"My Paris Kitchen: Recipes and Stories\"', '\"Masks and Shadows\"', '\"Join\"', '\"In the Country We Love: My Family Divided\"']\n",
      "request was successful\n",
      "['\"How to Be Miserable: 40 Strategies You Already Use\"', '\"Codename Baboushka, Volume 1: The Conclave of Death\"', '\"Camp Midnight\"', '\"Burning\"', '\"Bossypants\"']\n",
      "request was successful\n",
      "['\"A World of Flavor: Your Gluten Free Passport\"', '\"A Murder in Time\"', '\"A Fierce and Subtle Poison\"', '\"You Are What You Love: The Spiritual Power of Habit\"', '\"Tracing Numbers on a Train\"', '\"Thirteen Reasons Why\"', '\"The Wedding Dress\"', '\"The Vacationers\"', '\"The Stranger\"']\n",
      "request was successful\n",
      "['\"The Project\"', '\"The Power of Now: A Guide to Spiritual Enlightenment\"', '\"The Kite Runner\"', '\"The House by the Lake\"', '\"The Girl on the Train\"', '\"The Genius of Birds\"', '\"The Emerald Mystery\"']\n",
      "request was successful\n",
      "['\"The Art of War\"', '\"The Argonauts\"', '\"Something More Than This\"', '\"Soft Apocalypse\"', '\"So You\\'ve Been Publicly Shamed\"', '\"Scarlett Epstein Hates It Here\"', '\"Romero and Juliet: A Tragic Tale of Love and Zombies\"', '\"Redeeming Love\"', '\"Poems That Make Grown Women Cry\"', '\"Nightingale, Sing\"', '\"Night Sky with Exit Wounds\"', '\"Mrs. Houdini\"']\n",
      "request was successful\n",
      "['\"Modern Romance\"', '\"Louisa: The Extraordinary Life of Mrs. Adams\"', '\"Little Red\"', '\"Large Print Heart of the Pride\"', '\"Grumbles\"', '\"Follow You Home\"', '\"Drive: The Surprising Truth About What Motivates Us\"']\n",
      "request was successful\n",
      "['\"Dark Notes\"', '\"Close to You\"', '\"Chasing Heaven: What Dying Taught Me About Living\"', '\"Big Magic: Creative Living Beyond Fear\"', '\"Ayumi\\'s Violin\"', '\"Anonymous\"', '\"Amy Meets the Saints and Sages\"', '\"Amid the Chaos\"', '\"Amatus\"', '\"Agnostic: A Spirited Manifesto\"', '\"Zealot: The Life and Times of Jesus of Nazareth\"', '\"Wild Swans\"']\n",
      "request was successful\n",
      "['\"Walt Disney\\'s Alice in Wonderland\"', '\"Twenty Yawns\"', '\"Through the Woods\"', '\"This Is Where It Ends\"', '\"The Year of Magical Thinking\"', '\"The Wright Brothers\"', '\"The Time Keeper\"', '\"The Testament of Mary\"', '\"The Star-Touched Queen\"', '\"The Songs of the Gods\"', '\"The Song of Achilles\"']\n",
      "request was successful\n",
      "['\"The Marriage of Opposites\"', '\"The Loney\"', '\"The Immortal Life of Henrietta Lacks\"', '\"The Dovekeepers\"', '\"The Darkest Lie\"', '\"Take Me with You\"']\n",
      "request was successful\n",
      "['\"Swell: A Year of Waves\"', '\"Still Life with Bread Crumbs\"', '\"Steve Jobs\"', '\"Shtum\"', '\"Robin War\"', '\"Rain Fish\"', '\"Pet Sematary\"', '\"Once Was a Time\"']\n",
      "request was successful\n",
      "['\"My Name Is Lucy Barton\"', '\"My Mrs. Brown\"', '\"My Kind of Crazy\"', '\"Made to Stick: Why Some Ideas Survive and Others Die\"', '\"Luis Paints the World\"', '\"Luckiest Girl Alive\"', '\"Let It Out: A Journey Through Journaling\"']\n",
      "request was successful\n",
      "['\"Hamilton: The Revolution\"', '\"Greek Mythic History\"', '\"God: The Most Unpleasant Character in All Fiction\"', '\"Glory over Everything: Beyond The Kitchen House\"', '\"Feathers: Displays of Brilliant Plumage\"', '\"Every Last Word\"', '\"El Deafo\"', '\"Eight Hundred Grapes\"', '\"Eat Fat, Get Thin\"', '\"Don\\'t Get Caught\"', '\"Dear Mr. Knightley\"', '\"Daily Fantasy Sports\"', '\"Crazy Love: Overwhelmed by a Relentless God\"']\n",
      "request was successful\n",
      "['\"Cell\"', '\"Carry On, Warrior: Thoughts on Life Unarmed\"', '\"Carrie\"', '\"Brain on Fire: My Month of Madness\"', '\"Batman: Europa\"', '\"Barefoot Contessa Back to Basics\"', '\"Balloon Animals\"']\n",
      "request was successful\n",
      "['\"All the Light We Cannot See\"', '\"Abstract City\"', '\"A People\\'s History of the United States\"', '\"A Man Called Ove\"', '\"A Distant Mirror: The Calamitous 14th Century\"', '\"The Three Searches, Meaning, and the Story\"', '\"Searching for Meaning in Gailana\"', '\"Rook\"', '\"My Kitchen Year: 136 Recipes That Saved My Life\"', '\"The Star-Touched Queen\"']\n",
      "request was successful\n",
      "['\"The Dinner Party\"', '\"The Diary of a Young Girl\"', '\"The Children\"', '\"Most Wanted\"', '\"Love, Lies and Spies\"']\n",
      "request was successful\n",
      "['\"Furiously Happy: A Funny Book About Horrible Things\"', '\"Everyday Italian: 125 Simple and Delicious Recipes\"', '\"Eleanor &amp; Park\"', '\"A Paris Apartment\"', '\"Troublemaker: Surviving Hollywood and Scientology\"', '\"The Widow\"', '\"The Improbability of Love\"', '\"The Art of Startup Fundraising\"', '\"Playing with Fire\"']\n",
      "request was successful\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "class IronhackSpider:\n",
    "    \"\"\"\n",
    "    This is the constructor class to which you can pass a bunch of parameters. \n",
    "    These parameters are stored to the class instance variables so that the\n",
    "    class functions can access them later.\n",
    "    \n",
    "    url_pattern: the regex pattern of the web urls to scape\n",
    "    pages_to_scrape: how many pages to scrape\n",
    "    sleep_interval: the time interval in seconds to delay between requests. If <0, requests will not be delayed.\n",
    "    content_parser: a function reference that will extract the intended info from the scraped content.\n",
    "    \"\"\"\n",
    "    def __init__(self, url_pattern, pages_to_scrape=10, sleep_interval=-1, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "    \n",
    "    \"\"\"\n",
    "    Scrape the content of a single url.\n",
    "    \"\"\"\n",
    "    def scrape_url(self, url):\n",
    "            response = requests.get(url)\n",
    "            result = self.content_parser(response.content)\n",
    "            self.output_results(result)\n",
    "            if response.status_code < 300:\n",
    "                print('request was successful')\n",
    "            elif response.status_code >= 400 and response.status_code < 500:\n",
    "                print('request failed because the resource either does not exist or is forbidden')\n",
    "            else:\n",
    "                print('request failed because the response server encountered an error')\n",
    "    \n",
    "    \"\"\"\n",
    "    Export the scraped content. Right now it simply print out the results.\n",
    "    But in the future you can export the results into a text file or database.\n",
    "    \"\"\"\n",
    "    def output_results(self, r):\n",
    "        print(r)\n",
    "    \n",
    "    \"\"\"\n",
    "    After the class is instantiated, call this function to start the scraping jobs.\n",
    "    This function uses a FOR loop to call `scrape_url()` for each url to scrape.\n",
    "    \"\"\"\n",
    "    def kickstart(self):\n",
    "        import time\n",
    "\n",
    "        for i in range(1, self.pages_to_scrape+1):\n",
    "            time.sleep(self.sleep_interval+1)\n",
    "            self.scrape_url(self.url_pattern % i)\n",
    "\n",
    "\n",
    "URL_PATTERN = 'http://quotes.toscrape.com/page/%s/' # regex pattern for the urls to scrape\n",
    "URL_PATTERN = 'http://books.toscrape.com/catalogue/page-%s.html'\n",
    "PAGES_TO_SCRAPE = 20 # how many webpages to scrapge\n",
    "\n",
    "\"\"\"\n",
    "This is a custom parser function you will complete in the challenge.\n",
    "Right now it simply returns the string passed to it. But in this lab\n",
    "you will complete this function so that it extracts the quotes.\n",
    "This function will be passed to the IronhackSpider class.\n",
    "\"\"\"\n",
    "def quotes_parser(content):\n",
    "    import re\n",
    "    if URL_PATTERN == 'http://quotes.toscrape.com/page/%s/':\n",
    "        html = BeautifulSoup(content,'lxml')\n",
    "        search = str(html.find_all('span',{'class':'text'}))\n",
    "        split = search.split('</span>, <span class=\"text\" itemprop=\"text\">')\n",
    "        strip = [i.strip('</span>, <span class=\"text\" itemprop=\"text\">') for i in split]\n",
    "        return strip\n",
    "    else: \n",
    "        html = BeautifulSoup(content,'lxml')\n",
    "        search = str(html.find_all('h3'))\n",
    "        re = re.findall('title=\"[A-Z].{1,50}[a-z]\"',search)\n",
    "        lstrip = [i.lstrip('title=') for i in re]\n",
    "        return lstrip\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "my_spider = IronhackSpider(URL_PATTERN, PAGES_TO_SCRAPE, content_parser=quotes_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "my_spider.kickstart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 1 - Custom Parser Function\n",
    "\n",
    "In this challenge, complete the custom `quotes_parser()` function so that the returned result contains the quote string instead of the whole html page content.\n",
    "\n",
    "In the cell below, write your updated `quotes_parser()` function and kickstart the spider. Make sure the results being printed contain a list of quote strings extracted from the html content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>, <span class=\"text\" itemprop=\"text\">“It is our choices, Harry, that show what we truly are, far more than our abilities.”</span>, <span class=\"text\" itemprop=\"text\">“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”</span>, <span class=\"text\" itemprop=\"text\">“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”</span>, <span class=\"text\" itemprop=\"text\">“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”</span>, <span class=\"text\" itemprop=\"text\">“Try not to become a man of success. Rather become a man of value.”</span>, <span class=\"text\" itemprop=\"text\">“It is better to be hated for what you are than to be loved for what you are not.”</span>, <span class=\"text\" itemprop=\"text\">“I have not failed. I've just found 10,000 ways that won't work.”</span>, <span class=\"text\" itemprop=\"text\">“A woman is like a tea bag; you never know how strong it is until it's in hot water.”</span>, <span class=\"text\" itemprop=\"text\">“A day without sunshine is like, you know, night.”</span>]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "def quotes_parser(content):\n",
    "    html = BeautifulSoup(content,'lxml')\n",
    "    search = str(html.find_all('span',{'class':'text'}))\n",
    "    split = search.split('</span>, <span class=\"text\" itemprop=\"text\">')\n",
    "    strip = [i.strip('</span>, <span class=\"text\" itemprop=\"text\">') for i in split]\n",
    "    return strip\n",
    "\n",
    "my_spider.kickstart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge 2 - Error Handling\n",
    "\n",
    "In `IronhackSpider.scrape_url()`, catch any error that might occur when you make requests to scrape the webpage. This includes checking the response status code and catching http request errors such as timeout, SSL, and too many redirects.\n",
    "\n",
    "In the cell below, place your entire code including the updated `IronhackSpdier` class and the code to kickstart the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def scrape_url(self, url):\n",
    "        response = requests.get(url)\n",
    "        result = self.content_parser(response.content)\n",
    "        self.output_results(result)\n",
    "        if response.status_code < 300:\n",
    "            print('request was successful')\n",
    "        elif response.status_code >= 400 and r.status_code < 500:\n",
    "            print('request failed because the resource either does not exist or is forbidden')\n",
    "        else:\n",
    "            print('request failed because the response server encountered an error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Sleep Interval\n",
    "\n",
    "In `IronhackSpider.kickstart()`, implement `sleep_interval`. You will check if `self.sleep_interval` is larger than 0. If so, tell the FOR loop to sleep the given amount of time before making the next request.\n",
    "\n",
    "In the cell below, place your entire code including the updated `IronhackSpdier` class and the code to kickstart the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "def kickstart(self):\n",
    "    import time\n",
    "    \n",
    "    for i in range(1, self.pages_to_scrape+1):\n",
    "        time.sleep(sleep_interval+1)\n",
    "        self.scrape_url(self.url_pattern % i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Test Batch Scraping\n",
    "\n",
    "Change the `PAGES_TO_SCRAPE` value from `1` to `10`. Try if your code still works as intended to scrape 10 webpages. If there are errors in your code, fix them.\n",
    "\n",
    "In the cell below, place your entire code including the updated `IronhackSpdier` class and the code to kickstart the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works, check code above\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 5 - Scrape a Different Website\n",
    "\n",
    "Update the parameters passed to the `IronhackSpider` constructor so that you coder can crawl [books.toscrape.com](http://books.toscrape.com/). You will need to use a different `URL_PATTERN` (figure out the new url pattern by yourself) and write another parser function to be passed to `IronhackSpider`. \n",
    "\n",
    "In the cell below, place your entire code including the updated `IronhackSpdier` class and the code to kickstart the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give me a break\n",
    "def quotes_parser(content):\n",
    "    import re\n",
    "    if URL_PATTERN == 'http://quotes.toscrape.com/page/%s/':\n",
    "        html = BeautifulSoup(content,'lxml')\n",
    "        search = str(html.find_all('span',{'class':'text'}))\n",
    "        split = search.split('</span>, <span class=\"text\" itemprop=\"text\">')\n",
    "        strip = [i.strip('</span>, <span class=\"text\" itemprop=\"text\">') for i in split]\n",
    "        return strip\n",
    "    else: \n",
    "        html = BeautifulSoup(content,'lxml')\n",
    "        search = str(html.find_all('h3'))\n",
    "        re = re.findall('title=\"[A-Z].{1,50}[a-z]\"',search)\n",
    "        lstrip = [i.lstrip('title=') for i in re]\n",
    "        return lstrip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 1 - Making Your Spider Unblockable\n",
    "\n",
    "Use techniques such as randomizing user agents and referers in your requests to reduce the likelihood that your spider is blocked by websites. [Here](http://blog.adnansiddiqi.me/5-strategies-to-write-unblock-able-web-scrapers-in-python/) is a great article to learn these techniques.\n",
    "\n",
    "In the cell below, place your entire code including the updated `IronhackSpdier` class and the code to kickstart the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 2 - Making Asynchronous Calls\n",
    "\n",
    "Implement asynchronous calls to `IronhackSpider`. You will make requests in parallel to complete your tasks faster.\n",
    "\n",
    "In the cell below, place your entire code including the updated `IronhackSpdier` class and the code to kickstart the spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
