{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time \n",
    "\n",
    "class IronhackSpider:\n",
    "    \n",
    "    def __init__(self, url_pattern, pages_to_scrape=7, sleep_interval=2, content_parser=None):\n",
    "        self.url_pattern = url_pattern\n",
    "        self.pages_to_scrape = pages_to_scrape\n",
    "        self.sleep_interval = sleep_interval\n",
    "        self.content_parser = content_parser\n",
    "    \n",
    "\n",
    "    def scrape_url(self, url):\n",
    "        response = requests.get(url, timeout = 10)\n",
    "        response_content = response.content\n",
    "        soup = BeautifulSoup(response_content,'html.parser')\n",
    "        soup.prettify()\n",
    "        if response.status_code < 300:\n",
    "            print('request was successful')\n",
    "            result = self.content_parser(soup)\n",
    "            self.output_results(result)\n",
    "        elif response.status_code >= 400 and r.status_code < 500:\n",
    "            print('request failed because the resource either does not exist or is forbidden')\n",
    "        else:\n",
    "            print('request failed because the response server encountered an error')\n",
    "    \n",
    "    \"\"\"\n",
    "    Export the scraped content. Right now it simply print out the results.\n",
    "    But in the future you can export the results into a text file or database.\n",
    "    \"\"\"\n",
    "    def output_results(self, r):\n",
    "        print(r)\n",
    "    \n",
    "    \"\"\"\n",
    "    After the class is instantiated, call this function to start the scraping jobs.\n",
    "    This function uses a FOR loop to call `scrape_url()` for each url to scrape.\n",
    "    \"\"\"\n",
    "    def kickstart(self):\n",
    "        if self.sleep_interval>0:\n",
    "            for i in range(1, self.pages_to_scrape+1):\n",
    "                self.scrape_url(self.url_pattern % i)\n",
    "                time.sleep(self.sleep_interval)\n",
    "        else:\n",
    "            for i in range(1, self.pages_to_scrape+1):\n",
    "                self.scrape_url(self.url_pattern % i)\n",
    "\n",
    "\n",
    "URL_PATTERN = 'http://quotes.toscrape.com/page/%s/' # regex pattern for the urls to scrape\n",
    "PAGES_TO_SCRAPE = int(input(\"Enter number of pages to be scraped\")) # how many webpages to scrapge\n",
    "\n",
    "\"\"\"\n",
    "This is a custom parser function you will complete in the challenge.\n",
    "Right now it simply returns the string passed to it. But in this lab\n",
    "you will complete this function so that it extracts the quotes.\n",
    "This function will be passed to the IronhackSpider class.\n",
    "\"\"\"\n",
    "def quotes_parser(content):\n",
    "    quoteDATA = content.find_all('span', attrs = {'class':'text'})\n",
    "    quotes = []\n",
    "    for i in range(len(quoteDATA)):\n",
    "        quotes.append(quoteDATA[i].text)\n",
    "    \n",
    "    return quotes\n",
    "\n",
    "# Instantiate the IronhackSpider class\n",
    "my_spider = IronhackSpider(URL_PATTERN, PAGES_TO_SCRAPE, content_parser=quotes_parser)\n",
    "\n",
    "# Start scraping jobs\n",
    "my_spider.kickstart()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
